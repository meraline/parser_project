#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç–∑—ã–≤–∞
"""

import requests
from bs4 import BeautifulSoup
import re

def analyze_review_page_structure():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞"""
    
    review_url = "https://www.drom.ru/reviews/aito/m7/1455586/"
    
    print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç–∑—ã–≤–∞")
    print(f"üìç URL: {review_url}")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        response = requests.get(review_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä: {len(response.content)} –±–∞–π—Ç")
        
        # 1. –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å data-ftid
        ftid_elements = soup.find_all(attrs={"data-ftid": True})
        print(f"\nüè∑Ô∏è –ù–∞–π–¥–µ–Ω–æ {len(ftid_elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å data-ftid:")
        
        ftid_dict = {}
        for elem in ftid_elements:
            ftid = elem.get('data-ftid')
            if ftid not in ftid_dict:
                ftid_dict[ftid] = []
            ftid_dict[ftid].append(elem.get_text(strip=True)[:50])
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ ftid
        important_ftids = [key for key in ftid_dict.keys() if 'review' in key.lower()]
        for ftid in sorted(important_ftids):
            print(f"  üìå {ftid}: {ftid_dict[ftid][0]}...")
        
        # 2. –ò—â–µ–º –∫–ª–∞—Å—Å—ã —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        print(f"\nüé® CSS –∫–ª–∞—Å—Å—ã —Å 'content' –∏–ª–∏ 'review':")
        all_elements = soup.find_all()
        content_classes = set()
        
        for elem in all_elements:
            classes = elem.get('class', [])
            for cls in classes:
                if 'content' in cls.lower() or 'review' in cls.lower():
                    content_classes.add(cls)
        
        for cls in sorted(content_classes)[:10]:
            elements = soup.find_all(class_=cls)
            if elements:
                text = elements[0].get_text(strip=True)[:50]
                print(f"  üéØ .{cls}: {text}...")
        
        # 3. –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫ –æ—Ç–∑—ã–≤–∞
        main_content_selectors = [
            'article',
            'main',
            '.review',
            '.content',
            '[role="main"]'
        ]
        
        print(f"\nüìù –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
        for selector in main_content_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                print(f"  ‚úÖ {selector}: –Ω–∞–π–¥–µ–Ω, –¥–ª–∏–Ω–∞ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                if len(text) > 100:
                    print(f"      –ù–∞—á–∞–ª–æ: {text[:100]}...")
        
        # 4. –ò—â–µ–º –±–æ–ª—å—à–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏
        print(f"\nüìñ –ö—Ä—É–ø–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏ (>100 —Å–∏–º–≤–æ–ª–æ–≤):")
        all_texts = soup.find_all(text=True)
        large_texts = []
        
        for text in all_texts:
            cleaned = text.strip()
            if len(cleaned) > 100 and not re.match(r'^[\s\n\r]*$', cleaned):
                parent = text.parent
                if parent:
                    tag_info = f"{parent.name}"
                    if parent.get('class'):
                        tag_info += f".{'.'.join(parent.get('class'))}"
                    if parent.get('data-ftid'):
                        tag_info += f"[data-ftid='{parent.get('data-ftid')}']"
                    
                    large_texts.append((tag_info, cleaned[:100]))
        
        for i, (tag_info, text) in enumerate(large_texts[:5], 1):
            print(f"  {i}. {tag_info}: {text}...")
        
        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç—å HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç—å HTML –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        
        with open('/home/analityk/–î–æ–∫—É–º–µ–Ω—Ç—ã/projects/parser_project/review_page_sample.html', 'w', encoding='utf-8') as f:
            f.write(str(soup.prettify())[:50000])  # –ü–µ—Ä–≤—ã–µ 50KB
        
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ review_page_sample.html")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    analyze_review_page_structure()
