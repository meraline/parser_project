#!/usr/bin/env python3
"""
–°—Ç–∞–±–∏–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –æ—Ç–∑—ã–≤–æ–≤ –∏ –±–æ—Ä—Ç–∂—É—Ä–Ω–∞–ª–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å Drom.ru –∏ Drive2.ru –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
–†–∞–±–æ—Ç–∞–µ—Ç –≤ —â–∞–¥—è—â–µ–º —Ä–µ–∂–∏–º–µ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
"""

import sqlite3
import time
import random
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import re
import json
import logging
from dataclasses import dataclass
from urllib.parse import urljoin, urlparse
import hashlib
from pathlib import Path

from botasaurus.browser import browser, Driver
from botasaurus.request import request, Request
from botasaurus.soupify import soupify
from botasaurus import bt

# ==================== –ù–ê–°–¢–†–û–ô–ö–ò ====================

class Config:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
    
    # –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö
    DB_PATH = "auto_reviews.db"
    
    # –ó–∞–¥–µ—Ä–∂–∫–∏ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
    MIN_DELAY = 5      # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    MAX_DELAY = 15     # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    ERROR_DELAY = 30   # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    RATE_LIMIT_DELAY = 300  # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–∏ rate limit (5 –º–∏–Ω—É—Ç)
    
    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
    MAX_RETRIES = 3    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤
    PAGES_PER_SESSION = 50  # –°—Ç—Ä–∞–Ω–∏—Ü –∑–∞ —Å–µ—Å—Å–∏—é
    MAX_REVIEWS_PER_MODEL = 1000  # –ú–∞–∫—Å–∏–º—É–º –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ –º–æ–¥–µ–ª—å
    
    # User agents –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ]
    
    # –°–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    TARGET_BRANDS = {
        'toyota': ['camry', 'corolla', 'rav4', 'highlander', 'prius', 'land-cruiser'],
        'volkswagen': ['polo', 'golf', 'passat', 'tiguan', 'touareg', 'jetta'],
        'nissan': ['qashqai', 'x-trail', 'almera', 'teana', 'murano', 'pathfinder'],
        'hyundai': ['solaris', 'elantra', 'tucson', 'santa-fe', 'creta', 'sonata'],
        'kia': ['rio', 'cerato', 'sportage', 'sorento', 'soul', 'optima'],
        'mazda': ['mazda3', 'mazda6', 'cx-5', 'cx-3', 'mx-5', 'cx-9'],
        'ford': ['focus', 'fiesta', 'mondeo', 'kuga', 'explorer', 'ecosport'],
        'chevrolet': ['cruze', 'aveo', 'captiva', 'lacetti', 'tahoe', 'suburban'],
        'skoda': ['octavia', 'rapid', 'fabia', 'superb', 'kodiaq', 'karoq'],
        'renault': ['logan', 'sandero', 'duster', 'kaptur', 'megane', 'fluence'],
        'mitsubishi': ['lancer', 'outlander', 'asx', 'pajero', 'eclipse-cross', 'l200'],
        'honda': ['civic', 'accord', 'cr-v', 'pilot', 'fit', 'hr-v'],
        'bmw': ['3-series', '5-series', 'x3', 'x5', 'x1', '1-series'],
        'mercedes-benz': ['c-class', 'e-class', 's-class', 'glc', 'gle', 'gla'],
        'audi': ['a3', 'a4', 'a6', 'q3', 'q5', 'q7'],
        'lada': ['granta', 'kalina', 'priora', 'vesta', 'xray', 'largus']
    }

# ==================== –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–• ====================

@dataclass
class ReviewData:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–∞"""
    source: str          # drom.ru, drive2.ru
    type: str           # review, board_journal
    brand: str
    model: str
    generation: Optional[str] = None
    year: Optional[int] = None
    url: str = ""
    title: str = ""
    content: str = ""
    author: str = ""
    rating: Optional[float] = None
    pros: str = ""
    cons: str = ""
    mileage: Optional[int] = None
    engine_volume: Optional[float] = None
    fuel_type: str = ""
    transmission: str = ""
    body_type: str = ""
    drive_type: str = ""
    publish_date: Optional[datetime] = None
    views_count: Optional[int] = None
    likes_count: Optional[int] = None
    comments_count: Optional[int] = None
    parsed_at: datetime = None
    content_hash: str = ""
    
    def __post_init__(self):
        if self.parsed_at is None:
            self.parsed_at = datetime.now()
        
        # –°–æ–∑–¥–∞–µ–º —Ö–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        content_for_hash = f"{self.url}_{self.title}_{self.content[:100]}"
        self.content_hash = hashlib.md5(content_for_hash.encode()).hexdigest()

# ==================== –ë–ê–ó–ê –î–ê–ù–ù–´–• ====================

class ReviewsDatabase:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"""
    
    def __init__(self, db_path: str = Config.DB_PATH):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –æ—Ç–∑—ã–≤–æ–≤
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS reviews (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source TEXT NOT NULL,
                type TEXT NOT NULL,
                brand TEXT NOT NULL,
                model TEXT NOT NULL,
                generation TEXT,
                year INTEGER,
                url TEXT UNIQUE,
                title TEXT,
                content TEXT,
                author TEXT,
                rating REAL,
                pros TEXT,
                cons TEXT,
                mileage INTEGER,
                engine_volume REAL,
                fuel_type TEXT,
                transmission TEXT,
                body_type TEXT,
                drive_type TEXT,
                publish_date DATETIME,
                views_count INTEGER,
                likes_count INTEGER,
                comments_count INTEGER,
                parsed_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                content_hash TEXT UNIQUE,
                UNIQUE(url, content_hash)
            )
        """)
        
        # –¢–∞–±–ª–∏—Ü–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS parsing_stats (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                brand TEXT,
                model TEXT,
                source TEXT,
                pages_parsed INTEGER DEFAULT 0,
                reviews_found INTEGER DEFAULT 0,
                errors_count INTEGER DEFAULT 0,
                started_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                finished_at DATETIME,
                status TEXT DEFAULT 'running'
            )
        """)
        
        # –¢–∞–±–ª–∏—Ü–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS sources_queue (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                brand TEXT,
                model TEXT,
                source TEXT,
                base_url TEXT,
                priority INTEGER DEFAULT 1,
                status TEXT DEFAULT 'pending',
                last_parsed DATETIME,
                total_pages INTEGER DEFAULT 0,
                parsed_pages INTEGER DEFAULT 0,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_brand_model ON reviews(brand, model)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_source_type ON reviews(source, type)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_parsed_at ON reviews(parsed_at)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_content_hash ON reviews(content_hash)")
        
        conn.commit()
        conn.close()
    
    def save_review(self, review: ReviewData) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–∞ –≤ –±–∞–∑—É"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT OR IGNORE INTO reviews (
                    source, type, brand, model, generation, year, url, title, content,
                    author, rating, pros, cons, mileage, engine_volume, fuel_type,
                    transmission, body_type, drive_type, publish_date, views_count,
                    likes_count, comments_count, parsed_at, content_hash
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                review.source, review.type, review.brand, review.model, review.generation,
                review.year, review.url, review.title, review.content, review.author,
                review.rating, review.pros, review.cons, review.mileage, review.engine_volume,
                review.fuel_type, review.transmission, review.body_type, review.drive_type,
                review.publish_date, review.views_count, review.likes_count,
                review.comments_count, review.parsed_at, review.content_hash
            ))
            
            success = cursor.rowcount > 0
            conn.commit()
            conn.close()
            return success
            
        except sqlite3.IntegrityError:
            # –î—É–±–ª–∏—Ä—É—é—â–∞—è –∑–∞–ø–∏—Å—å
            return False
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–∑—ã–≤–∞: {e}")
            return False
    
    def get_reviews_count(self, brand: str = None, model: str = None) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ—Ç–∑—ã–≤–æ–≤"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        if brand and model:
            cursor.execute("SELECT COUNT(*) FROM reviews WHERE brand = ? AND model = ?", (brand, model))
        elif brand:
            cursor.execute("SELECT COUNT(*) FROM reviews WHERE brand = ?", (brand,))
        else:
            cursor.execute("SELECT COUNT(*) FROM reviews")
        
        count = cursor.fetchone()[0]
        conn.close()
        return count
    
    def is_url_parsed(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –±—ã–ª –ª–∏ URL —É–∂–µ —Å–ø–∞—Ä—Å–µ–Ω"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT 1 FROM reviews WHERE url = ? LIMIT 1", (url,))
        exists = cursor.fetchone() is not None
        
        conn.close()
        return exists
    
    def get_parsing_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        cursor.execute("SELECT COUNT(*) FROM reviews")
        total_reviews = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(DISTINCT brand) FROM reviews")
        unique_brands = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(DISTINCT model) FROM reviews")
        unique_models = cursor.fetchone()[0]
        
        cursor.execute("SELECT source, COUNT(*) FROM reviews GROUP BY source")
        by_source = dict(cursor.fetchall())
        
        cursor.execute("SELECT type, COUNT(*) FROM reviews GROUP BY type")
        by_type = dict(cursor.fetchall())
        
        conn.close()
        
        return {
            'total_reviews': total_reviews,
            'unique_brands': unique_brands,
            'unique_models': unique_models,
            'by_source': by_source,
            'by_type': by_type
        }

# ==================== –ü–ê–†–°–ï–†–´ ====================

class BaseParser:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–µ—Ä–æ–≤"""
    
    def __init__(self, db: ReviewsDatabase):
        self.db = db
        self.session_stats = {
            'parsed': 0,
            'saved': 0,
            'errors': 0,
            'skipped': 0
        }
    
    def random_delay(self, min_delay: int = None, max_delay: int = None):
        """–°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
        min_delay = min_delay or Config.MIN_DELAY
        max_delay = max_delay or Config.MAX_DELAY
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)
    
    def normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text.strip())
        
        # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å
        text = re.sub(r'<[^>]+>', '', text)
        
        return text
    
    def extract_year(self, text: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        year_match = re.search(r'\b(19|20)\d{2}\b', text)
        if year_match:
            year = int(year_match.group())
            if 1980 <= year <= datetime.now().year:
                return year
        return None
    
    def extract_mileage(self, text: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–±–µ–≥–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ò—â–µ–º —á–∏—Å–ª–∞ —Å "–∫–º", "—Ç—ã—Å", "k" –∏ —Ç.–¥.
        patterns = [
            r'(\d+(?:\s*\d{3})*)\s*(?:—Ç—ã—Å\.?\s*)?–∫–º',
            r'(\d+)\s*(?:k|–ö)\s*–∫–º',
            r'–ø—Ä–æ–±–µ–≥[:\s]*(\d+(?:\s*\d{3})*)',
            r'(\d+(?:\s*\d{3})*)\s*(?:—Ç—ã—Å—è—á|—Ç—ã—Å)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                mileage_str = match.group(1).replace(' ', '')
                try:
                    mileage = int(mileage_str)
                    # –ï—Å–ª–∏ —á–∏—Å–ª–æ –º–µ–Ω—å—à–µ 1000, –≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ —Ç—ã—Å—è—á–∏ –∫–∏–ª–æ–º–µ—Ç—Ä–æ–≤
                    if mileage < 1000:
                        mileage *= 1000
                    return mileage
                except ValueError:
                    continue
        
        return None
    
    def extract_engine_volume(self, text: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –¥–≤–∏–≥–∞—Ç–µ–ª—è –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        patterns = [
            r'(\d+(?:\.\d+)?)\s*–ª',
            r'(\d{4})\s*—Å–º¬≥',  # 1600 —Å–º¬≥
            r'(\d+\.\d+)',     # 1.6, 2.0 –∏ —Ç.–¥.
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                try:
                    volume = float(match)
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–º¬≥ –≤ –ª–∏—Ç—Ä—ã
                    if volume > 100:  # —Å–º¬≥
                        volume = volume / 1000
                    
                    if 0.8 <= volume <= 8.0:  # –†–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã
                        return volume
                except ValueError:
                    continue
        
        return None

# @browser(
#     block_images=True,
#     cache=True,
#     reuse_driver=True,
#     max_retry=3,
#     user_agent=random.choice(Config.USER_AGENTS),
#     headless=True
# )
class DromParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –æ—Ç–∑—ã–≤–æ–≤ —Å Drom.ru"""

    @staticmethod
    @browser(
        block_images=True,
        cache=False,
        reuse_driver=True,
        max_retry=3,
        user_agent=random.choice(Config.USER_AGENTS),
        headless=True,
    )
    def parse_brand_model_reviews(driver: Driver, data: Dict, parser) -> List[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–∞—Ä–∫–∏ –∏ –º–æ–¥–µ–ª–∏"""
        brand = data['brand']
        model = data['model']
        max_pages = data.get('max_pages', 50)

        reviews = []
        base_url = f"https://www.drom.ru/reviews/{brand}/{model}/"

        try:
            print(f"  üîç Drom.ru: –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ {brand} {model}")

            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç–∑—ã–≤–æ–≤
            driver.google_get(base_url, bypass_cloudflare=True)
            parser.random_delay(3, 7)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
            if driver.select('.error-page') or "404" in driver.title:
                print(f"    ‚ö†Ô∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {base_url}")
                return reviews

            # –ü–∞—Ä—Å–∏–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            current_page = 1

            while current_page <= max_pages:
                print(f"    üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page}")

                # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –æ—Ç–∑—ã–≤–æ–≤
                review_cards = driver.select_all('[data-ftid="component_reviews-item"]')
                if not review_cards:
                    review_cards = driver.select_all('.css-1ksh4lf')

                if not review_cards:
                    print(f"    ‚ö†Ô∏è –û—Ç–∑—ã–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page}")
                    break

                page_reviews = 0

                for card in review_cards:
                    try:
                        review = parser._parse_review_card(card, brand, model, base_url)
                        if review and not parser.db.is_url_parsed(review.url):
                            reviews.append(review)
                            page_reviews += 1

                    except Exception as e:
                        parser.session_stats['errors'] += 1
                        logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ –æ—Ç–∑—ã–≤–∞: {e}")

                print(f"    ‚úì –ù–∞–π–¥–µ–Ω–æ {page_reviews} –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤")

                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                next_link = driver.select('a[rel="next"]')
                if not next_link:
                    print(f"    üìã –ë–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ—Ç")
                    break

                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                next_url = next_link.get_attribute('href')
                if next_url:
                    if not next_url.startswith('http'):
                        next_url = urljoin(base_url, next_url)

                    driver.get_via_this_page(next_url)
                    parser.random_delay()
                    current_page += 1
                else:
                    break

            print(f"  ‚úì Drom.ru: –°–æ–±—Ä–∞–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {brand} {model}")

        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Drom.ru {brand} {model}: {e}")
            parser.session_stats['errors'] += 1

        return reviews
    
    def _parse_drive2_card(self, card, brand: str, model: str, review_type: str, base_url: str) -> Optional[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ Drive2"""
        try:
            review = ReviewData(
                source="drive2.ru",
                type=review_type,
                brand=brand,
                model=model
            )
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫–∞
            title_link = card.select('a.c-car-card__caption') or card.select('a.c-post-card__title') or card.select('h3 a')
            if title_link:
                review.title = self.normalize_text(title_link.get_text())
                href = title_link.get_attribute('href')
                if href:
                    review.url = urljoin(base_url, href)
            
            # –ê–≤—Ç–æ—Ä
            author_elem = card.select('.c-username__link') or card.select('.c-post-card__author')
            if author_elem:
                review.author = self.normalize_text(author_elem.get_text())
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–≤—Ç–æ–º–æ–±–∏–ª–µ
            info_elem = card.select('.c-car-card__info') or card.select('.c-post-card__car-info')
            if info_elem:
                info_text = info_elem.get_text()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                review.year = self.extract_year(info_text)
                review.engine_volume = self.extract_engine_volume(info_text)
                review.mileage = self.extract_mileage(info_text)
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                if '–±–µ–Ω–∑–∏–Ω' in info_text.lower():
                    review.fuel_type = '–±–µ–Ω–∑–∏–Ω'
                elif '–¥–∏–∑–µ–ª—å' in info_text.lower():
                    review.fuel_type = '–¥–∏–∑–µ–ª—å'
                
                if '–∞–≤—Ç–æ–º–∞—Ç' in info_text.lower():
                    review.transmission = '–∞–≤—Ç–æ–º–∞—Ç'
                elif '–º–µ—Ö–∞–Ω–∏–∫' in info_text.lower():
                    review.transmission = '–º–µ—Ö–∞–Ω–∏–∫–∞'
                
                # –ü—Ä–∏–≤–æ–¥
                if '–ø–æ–ª–Ω—ã–π' in info_text.lower() or '4wd' in info_text.lower():
                    review.drive_type = '–ø–æ–ª–Ω—ã–π'
                elif '–ø–µ—Ä–µ–¥–Ω–∏–π' in info_text.lower() or 'fwd' in info_text.lower():
                    review.drive_type = '–ø–µ—Ä–µ–¥–Ω–∏–π'
                elif '–∑–∞–¥–Ω–∏–π' in info_text.lower() or 'rwd' in info_text.lower():
                    review.drive_type = '–∑–∞–¥–Ω–∏–π'
            
            # –ü—Ä–æ–±–µ–≥ –æ—Ç–¥–µ–ª—å–Ω–æ
            mileage_elem = card.select('.c-car-card__param_mileage')
            if mileage_elem:
                mileage_text = mileage_elem.get_text()
                review.mileage = self.extract_mileage(mileage_text)
            
            # –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
            preview_elem = card.select('.c-car-card__preview') or card.select('.c-post-card__preview')
            if preview_elem:
                review.content = self.normalize_text(preview_elem.get_text())
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            views_elem = card.select('.c-post-card__views')
            if views_elem:
                views_text = views_elem.get_text()
                views_match = re.search(r'(\d+)', views_text)
                if views_match:
                    review.views_count = int(views_match.group(1))
            
            likes_elem = card.select('.c-post-card__likes')
            if likes_elem:
                likes_text = likes_elem.get_text()
                likes_match = re.search(r'(\d+)', likes_text)
                if likes_match:
                    review.likes_count = int(likes_match.group(1))
            
            # –î–∞—Ç–∞
            date_elem = card.select('.c-post-card__date') or card.select('.c-car-card__date')
            if date_elem:
                date_text = date_elem.get_text()
                review.publish_date = self._parse_date(date_text)
            
            return review if review.url else None
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ Drive2: {e}")
            return None
    
    def _parse_date(self, date_text: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ Drive2"""
        try:
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
            date_text = re.sub(r'[^\d\.\s\w]', '', date_text).strip()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞—Ç—ã
            now = datetime.now()
            
            if '—Å–µ–≥–æ–¥–Ω—è' in date_text.lower():
                return now.replace(hour=12, minute=0, second=0, microsecond=0)
            elif '–≤—á–µ—Ä–∞' in date_text.lower():
                return (now - timedelta(days=1)).replace(hour=12, minute=0, second=0, microsecond=0)
            elif '–Ω–∞–∑–∞–¥' in date_text.lower():
                if '–¥–Ω' in date_text:
                    days_match = re.search(r'(\d+)\s*–¥–Ω', date_text)
                    if days_match:
                        days = int(days_match.group(1))
                        return now - timedelta(days=days)
                elif '—á–∞—Å' in date_text:
                    hours_match = re.search(r'(\d+)\s*—á–∞—Å', date_text)
                    if hours_match:
                        hours = int(hours_match.group(1))
                        return now - timedelta(hours=hours)
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            patterns = [
                r'(\d{1,2})\.(\d{1,2})\.(\d{4})',  # 01.01.2023
                r'(\d{1,2})\s+(\w+)\s+(\d{4})',   # 1 —è–Ω–≤–∞—Ä—è 2023
                r'(\d{4})-(\d{2})-(\d{2})',       # 2023-01-01
            ]
            
            months_map = {
                '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4,
                '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8,
                '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12
            }
            
            for pattern in patterns:
                match = re.search(pattern, date_text)
                if match:
                    groups = match.groups()
                    
                    if len(groups) == 3:
                        if groups[1].isdigit():
                            day, month, year = map(int, groups)
                        else:
                            day = int(groups[0])
                            month = months_map.get(groups[1].lower(), 1)
                            year = int(groups[2])
                        
                        return datetime(year, month, day)
            
        except Exception:
            pass
        
        return None

# ==================== –ì–õ–ê–í–ù–´–ô –ü–ê–†–°–ï–† ====================

class AutoReviewsParser:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –ø–∞—Ä—Å–µ—Ä–∞ –æ—Ç–∑—ã–≤–æ–≤ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π"""
    
    def __init__(self, db_path: str = Config.DB_PATH):
        self.db = ReviewsDatabase(db_path)
        self.setup_logging()
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–æ–≤
        self.drom_parser = DromParser(self.db)
        self.drive2_parser = Drive2Parser(self.db)
    
    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / f"parser_{datetime.now().strftime('%Y%m%d')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    def initialize_sources_queue(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—á–µ—Ä–µ–¥–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        conn = sqlite3.connect(self.db.db_path)
        cursor = conn.cursor()
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—É—é –æ—á–µ—Ä–µ–¥—å
        cursor.execute("DELETE FROM sources_queue")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π
        for brand, models in Config.TARGET_BRANDS.items():
            for model in models:
                for source in ['drom.ru', 'drive2.ru']:
                    cursor.execute("""
                        INSERT INTO sources_queue (brand, model, source, priority)
                        VALUES (?, ?, ?, ?)
                    """, (brand, model, source, 1))
        
        conn.commit()
        conn.close()
        
        print(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –æ—á–µ—Ä–µ–¥—å –∏–∑ {len(Config.TARGET_BRANDS) * sum(len(models) for models in Config.TARGET_BRANDS.values()) * 2} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    def get_next_source(self) -> Optional[Tuple[str, str, str]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        conn = sqlite3.connect(self.db.db_path)
        cursor = conn.cursor()
        
        # –ò—â–µ–º –Ω–µ—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        cursor.execute("""
            SELECT id, brand, model, source FROM sources_queue 
            WHERE status = 'pending' 
            ORDER BY priority DESC, RANDOM()
            LIMIT 1
        """)
        
        result = cursor.fetchone()
        
        if result:
            source_id, brand, model, source = result
            
            # –û—Ç–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã–π
            cursor.execute("""
                UPDATE sources_queue 
                SET status = 'processing', last_parsed = CURRENT_TIMESTAMP 
                WHERE id = ?
            """, (source_id,))
            
            conn.commit()
            conn.close()
            
            return brand, model, source
        
        conn.close()
        return None
    
    def mark_source_completed(self, brand: str, model: str, source: str, pages_parsed: int, reviews_found: int):
        """–û—Ç–º–µ—Ç–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–≥–æ"""
        conn = sqlite3.connect(self.db.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            UPDATE sources_queue 
            SET status = 'completed', parsed_pages = ?, total_pages = ?
            WHERE brand = ? AND model = ? AND source = ?
        """, (pages_parsed, pages_parsed, brand, model, source))
        
        conn.commit()
        conn.close()
    
    def parse_single_source(self, brand: str, model: str, source: str) -> int:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        print(f"\nüéØ –ü–∞—Ä—Å–∏–Ω–≥: {brand} {model} –Ω–∞ {source}")
        
        reviews = []
        data = {
            'brand': brand,
            'model': model,
            'max_pages': Config.PAGES_PER_SESSION
        }
        
        try:
            if source == 'drom.ru':
                reviews = self.drom_parser.parse_brand_model_reviews(
                    data, metadata=self.drom_parser
                )
            elif source == 'drive2.ru':
                reviews = self.drive2_parser.parse_brand_model_reviews(
                    data, metadata=self.drive2_parser
                )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–∑—ã–≤—ã –≤ –±–∞–∑—É
            saved_count = 0
            for review in reviews:
                if self.db.save_review(review):
                    saved_count += 1
            
            print(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –∏–∑ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤")
            
            # –û—Ç–º–µ—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π
            self.mark_source_completed(brand, model, source, Config.PAGES_PER_SESSION, saved_count)
            
            return saved_count
            
        except Exception as e:
            logging.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {brand} {model} {source}: {e}")
            return 0
    
    def run_parsing_session(self, max_sources: int = 10, session_duration_hours: int = 2):
        """–ó–∞–ø—É—Å–∫ —Å–µ—Å—Å–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        print(f"\nüöÄ –ó–ê–ü–£–°–ö –°–ï–°–°–ò–ò –ü–ê–†–°–ò–ù–ì–ê")
        print(f"{'='*60}")
        print(f"–ú–∞–∫—Å–∏–º—É–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–∞ —Å–µ—Å—Å–∏—é: {max_sources}")
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {session_duration_hours} —á–∞—Å–æ–≤")
        print(f"{'='*60}")
        
        session_start = datetime.now()
        session_end = session_start + timedelta(hours=session_duration_hours)
        
        sources_processed = 0
        total_reviews_saved = 0
        
        while sources_processed < max_sources and datetime.now() < session_end:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫
            source_info = self.get_next_source()
            
            if not source_info:
                print("\n‚úÖ –í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
                break
            
            brand, model, source = source_info
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
            current_count = self.db.get_reviews_count(brand, model)
            if current_count >= Config.MAX_REVIEWS_PER_MODEL:
                print(f"  ‚ö†Ô∏è –õ–∏–º–∏—Ç –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {brand} {model} –¥–æ—Å—Ç–∏–≥–Ω—É—Ç ({current_count})")
                self.mark_source_completed(brand, model, source, 0, 0)
                continue
            
            # –ü–∞—Ä—Å–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫
            try:
                reviews_saved = self.parse_single_source(brand, model, source)
                total_reviews_saved += reviews_saved
                sources_processed += 1
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
                if sources_processed < max_sources:
                    delay = random.uniform(30, 60)  # 30-60 —Å–µ–∫—É–Ω–¥ –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
                    print(f"  ‚è≥ –ü–∞—É–∑–∞ {delay:.1f} —Å–µ–∫...")
                    time.sleep(delay)
                    
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {brand} {model} {source}: {e}")
                sources_processed += 1
                
                # –£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –ø–∞—É–∑–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                time.sleep(Config.ERROR_DELAY)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏
        session_duration = datetime.now() - session_start
        
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ï–°–°–ò–ò")
        print(f"{'='*60}")
        print(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {session_duration}")
        print(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {sources_processed}")
        print(f"–û—Ç–∑—ã–≤–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {total_reviews_saved}")
        print(f"{'='*60}")
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã
        stats = self.db.get_parsing_stats()
        print(f"\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–•")
        print(f"{'='*60}")
        print(f"–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {stats['total_reviews']}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤: {stats['unique_brands']}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {stats['unique_models']}")
        print(f"–ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º: {stats['by_source']}")
        print(f"–ü–æ —Ç–∏–ø–∞–º: {stats['by_type']}")
        print(f"{'='*60}")
    
    def run_continuous_parsing(self, daily_sessions: int = 4, session_sources: int = 10):
        """–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏"""
        print(f"\nüîÑ –†–ï–ñ–ò–ú –ù–ï–ü–†–ï–†–´–í–ù–û–ì–û –ü–ê–†–°–ò–ù–ì–ê")
        print(f"–°–µ—Å—Å–∏–π –≤ –¥–µ–Ω—å: {daily_sessions}")
        print(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–∞ —Å–µ—Å—Å–∏—é: {session_sources}")
        print(f"–ò–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏: {24 // daily_sessions} —á–∞—Å–æ–≤")
        
        session_interval = timedelta(hours=24 // daily_sessions)
        
        while True:
            try:
                # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Å—Å–∏—é –ø–∞—Ä—Å–∏–Ω–≥–∞
                self.run_parsing_session(max_sources=session_sources, session_duration_hours=2)
                
                # –ñ–¥–µ–º –¥–æ —Å–ª–µ–¥—É—é—â–µ–π —Å–µ—Å—Å–∏–∏
                next_session = datetime.now() + session_interval
                print(f"\n‚è∞ –°–ª–µ–¥—É—é—â–∞—è —Å–µ—Å—Å–∏—è: {next_session.strftime('%Y-%m-%d %H:%M:%S')}")
                
                while datetime.now() < next_session:
                    remaining = next_session - datetime.now()
                    print(f"‚è≥ –î–æ —Å–ª–µ–¥—É—é—â–µ–π —Å–µ—Å—Å–∏–∏: {remaining}", end='\r')
                    time.sleep(60)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
                
            except KeyboardInterrupt:
                print("\nüëã –ü–∞—Ä—Å–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                break
            except Exception as e:
                logging.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
                print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
                print("‚è≥ –ü–∞—É–∑–∞ 30 –º–∏–Ω—É—Ç –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º...")
                time.sleep(1800)  # 30 –º–∏–Ω—É—Ç –ø–∞—É–∑–∞ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–µ

# ==================== –£–¢–ò–õ–ò–¢–´ –£–ü–†–ê–í–õ–ï–ù–ò–Ø ====================

class ParserManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–æ–º"""
    
    def __init__(self, db_path: str = Config.DB_PATH):
        self.parser = AutoReviewsParser(db_path)
    
    def show_status(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—É—Å –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ –æ—á–µ—Ä–µ–¥–∏"""
        stats = self.parser.db.get_parsing_stats()
        
        print(f"\nüìä –°–¢–ê–¢–£–° –ë–ê–ó–´ –î–ê–ù–ù–´–•")
        print(f"{'='*50}")
        print(f"–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {stats['total_reviews']:,}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤: {stats['unique_brands']}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {stats['unique_models']}")
        
        if stats['by_source']:
            print(f"\n–ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
            for source, count in stats['by_source'].items():
                print(f"  {source}: {count:,}")
        
        if stats['by_type']:
            print(f"\n–ü–æ —Ç–∏–ø–∞–º:")
            for type_name, count in stats['by_type'].items():
                print(f"  {type_name}: {count:,}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–µ—Ä–µ–¥–∏
        conn = sqlite3.connect(self.parser.db.db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT status, COUNT(*) FROM sources_queue GROUP BY status")
        queue_stats = dict(cursor.fetchall())
        
        conn.close()
        
        print(f"\nüìã –°–¢–ê–¢–£–° –û–ß–ï–†–ï–î–ò")
        print(f"{'='*50}")
        total_sources = sum(queue_stats.values())
        
        for status, count in queue_stats.items():
            percentage = (count / total_sources * 100) if total_sources > 0 else 0
            print(f"{status}: {count} ({percentage:.1f}%)")
        
        print(f"–í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {total_sources}")
    
    def reset_queue(self):
        """–°–±—Ä–æ—Å –æ—á–µ—Ä–µ–¥–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        print("üîÑ –°–±—Ä–æ—Å –æ—á–µ—Ä–µ–¥–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
        self.parser.initialize_sources_queue()
    
    def export_data(self, output_format: str = 'xlsx'):
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã"""
        print(f"üì§ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ {output_format}...")
        
        conn = sqlite3.connect(self.parser.db.db_path)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ—Ç–∑—ã–≤—ã
        query = """
            SELECT 
                source, type, brand, model, year, title, author, rating,
                content, pros, cons, mileage, engine_volume, fuel_type,
                transmission, body_type, drive_type, publish_date, 
                views_count, likes_count, comments_count, url, parsed_at
            FROM reviews
            ORDER BY brand, model, parsed_at DESC
        """
        
        df_data = []
        cursor = conn.cursor()
        cursor.execute(query)
        
        columns = [description[0] for description in cursor.description]
        
        for row in cursor.fetchall():
            df_data.append(dict(zip(columns, row)))
        
        conn.close()
        
        if not df_data:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if output_format.lower() == 'xlsx':
            filename = f"auto_reviews_export_{timestamp}.xlsx"
            bt.write_excel(df_data, filename.replace('.xlsx', ''))
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filename}")
        
        elif output_format.lower() == 'json':
            filename = f"auto_reviews_export_{timestamp}.json"
            bt.write_json(df_data, filename.replace('.json', ''))
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filename}")
        
        else:
            print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {output_format}")

# ==================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ====================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–ü–∞—Ä—Å–µ—Ä –æ—Ç–∑—ã–≤–æ–≤ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π")
    parser.add_argument('command', choices=['init', 'parse', 'continuous', 'status', 'export'], 
                       help='–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è')
    parser.add_argument('--sources', type=int, default=10, 
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–∞ —Å–µ—Å—Å–∏—é (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)')
    parser.add_argument('--sessions', type=int, default=4, 
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ—Å—Å–∏–π –≤ –¥–µ–Ω—å –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 4)')
    parser.add_argument('--format', default='xlsx', choices=['xlsx', 'json'],
                       help='–§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: xlsx)')
    
    args = parser.parse_args()
    
    manager = ParserManager()
    
    if args.command == 'init':
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞...")
        manager.reset_queue()
        print("‚úÖ –ü–∞—Ä—Å–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
    
    elif args.command == 'parse':
        print("üéØ –ó–∞–ø—É—Å–∫ —Ä–∞–∑–æ–≤–æ–π —Å–µ—Å—Å–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
        manager.parser.run_parsing_session(max_sources=args.sources)
    
    elif args.command == 'continuous':
        print("üîÑ –ó–∞–ø—É—Å–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
        manager.parser.run_continuous_parsing(
            daily_sessions=args.sessions, 
            session_sources=args.sources
        )
    
    elif args.command == 'status':
        manager.show_status()
    
    elif args.command == 'export':
        manager.export_data(output_format=args.format)

if __name__ == "__main__":
    main()swith('http'):
                        next_url = urljoin(base_url, next_url)
                    
                    driver.get_via_this_page(next_url)
                    self.random_delay()
                    current_page += 1
                else:
                    break
            
            print(f"  ‚úì Drom.ru: –°–æ–±—Ä–∞–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {brand} {model}")
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Drom.ru {brand} {model}: {e}")
            self.session_stats['errors'] += 1
        
        return reviews
    
    def _parse_review_card(self, card, brand: str, model: str, base_url: str) -> Optional[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ –æ—Ç–∑—ã–≤–∞"""
        try:
            review = ReviewData(
                source="drom.ru",
                type="review",
                brand=brand,
                model=model
            )
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫–∞
            title_link = card.select('h3 a') or card.select('a[data-ftid="component_reviews-item-title"]')
            if title_link:
                review.title = self.normalize_text(title_link.get_text())
                href = title_link.get_attribute('href')
                if href:
                    review.url = urljoin(base_url, href)
            
            # –†–µ–π—Ç–∏–Ω–≥
            rating_elem = card.select('.css-kxziuu') or card.select('[data-ftid="component_rating"]')
            if rating_elem:
                rating_text = rating_elem.get_text()
                rating_match = re.search(r'(\d+(?:\.\d+)?)', rating_text)
                if rating_match:
                    review.rating = float(rating_match.group(1))
            
            # –ê–≤—Ç–æ—Ä
            author_elem = card.select('.css-username') or card.select('[data-ftid="component_username"]')
            if author_elem:
                review.author = self.normalize_text(author_elem.get_text())
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–≤—Ç–æ–º–æ–±–∏–ª–µ
            specs_elem = card.select('.css-1x4jntm') or card.select('.css-car-info')
            if specs_elem:
                specs_text = specs_elem.get_text()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                review.year = self.extract_year(specs_text)
                review.engine_volume = self.extract_engine_volume(specs_text)
                review.mileage = self.extract_mileage(specs_text)
                
                # –¢–∏–ø —Ç–æ–ø–ª–∏–≤–∞
                if '–±–µ–Ω–∑–∏–Ω' in specs_text.lower():
                    review.fuel_type = '–±–µ–Ω–∑–∏–Ω'
                elif '–¥–∏–∑–µ–ª—å' in specs_text.lower():
                    review.fuel_type = '–¥–∏–∑–µ–ª—å'
                elif '–≥–∏–±—Ä–∏–¥' in specs_text.lower():
                    review.fuel_type = '–≥–∏–±—Ä–∏–¥'
                
                # –ö–æ—Ä–æ–±–∫–∞ –ø–µ—Ä–µ–¥–∞—á
                if '–∞–≤—Ç–æ–º–∞—Ç' in specs_text.lower() or '–∞–∫–ø–ø' in specs_text.lower():
                    review.transmission = '–∞–≤—Ç–æ–º–∞—Ç'
                elif '–º–µ—Ö–∞–Ω–∏–∫' in specs_text.lower() or '–º–∫–ø–ø' in specs_text.lower():
                    review.transmission = '–º–µ—Ö–∞–Ω–∏–∫–∞'
                elif '–≤–∞—Ä–∏–∞—Ç–æ—Ä' in specs_text.lower():
                    review.transmission = '–≤–∞—Ä–∏–∞—Ç–æ—Ä'
            
            # –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
            content_elem = card.select('.css-1wdvlz0') or card.select('.review-preview')
            if content_elem:
                review.content = self.normalize_text(content_elem.get_text())
            
            # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            date_elem = card.select('.css-date') or card.select('[data-ftid="component_date"]')
            if date_elem:
                date_text = date_elem.get_text()
                review.publish_date = self._parse_date(date_text)
            
            return review if review.url else None
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ –æ—Ç–∑—ã–≤–∞ Drom: {e}")
            return None
    
    def _parse_date(self, date_text: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
            date_text = re.sub(r'[^\d\.\s\w]', '', date_text).strip()
            
            # –†–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
            patterns = [
                r'(\d{1,2})\.(\d{1,2})\.(\d{4})',  # 01.01.2023
                r'(\d{1,2})\s+(\w+)\s+(\d{4})',   # 1 —è–Ω–≤–∞—Ä—è 2023
                r'(\d{4})-(\d{2})-(\d{2})',       # 2023-01-01
            ]
            
            months_map = {
                '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4,
                '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8,
                '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12
            }
            
            for pattern in patterns:
                match = re.search(pattern, date_text)
                if match:
                    groups = match.groups()
                    
                    if len(groups) == 3:
                        if groups[1].isdigit():  # –§–æ—Ä–º–∞—Ç dd.mm.yyyy
                            day, month, year = map(int, groups)
                        else:  # –§–æ—Ä–º–∞—Ç —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –º–µ—Å—è—Ü–∞
                            day = int(groups[0])
                            month = months_map.get(groups[1].lower(), 1)
                            year = int(groups[2])
                        
                        return datetime(year, month, day)
            
        except Exception:
            pass
        
        return None

class Drive2Parser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –æ—Ç–∑—ã–≤–æ–≤ –∏ –±–æ—Ä—Ç–∂—É—Ä–Ω–∞–ª–æ–≤ —Å Drive2.ru"""

    @staticmethod
    @browser(
        block_images=True,
        cache=True,
        reuse_driver=True,
        max_retry=3,
        user_agent=random.choice(Config.USER_AGENTS),
        headless=True,
    )
    def parse_brand_model_reviews(driver: Driver, data: Dict, parser) -> List[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–∞—Ä–∫–∏ –∏ –º–æ–¥–µ–ª–∏"""
        brand = data['brand']
        model = data['model']
        max_pages = data.get('max_pages', 50)

        reviews = []

        # –ü–∞—Ä—Å–∏–º –∏ –æ—Ç–∑—ã–≤—ã, –∏ –±–æ—Ä—Ç–∂—É—Ä–Ω–∞–ª—ã
        for content_type in ['experience', 'logbook']:
            try:
                type_reviews = parser._parse_content_type(
                    driver, brand, model, content_type, max_pages // 2
                )
                reviews.extend(type_reviews)
                parser.random_delay(5, 10)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–∏–ø–∞–º–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

            except Exception as e:
                logging.error(
                    f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {content_type} Drive2.ru {brand} {model}: {e}"
                )
                parser.session_stats['errors'] += 1

        return reviews
    
    def _parse_content_type(self, driver: Driver, brand: str, model: str, content_type: str, max_pages: int) -> List[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        reviews = []
        
        # URL –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if content_type == 'experience':
            base_url = f"https://www.drive2.ru/experience/{brand}/{model}/"
            review_type = "review"
        else:  # logbook
            base_url = f"https://www.drive2.ru/cars/{brand}/{model}/logbook/"
            review_type = "board_journal"
        
        print(f"  üîç Drive2.ru: –ü–∞—Ä—Å–∏–Ω–≥ {review_type} {brand} {model}")
        
        try:
            driver.google_get(base_url, bypass_cloudflare=True)
            self.random_delay(3, 7)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–∫–∏
            if driver.select('.c-error') or "404" in driver.title:
                print(f"    ‚ö†Ô∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {base_url}")
                return reviews
            
            current_page = 1
            
            while current_page <= max_pages:
                print(f"    üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page} ({review_type})")
                
                # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏
                if content_type == 'experience':
                    cards = driver.select_all('.c-car-card')
                else:
                    cards = driver.select_all('.c-post-card') or driver.select_all('.c-logbook-card')
                
                if not cards:
                    print(f"    ‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page}")
                    break
                
                page_reviews = 0
                
                for card in cards:
                    try:
                        review = self._parse_drive2_card(card, brand, model, review_type, base_url)
                        if review and not self.db.is_url_parsed(review.url):
                            reviews.append(review)
                            page_reviews += 1
                        
                    except Exception as e:
                        self.session_stats['errors'] += 1
                        logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ Drive2: {e}")
                
                print(f"    ‚úì –ù–∞–π–¥–µ–Ω–æ {page_reviews} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
                
                # –ü–æ–∏—Å–∫ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                next_link = driver.select('.c-pagination__next') or driver.select('a[rel="next"]')
                if not next_link or 'disabled' in next_link.get_attribute('class', ''):
                    print(f"    üìã –ë–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ—Ç")
                    break
                
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                next_url = next_link.get_attribute('href')
                if next_url:
                    if not next_url.startswith('http'):
                        next_url = base_url + next_url
                    driver.google_get(next_url, bypass_cloudflare=True)
                    self.random_delay(3, 7)
                    current_page += 1
                
                else:
                    print(f"    üìã –ë–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ—Ç")
                    break
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Drive2.ru {brand} {model}: {e}")
            self.session_stats['errors'] += 1
        
        return reviews