#!/usr/bin/env python3
"""
üöó –ú–ê–°–¢–ï–†-–ü–ê–†–°–ï–† DROM.RU - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
=============================================

–ï–î–ò–ù–´–ô –ò –ì–õ–ê–í–ù–´–ô –ü–ê–†–°–ï–† –¥–ª—è drom.ru
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å—é –ª—É—á—à—É—é –ª–æ–≥–∏–∫—É –∏–∑ –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞:

‚úÖ –î–ª–∏–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ–±–∑–æ—Ä—ã) 
‚úÖ –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–∑—ã–≤—ã (–∫—Ä–∞—Ç–∫–∏–µ –º–Ω–µ–Ω–∏—è)
‚úÖ –ö–∞—Ç–∞–ª–æ–≥ –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π
‚úÖ –ù–∞–¥–µ–∂–Ω—É—é —Å–µ—Ç–µ–≤—É—é –ª–æ–≥–∏–∫—É
‚úÖ –û–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –∏ —Ä–µ—Ç—Ä–∞–∏
‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–µ—Ç—Ä–∏–∫–∏

–ì–õ–ê–í–ù–´–ô –ü–ê–†–°–ï–† –ü–†–û–ï–ö–¢–ê - –ù–ï –°–û–ó–î–ê–í–ê–ô–¢–ï –ù–û–í–´–•!

–ê–≤—Ç–æ—Ä: AI Assistant
–î–∞—Ç–∞: 26.08.2025
"""

import json
import logging
import os
import re
import random
import time
import hashlib
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class ReviewData:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–∞"""
    review_id: str
    brand: str
    model: str
    review_type: str  # 'long' –∏–ª–∏ 'short'
    
    # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è  
    year: Optional[int] = None
    engine_volume: Optional[float] = None
    fuel_type: Optional[str] = None
    transmission: Optional[str] = None
    drive_type: Optional[str] = None
    body_type: Optional[str] = None
    
    # –î–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞
    author: Optional[str] = None
    city: Optional[str] = None
    date: Optional[str] = None
    rating: Optional[float] = None
    title: Optional[str] = None
    
    # –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–∞
    positive_text: Optional[str] = None
    negative_text: Optional[str] = None
    general_text: Optional[str] = None
    breakages_text: Optional[str] = None
    content: Optional[str] = None
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    photos: List[str] = field(default_factory=list)
    photos_count: int = 0
    url: Optional[str] = None
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    views_count: Optional[int] = None
    likes_count: Optional[int] = None
    useful_count: Optional[int] = None
    not_useful_count: Optional[int] = None
    comments_count: Optional[int] = None
    
    # –°–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è
    parsed_at: Optional[datetime] = None
    content_hash: Optional[str] = None
    
    def __post_init__(self):
        if self.parsed_at is None:
            self.parsed_at = datetime.now()
        self.content_hash = self.generate_hash()
        self.photos_count = len(self.photos) if self.photos else 0
    
    def generate_hash(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ —Ö–µ—à–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        content_for_hash = f"{self.url or ''}_{self.title or ''}_{self.content or ''}_{self.positive_text or ''}_{self.negative_text or ''}"
        return hashlib.md5(content_for_hash.encode()).hexdigest()


@dataclass
class BrandInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±—Ä–µ–Ω–¥–µ"""
    name: str
    url: str
    reviews_count: int
    url_name: str  # –∏–º—è –≤ URL, –Ω–∞–ø—Ä–∏–º–µ—Ä 'alfa_romeo' –¥–ª—è 'Alfa Romeo'


@dataclass
class ModelInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    name: str
    brand: str
    url: str
    long_reviews_count: int = 0
    short_reviews_count: int = 0
    url_name: str = ""


class SimpleCache:
    """–ü—Ä–æ—Å—Ç–æ–π —Ñ–∞–π–ª–æ–≤—ã–π –∫—ç—à"""
    def __init__(self, cache_dir: str):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def _get_cache_path(self, url: str) -> Path:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫—ç—à–∞"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return self.cache_dir / f"{url_hash}.html"
    
    def get(self, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞"""
        cache_path = self._get_cache_path(url)
        if cache_path.exists():
            try:
                return cache_path.read_text(encoding='utf-8')
            except Exception:
                return None
        return None
    
    def set(self, url: str, content: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à"""
        cache_path = self._get_cache_path(url)
        try:
            cache_path.write_text(content, encoding='utf-8')
        except Exception:
            pass


class MasterDromParser:
    """
    üöó –ú–ê–°–¢–ï–†-–ü–ê–†–°–ï–† DROM.RU - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –í–°–Æ –ª—É—á—à—É—é –ª–æ–≥–∏–∫—É –∏–∑ –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞:
    - –ü–∞—Ä—Å–∏–Ω–≥ –¥–ª–∏–Ω–Ω—ã—Ö –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤
    - –ö–∞—Ç–∞–ª–æ–≥ –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π  
    - –ù–∞–¥–µ–∂–Ω–∞—è —Å–µ—Ç–µ–≤–∞—è –ª–æ–≥–∏–∫–∞ —Å —Ä–µ—Ç—Ä–∞—è–º–∏
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
    """

    def __init__(self, 
                 delay: float = 1.0, 
                 cache_dir: str = "data/cache",
                 enable_cache: bool = True):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Ç–µ—Ä-–ø–∞—Ä—Å–µ—Ä–∞"""
        
        self.base_url = "https://www.drom.ru"
        self.delay = delay
        self.enable_cache = enable_cache
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        Path(cache_dir).mkdir(parents=True, exist_ok=True)
        Path("logs").mkdir(exist_ok=True)
        Path("data").mkdir(exist_ok=True)
        
        # –ö—ç—à
        if self.enable_cache:
            self.cache = SimpleCache(cache_dir)
        else:
            self.cache = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'requests_made': 0,
            'requests_cached': 0,
            'requests_failed': 0,
            'reviews_parsed': 0
        }
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏
        self.session = requests.Session()
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                         "AppleWebKit/537.36 (KHTML, like Gecko) "
                         "Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        logger.info("üöó –ú–∞—Å—Ç–µ—Ä-–ø–∞—Ä—Å–µ—Ä Drom.ru –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _make_request(self, url: str, use_cache: bool = True) -> Optional[BeautifulSoup]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º, —Ä–µ—Ç—Ä–∞—è–º–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        """
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if use_cache and self.cache:
            cached_content = self.cache.get(url)
            if cached_content:
                logger.debug(f"üì¶ –ö—ç—à: {url}")
                self.stats['requests_cached'] += 1
                return BeautifulSoup(cached_content, "html.parser")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —Å —Ä–µ—Ç—Ä–∞—è–º–∏
        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.info(f"üåê –ó–∞–ø—Ä–æ—Å: {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                
                response = self.session.get(
                    url, 
                    headers=self.headers, 
                    timeout=30,
                    allow_redirects=True
                )
                response.raise_for_status()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                if use_cache and self.cache:
                    self.cache.set(url, response.text)
                
                self.stats['requests_made'] += 1
                soup = BeautifulSoup(response.content, "html.parser")
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
                time.sleep(random.uniform(self.delay, self.delay * 1.5))
                
                return soup
                
            except requests.RequestException as e:
                logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                
                if attempt < max_retries - 1:
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ –æ—à–∏–±–∫–µ
                    error_delay = (attempt + 1) * 3
                    logger.info(f"‚è≥ –ñ–¥–µ–º {error_delay} —Å–µ–∫ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π")
                    time.sleep(error_delay)
                else:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å {url} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                    self.stats['requests_failed'] += 1
                    
        return None

    def get_brands_catalog(self) -> List[BrandInfo]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∫–∞—Ç–∞–ª–æ–≥ –≤—Å–µ—Ö –±—Ä–µ–Ω–¥–æ–≤ –∏–∑ –±–ª–æ–∫–∞ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –æ—Ç–∑—ã–≤–æ–≤
        (–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑ production_drom_parser.py)
        """
        try:
            logger.info("üìã –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ –±—Ä–µ–Ω–¥–æ–≤")
            
            # –ü–∞—Ä—Å–∏–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç–∑—ã–≤–æ–≤
            url = f"{self.base_url}/reviews/"
            response = self.session.get(url, headers=self.headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º –±–ª–æ–∫ —Å –±—Ä–µ–Ω–¥–∞–º–∏
            brands_block = soup.find('div', {'data-ftid': 'component_cars-list'})
            if not brands_block or not hasattr(brands_block, 'find_all'):
                raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å –±—Ä–µ–Ω–¥–∞–º–∏")
            
            brands = []
            
            # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –±—Ä–µ–Ω–¥
            brand_items = brands_block.find_all('div', class_='frg44i0')
            
            for item in brand_items:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É
                    link = item.find('a', {'data-ftid': 'component_cars-list-item_hidden-link'})
                    if not link:
                        continue
                    
                    brand_url = link.get('href')
                    if not brand_url:
                        continue
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –±—Ä–µ–Ω–¥–∞ –∏–∑ URL
                    url_name = brand_url.rstrip('/').split('/')[-1]
                    
                    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è
                    name_span = item.find('span', {'data-ftid': 'component_cars-list-item_name'})
                    if not name_span:
                        continue
                    
                    brand_name = name_span.get_text(strip=True)
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤
                    counter_span = item.find('span', {'data-ftid': 'component_cars-list-item_counter'})
                    reviews_count = 0
                    if counter_span:
                        counter_text = counter_span.get_text(strip=True)
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                        count_match = re.search(r'\d+', counter_text.replace(' ', ''))
                        if count_match:
                            reviews_count = int(count_match.group())
                    
                    # –ü–æ–ª–Ω—ã–π URL –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if not brand_url.startswith('http'):
                        brand_url = f"{self.base_url}{brand_url}"
                    
                    brand_info = BrandInfo(
                        name=brand_name,
                        url=brand_url,
                        reviews_count=reviews_count,
                        url_name=url_name
                    )
                    brands.append(brand_info)
                    
                    logger.debug(f"–ù–∞–π–¥–µ–Ω –±—Ä–µ–Ω–¥: {brand_name} ({reviews_count} –æ—Ç–∑—ã–≤–æ–≤)")
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –±—Ä–µ–Ω–¥–∞: {e}")
                    continue
            
            time.sleep(self.delay)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–æ—Å–ª–µ –∑–∞–ø—Ä–æ—Å–∞
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(brands)} –±—Ä–µ–Ω–¥–æ–≤")
            return brands
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞ –±—Ä–µ–Ω–¥–æ–≤: {e}")
            return []

    def get_models_for_brand(self, brand: BrandInfo) -> List[ModelInfo]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞
        (–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑ production_drom_parser.py)
        """
        try:
            logger.info(f"üè≠ –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞ {brand.name}")
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –±—Ä–µ–Ω–¥–∞
            time.sleep(self.delay)
            response = self.session.get(brand.url, headers=self.headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            models = []
            
            # –ò—â–µ–º –±–ª–æ–∫ —Å –º–æ–¥–µ–ª—è–º–∏ - —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
            model_items = soup.find_all('a', href=re.compile(rf'/reviews/{brand.url_name}/[^/]+/?$'))
            
            for item in model_items:
                try:
                    model_url = item.get('href')
                    if not model_url:
                        continue
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑ URL
                    url_parts = model_url.rstrip('/').split('/')
                    if len(url_parts) < 4:
                        continue
                    
                    model_url_name = url_parts[-1]
                    
                    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏
                    model_name = item.get_text(strip=True)
                    if not model_name:
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π URL
                    if not model_url.startswith('http'):
                        full_model_url = f"{self.base_url}{model_url}"
                    else:
                        full_model_url = model_url
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–∞ –º–æ–¥–µ–ª—å –µ—â–µ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω–∞
                    if any(m.url_name == model_url_name for m in models):
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
                    long_count, short_count = self.get_review_counts_for_model_url(full_model_url)
                    
                    model_info = ModelInfo(
                        name=model_name,
                        brand=brand.name,
                        url=full_model_url,
                        long_reviews_count=long_count,
                        short_reviews_count=short_count,
                        url_name=model_url_name
                    )
                    models.append(model_info)
                    
                    logger.debug(f"üöó –ú–æ–¥–µ–ª—å: {model_name} (–¥–ª–∏–Ω–Ω—ã—Ö: {long_count}, –∫–æ—Ä–æ—Ç–∫–∏—Ö: {short_count})")
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –º–æ–¥–µ–ª–∏: {e}")
                    continue
            
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π –¥–ª—è {brand.name}")
            return models
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è {brand.name}: {e}")
            return []

    def get_review_counts_for_model_url(self, model_url: str) -> Tuple[int, int]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –ø–æ URL"""
        if not model_url.startswith('http'):
            full_url = urljoin(self.base_url, model_url)
        else:
            full_url = model_url
            
        soup = self._make_request(full_url)
        if not soup:
            return 0, 0
        
        long_reviews_count = 0
        short_reviews_count = 0
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É –∏–∑ production_drom_parser.py
            # –ò—â–µ–º –∫–Ω–æ–ø–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–µ–∂–¥—É –¥–ª–∏–Ω–Ω—ã–º–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏
            tabs_block = soup.find('div', class_='_65ykvx0')
            if tabs_block and hasattr(tabs_block, 'find'):
                # –ö–Ω–æ–ø–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
                long_reviews_tab = tabs_block.find('a', {'data-ftid': 'reviews_tab_button_long_reviews'})
                if long_reviews_tab and hasattr(long_reviews_tab, 'get_text'):
                    text = long_reviews_tab.get_text(strip=True)
                    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏–∑ —á–∏—Å–ª–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                    match = re.search(r'(\d+)', text.replace(' ', ''))
                    if match:
                        long_reviews_count = int(match.group(1))
                
                # –ö–Ω–æ–ø–∫–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤
                short_reviews_tab = tabs_block.find('a', {'data-ftid': 'reviews_tab_button_short_reviews'})
                if short_reviews_tab and hasattr(short_reviews_tab, 'get_text'):
                    text = short_reviews_tab.get_text(strip=True)
                    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏–∑ —á–∏—Å–ª–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                    match = re.search(r'(\d+)', text.replace(' ', ''))
                    if match:
                        short_reviews_count = int(match.group(1))
            
            logger.debug(f"üìä {full_url}: {long_reviews_count} –¥–ª–∏–Ω–Ω—ã—Ö, {short_reviews_count} –∫–æ—Ä–æ—Ç–∫–∏—Ö")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥—Å—á–µ—Ç–µ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {full_url}: {e}")
        
        return long_reviews_count, short_reviews_count

    def parse_long_reviews(self, model: ModelInfo, limit: Optional[int] = None) -> List[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ —Å –ø–æ–ª—É—á–µ–Ω–∏–µ–º URL –∏ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º"""
        logger.info(f"üìù –ü–∞—Ä—Å–∏–Ω–≥ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
        
        reviews = []
        
        # –ü–æ–ª—É—á–∞–µ–º URL –æ—Ç–∑—ã–≤–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–∞
        review_urls = self._get_review_urls_from_list_page(model)
        
        if not review_urls:
            logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ URL –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
            return reviews
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(review_urls)} URL –æ—Ç–∑—ã–≤–æ–≤")
        
        # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –æ—Ç–∑—ã–≤ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ
        for i, review_url in enumerate(review_urls):
            if limit and i >= limit:
                break
                
            logger.debug(f"–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–∞ {i+1}/{len(review_urls)}: {review_url}")
            
            review = self._parse_individual_review_page(review_url, model)
            if review:
                reviews.append(review)
                self.stats['reviews_parsed'] += 1
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(self.delay)
        
        logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(reviews)} –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤")
        return reviews

    def _get_review_urls_from_list_page(self, model: ModelInfo) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –æ—Ç–∑—ã–≤–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–∞ –æ—Ç–∑—ã–≤–æ–≤"""
        try:
            # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏  
            if not model.url.startswith('http'):
                url = urljoin(self.base_url, model.url)
            else:
                url = model.url
                
            soup = self._make_request(url)
            if not soup:
                return []
            
            review_urls = []
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ç–∑—ã–≤—ã –≤ HTML
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link['href']
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –æ—Ç–∑—ã–≤ (—Å–æ–¥–µ—Ä–∂–∏—Ç ID –æ—Ç–∑—ã–≤–∞ –≤ –∫–æ–Ω—Ü–µ)
                if '/reviews/' in href and href.count('/') >= 4:
                    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π URL
                    if href.startswith('http'):
                        full_url = href
                    else:
                        full_url = urljoin(self.base_url, href)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç URL
                    url_parts = href.rstrip('/').split('/')
                    if url_parts and url_parts[-1].isdigit():
                        review_urls.append(full_url)
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            review_urls = list(set(review_urls))
            
            logger.debug(f"–ù–∞–π–¥–µ–Ω–æ {len(review_urls)} URL –æ—Ç–∑—ã–≤–æ–≤")
            return review_urls
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è URL –æ—Ç–∑—ã–≤–æ–≤: {e}")
            return []
            
        logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(reviews)} –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤")
        return reviews

    def _parse_individual_review_page(self, review_url: str, model: ModelInfo) -> Optional[ReviewData]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞ —Å –ø–æ–ª–Ω—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏"""
        try:
            soup = self._make_request(review_url)
            if not soup:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –æ—Ç–∑—ã–≤–∞ –∏–∑ URL
            review_id = review_url.rstrip('/').split('/')[-1]
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞
            review_data = {
                'review_id': review_id,
                'brand': model.brand.lower(),
                'model': model.url_name,
                'review_type': 'long',
                'url': review_url,
                'photos': [],
                'photos_count': 0,
                'car_characteristics': {},
                'detailed_ratings': {},
                'experience_info': {},
                'pros': None,
                'cons': None
            }
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = soup.select_one('h1') or soup.find('title')
            if title_elem:
                review_data['title'] = title_elem.get_text(strip=True)
            
            # –ê–≤—Ç–æ—Ä (–±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä)
            author_elem = soup.select_one('[itemprop="author"]')
            if not author_elem:
                author_elem = soup.select_one('.reviewer [itemprop="name"]')
            if author_elem:
                review_data['author'] = author_elem.get_text(strip=True)
            
            # –†–µ–π—Ç–∏–Ω–≥
            rating_elem = soup.select_one('[itemprop="ratingValue"]')
            if rating_elem:
                try:
                    rating_value = rating_elem.get('content') or rating_elem.get_text(strip=True)
                    if isinstance(rating_value, str):
                        review_data['rating'] = float(rating_value)
                    else:
                        review_data['rating'] = 0
                except (ValueError, TypeError):
                    review_data['rating'] = 0
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç–∑—ã–≤–∞
            content_elem = soup.select_one('[itemprop="reviewBody"]')
            if content_elem:
                review_data['content'] = content_elem.get_text(strip=True)
            
            # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            date_elem = soup.select_one('[itemprop="datePublished"]')
            if date_elem:
                review_data['date'] = date_elem.get('content') or date_elem.get_text(strip=True)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏–∑ Schema.org
            car_characteristics = {}
            
            # –ë—Ä–µ–Ω–¥
            brand_elem = soup.select_one('[itemprop="brand"]')
            if brand_elem:
                car_characteristics['brand'] = brand_elem.get_text(strip=True)
            
            # –ú–æ–¥–µ–ª—å
            model_elem = soup.select_one('[itemprop="model"]')
            if model_elem:
                car_characteristics['model'] = model_elem.get_text(strip=True)
            
            # –ì–æ–¥ –≤—ã–ø—É—Å–∫–∞
            year_elem = soup.select_one('[itemprop="vehicleModelDate"]')
            if year_elem:
                car_characteristics['year'] = year_elem.get_text(strip=True)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã
            tables = soup.select('table')
            for table in tables:
                rows = table.select('tr')
                for row in rows:
                    cells = row.select('td, th')
                    if len(cells) >= 2:
                        key = cells[0].get_text(strip=True).replace(':', '').lower()
                        value = cells[1].get_text(strip=True)
                        
                        # –ú–∞–ø–∏–º –∫–ª—é—á–∏ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                        key_mapping = {
                            '–≥–æ–¥ –≤—ã–ø—É—Å–∫–∞': 'year',
                            '—Ç–∏–ø –∫—É–∑–æ–≤–∞': 'body_type', 
                            '—Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è': 'transmission',
                            '–ø—Ä–∏–≤–æ–¥': 'drive_type',
                            '–¥–≤–∏–≥–∞—Ç–µ–ª—å': 'engine',
                            '–æ–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è': 'engine_volume',
                            '–º–æ—â–Ω–æ—Å—Ç—å': 'engine_power',
                            '—Ç–æ–ø–ª–∏–≤–æ': 'fuel_type',
                            '—Ä–∞—Å—Ö–æ–¥ —Ç–æ–ø–ª–∏–≤–∞': 'fuel_consumption',
                            '—Ä–∞–∑–≥–æ–Ω –¥–æ 100': 'acceleration',
                            '–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å': 'max_speed'
                        }
                        
                        mapped_key = key_mapping.get(key, key)
                        if value and value != '-':
                            car_characteristics[mapped_key] = value
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–≤—Ç–æ–º–æ–±–∏–ª–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞
            all_text = soup.get_text()
            
            # –ü–æ–∏—Å–∫ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–≤–∏–≥–∞—Ç–µ–ª—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ)
            engine_patterns = [
                (r'(\d+[.,]\d+)\s*(?:–ª|–ª–∏—Ç—Ä–∞|–ª–∏—Ç—Ä–æ–≤)', 'engine_volume_text'),
                (r'(\d+)\s*–ª\.?—Å\.?', 'engine_power_text'),
                (r'(\d+)\s*–∫—É–±[.,]\s*—Å–º', 'engine_displacement'),
                (r'V(\d+)', 'engine_cylinders'),
                (r'(\d{4})\s*–∫—É–±\.—Å–º', 'engine_displacement_full')
            ]
            
            for pattern, key in engine_patterns:
                match = re.search(pattern, all_text, re.IGNORECASE)
                if match and key not in car_characteristics:
                    car_characteristics[key] = match.group(1)
            
            # –ü–æ–∏—Å–∫ —Ç–∏–ø–∞ –∫–æ—Ä–æ–±–∫–∏ –ø–µ—Ä–µ–¥–∞—á –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–∞–±–ª–∏—Ü–µ)
            if 'transmission' not in car_characteristics:
                transmission_patterns = [
                    r'(–∞–≤—Ç–æ–º–∞—Ç|–ê–ö–ü–ü|–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è)',
                    r'(–º–µ—Ö–∞–Ω–∏–∫–∞|–ú–ö–ü–ü|–º–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∞—è)',
                    r'(–≤–∞—Ä–∏–∞—Ç–æ—Ä|CVT)',
                    r'(—Ä–æ–±–æ—Ç|—Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)'
                ]
                
                for pattern in transmission_patterns:
                    match = re.search(pattern, all_text, re.IGNORECASE)
                    if match:
                        car_characteristics['transmission'] = match.group(1)
                        break
            
            # –ü–æ–∏—Å–∫ —Ç–∏–ø–∞ –ø—Ä–∏–≤–æ–¥–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–∞–±–ª–∏—Ü–µ)
            if 'drive_type' not in car_characteristics:
                drive_patterns = [
                    r'(4WD|4–≤–¥|–ø–æ–ª–Ω—ã–π –ø—Ä–∏–≤–æ–¥|AWD)',
                    r'(2WD|2–≤–¥|–ø–µ—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–≤–æ–¥|FWD)',
                    r'(–∑–∞–¥–Ω–∏–π –ø—Ä–∏–≤–æ–¥|RWD)'
                ]
                
                for pattern in drive_patterns:
                    match = re.search(pattern, all_text, re.IGNORECASE)
                    if match:
                        car_characteristics['drive_type'] = match.group(1)
                        break
            
            # –ü–æ–∏—Å–∫ —Ä–∞—Å—Ö–æ–¥–∞ —Ç–æ–ø–ª–∏–≤–∞
            fuel_patterns = [
                r'—Ä–∞—Å—Ö–æ–¥.*?(\d+[.,]\d+).*?–ª',
                r'(\d+[.,]\d+).*?–ª.*?100.*?–∫–º',
                r'–≥–æ—Ä–æ–¥.*?(\d+[.,]\d+).*?–ª–∏—Ç—Ä',
                r'—Ç—Ä–∞—Å—Å–∞.*?(\d+[.,]\d+).*?–ª–∏—Ç—Ä'
            ]
            
            fuel_consumption = []
            for pattern in fuel_patterns:
                matches = re.findall(pattern, all_text, re.IGNORECASE)
                fuel_consumption.extend(matches)
            
            if fuel_consumption and 'fuel_consumption' not in car_characteristics:
                car_characteristics['fuel_consumption'] = ', '.join(fuel_consumption[:3])  # –û–≥—Ä–∞–Ω–∏—á–∏–º 3 –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            
            # –ü–æ–∏—Å–∫ –ø—Ä–æ–±–µ–≥–∞
            mileage_patterns = [
                r'–ø—Ä–æ–±–µ–≥.*?(\d+[.,]?\d*)\s*(?:–∫–º|—Ç—ã—Å|—Ç—ã—Å—è—á)',
                r'(\d+[.,]?\d*)\s*(?:–∫–º|—Ç—ã—Å|—Ç—ã—Å—è—á).*?–ø—Ä–æ–±–µ–≥',
                r'(\d+)\s*–º–∏–ª—å'
            ]
            
            for pattern in mileage_patterns:
                match = re.search(pattern, all_text, re.IGNORECASE)
                if match:
                    car_characteristics['mileage'] = match.group(1)
                    break
            
            review_data['car_characteristics'] = car_characteristics
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–π—Ç–∏–Ω–≥–∏ - –ø–æ–∏—Å–∫ –≤—Å–µ—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –∏–∑ Schema.org
            detailed_ratings = {}
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏
            rating_elements = soup.select('[itemprop="ratingValue"]')
            for i, elem in enumerate(rating_elements):
                try:
                    rating_value = elem.get('content') or elem.get_text(strip=True)
                    if isinstance(rating_value, str) and rating_value.replace('.', '').replace(',', '').isdigit():
                        rating_float = float(rating_value.replace(',', '.'))
                        detailed_ratings[f'rating_{i+1}'] = rating_float
                except (ValueError, TypeError):
                    continue
            
            # –ü–æ–∏—Å–∫ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ (—Ü–∏—Ñ—Ä—ã –æ—Ç 1 –¥–æ 10 —Å –¥–µ—Å—è—Ç–∏—á–Ω—ã–º–∏)
            rating_text_patterns = [
                r'–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å.*?(\d+[.,]\d+)',
                r'–∫–æ–º—Ñ–æ—Ä—Ç.*?(\d+[.,]\d+)',
                r'–¥–∏–Ω–∞–º–∏–∫–∞.*?(\d+[.,]\d+)',
                r'–¥–∏–∑–∞–π–Ω.*?(\d+[.,]\d+)',
                r'–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å.*?(\d+[.,]\d+)',
                r'—ç–∫–æ–Ω–æ–º–∏—è.*?(\d+[.,]\d+)',
                r'—É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å.*?(\d+[.,]\d+)',
                r'–∫–∞—á–µ—Å—Ç–≤–æ.*?(\d+[.,]\d+)'
            ]
            
            for pattern in rating_text_patterns:
                match = re.search(pattern, all_text, re.IGNORECASE)
                if match:
                    try:
                        rating_value = float(match.group(1).replace(',', '.'))
                        if 1 <= rating_value <= 10:
                            category = pattern.split('.*?')[0]
                            detailed_ratings[category] = rating_value
                    except (ValueError, TypeError):
                        continue
            
            # –ü–æ–∏—Å–∫ –±–ª–æ–∫–æ–≤ —Å –æ—Ü–µ–Ω–∫–∞–º–∏ (—á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–∞—Ö)
            rating_blocks = soup.select('.b-info-block__image_centred-content, .rating-value, .score, [class*="rating"]')
            for i, block in enumerate(rating_blocks):
                rating_text = block.get_text(strip=True)
                if re.match(r'^\d+[.,]?\d*$', rating_text):
                    try:
                        rating_value = float(rating_text.replace(',', '.'))
                        if 1 <= rating_value <= 10:
                            detailed_ratings[f'block_rating_{i+1}'] = rating_value
                    except (ValueError, TypeError):
                        continue
            
            review_data['detailed_ratings'] = detailed_ratings
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–ø—ã—Ç–µ –≤–ª–∞–¥–µ–Ω–∏—è
            experience_info = {}
            
            # –ü–æ–∏—Å–∫ –≥–æ–¥–∞ –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏—è
            acquisition_match = re.search(r'(?:–ø—Ä–∏–æ–±—Ä–µ—Ç|–∫—É–ø–∏–ª|–≤–∑—è–ª).*?(\d{4})', all_text, re.IGNORECASE)
            if acquisition_match:
                experience_info['acquisition_year'] = acquisition_match.group(1)
            
            # –ü–æ–∏—Å–∫ —Å—Ä–æ–∫–∞ –≤–ª–∞–¥–µ–Ω–∏—è
            ownership_patterns = [
                r'–≤–ª–∞–¥–µ—é.*?(\d+).*?(?:–≥–æ–¥|–ª–µ—Ç)',
                r'–∫–∞—Ç–∞—é—Å—å.*?(\d+).*?(?:–≥–æ–¥|–ª–µ—Ç)',
                r'–µ–∑–∂—É.*?(\d+).*?(?:–≥–æ–¥|–ª–µ—Ç)'
            ]
            
            for pattern in ownership_patterns:
                match = re.search(pattern, all_text, re.IGNORECASE)
                if match:
                    experience_info['ownership_period'] = match.group(1)
                    break
            
            review_data['experience_info'] = experience_info
            
            # –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            photos = []
            img_elements = soup.select('img')
            for img in img_elements:
                src = img.get('src') or img.get('data-src') or img.get('srcset')
                if src and ('photo' in src or 'auto.drom.ru' in src or 's.auto.drom.ru' in src):
                    if isinstance(src, str) and src.startswith('http'):
                        photos.append(src)
            
            review_data['photos'] = photos
            review_data['photos_count'] = len(photos)
            
            logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω –æ—Ç–∑—ã–≤: {review_data.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')[:50]} —Å {len(car_characteristics)} —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏")
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç ReviewData
            return ReviewData(
                review_id=review_data['review_id'],
                brand=review_data['brand'],
                model=review_data['model'],
                review_type=review_data['review_type'],
                url=review_data['url'],
                title=review_data.get('title'),
                author=review_data.get('author'),
                rating=review_data.get('rating'),
                content=review_data.get('content'),
                date=review_data.get('date'),
                photos=review_data['photos'],
                photos_count=review_data['photos_count']
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –æ—Ç–∑—ã–≤–∞ {review_url}: {e}")
            return None

    def parse_short_reviews(self, model: ModelInfo, limit: Optional[int] = None) -> List[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤"""
        logger.info(f"üí≠ –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
        
        reviews = []
        page = 1
        max_pages = 10
        
        while page <= max_pages:
            if limit and len(reviews) >= limit:
                break
                
            # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏
            if not model.url.startswith('http'):
                base_url = urljoin(self.base_url, model.url)
            else:
                base_url = model.url
                
            url = f"{base_url}5kopeek/"
            if page > 1:
                url += f"?page={page}"
                
            soup = self._make_request(url)
            if not soup:
                break
                
            # –ü–æ–∏—Å–∫ –±–ª–æ–∫–æ–≤ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤
            review_blocks = soup.find_all("div", {"data-ftid": "short-review-item"})
            
            if not review_blocks:
                logger.info(f"üìÑ –ù–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
                break
                
            for block in review_blocks:
                if limit and len(reviews) >= limit:
                    break
                    
                review = self._parse_short_review_block(block, model)
                if review:
                    reviews.append(review)
                    self.stats['reviews_parsed'] += 1
                    
            page += 1
            
        logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(reviews)} –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤")
        return reviews

    def _parse_long_review_block(self, block, model: ModelInfo) -> Optional[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞ —Å —Ä–∞–±–æ—á–∏–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º ID –æ—Ç–∑—ã–≤–∞ –∏–∑ data-ga-stats-va-payload
            payload_str = block.get('data-ga-stats-va-payload', '{}')
            review_id = ''
            import json
            try:
                payload_data = json.loads(payload_str)
                review_id = str(payload_data.get('review_id', ''))
            except (json.JSONDecodeError, KeyError):
                pass
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ payload, –±–µ—Ä–µ–º –∏–∑ id –∞—Ç—Ä–∏–±—É—Ç–∞
            if not review_id:
                review_id = block.get('id', '')
            
            # URL –æ—Ç–∑—ã–≤–∞
            review_url = f"{model.url}{review_id}/"
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞
            review_data = {
                'review_id': review_id,
                'brand': model.brand.lower(),
                'model': model.url_name,
                'review_type': 'long',
                'url': review_url,
                'photos': [],
                'photos_count': 0
            }
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ - –∏—â–µ–º h3 –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            title = ""
            title_elem = block.find("h3")
            if not title_elem:
                title_elem = block.find("a", {"data-ftid": "component_review_title"})
            if title_elem:
                title = title_elem.get_text(strip=True)
            review_data["title"] = title
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç - —Å–æ–±–∏—Ä–∞–µ–º –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Å–µ–∫—Ü–∏–π –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–±–æ—á—É—é –ª–æ–≥–∏–∫—É
            content_parts = []
            
            # –ü–ª—é—Å—ã
            positive_elem = block.find("div", {"data-ftid": "review-content__positive"})
            if positive_elem:
                positive_text = positive_elem.get_text(strip=True)
                if positive_text:
                    content_parts.append(f"–ü–ª—é—Å—ã: {positive_text}")
            
            # –ú–∏–Ω—É—Å—ã
            negative_elem = block.find("div", {"data-ftid": "review-content__negative"})
            if negative_elem:
                negative_text = negative_elem.get_text(strip=True)
                if negative_text:
                    content_parts.append(f"–ú–∏–Ω—É—Å—ã: {negative_text}")
            
            # –ü–æ–ª–æ–º–∫–∏
            breakages_elem = block.find("div", {"data-ftid": "review-content__breakages"})
            if breakages_elem:
                breakages_text = breakages_elem.get_text(strip=True)
                if breakages_text:
                    content_parts.append(f"–ü–æ–ª–æ–º–∫–∏: {breakages_text}")
            
            # –î—Ä—É–≥–∏–µ —Å–µ–∫—Ü–∏–∏ —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
            content_sections = block.find_all("div", class_="css-6hj46s")
            for section in content_sections:
                text = section.get_text(strip=True)
                if text and text not in [part.split(': ', 1)[-1] for part in content_parts]:
                    content_parts.append(text)
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ–∫—Ü–∏—è—Ö, –ø–æ–ø—Ä–æ–±—É–µ–º –æ–±—â–∏–µ –∫–ª–∞—Å—Å—ã
            if not content_parts:
                general_content = block.find("div", class_="hxiweg0")
                if general_content:
                    content_parts.append(general_content.get_text(strip=True))
            
            review_data["content"] = "\n".join(content_parts).strip()
            
            # –ê–≤—Ç–æ—Ä –∏ –ª–æ–∫–∞—Ü–∏—è
            description_div = block.find("div", {"data-ftid": "component_review_descrption"})
            if description_div:
                author_span = description_div.find("span", class_="_1ngifes0")
                if author_span:
                    spans = author_span.find_all("span")
                    if spans:
                        review_data["author"] = spans[0].get_text(strip=True)
                    # –ì–æ—Ä–æ–¥ –æ–±—ã—á–Ω–æ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
                    full_text = author_span.get_text(strip=True)
                    if "," in full_text:
                        parts = full_text.split(",")
                        if len(parts) > 1:
                            review_data["city"] = parts[1].strip()
            
            # –†–µ–π—Ç–∏–Ω–≥
            rating = 0
            rating_elem = block.find("div", {"data-ftid": "component_rating"})
            if rating_elem:
                rating_text = rating_elem.get_text(strip=True)
                rating_match = re.search(r'(\d+(?:\.\d+)?)', rating_text)
                if rating_match:
                    try:
                        rating = float(rating_match.group(1))
                    except ValueError:
                        pass
            review_data["rating"] = rating
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–±–æ—á—É—é –ª–æ–≥–∏–∫—É
            photos = block.find_all("img")
            photo_urls = []
            for img in photos:
                src = img.get('src') or img.get('data-src')
                if src and ('photo' in src or 'auto.drom.ru' in src):
                    photo_urls.append(src)
            review_data["photos"] = photo_urls
            review_data["photos_count"] = len(photo_urls)
            
            # –°–æ–∑–¥–∞–µ–º ReviewData
            return ReviewData(**review_data)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞: {e}")
            return None

    def _parse_short_review_block(self, block, model: ModelInfo) -> Optional[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–∫–∞ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º ID –æ—Ç–∑—ã–≤–∞
            review_id = block.get('id', '')
            
            # URL –æ—Ç–∑—ã–≤–∞
            review_url = f"{model.url}5kopeek/{review_id}/"
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞
            review_data = {
                'review_id': review_id,
                'brand': model.brand.lower(),
                'model': model.url_name,
                'review_type': 'short',
                'url': review_url
            }
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–≤—Ç–æ—Ä–µ
            author_elem = block.find("span", class_="css-1u4ddp")
            if author_elem:
                review_data["author"] = author_elem.get_text(strip=True)
            
            # –ì–æ—Ä–æ–¥
            city_elem = block.find("span", {"data-ftid": "short-review-city"})
            if city_elem:
                review_data["city"] = city_elem.get_text(strip=True)
            
            # –ü–∞—Ä—Å–∏–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_div = block.find('div', {'data-ftid': 'short-review-item__title'})
            if title_div:
                self._extract_car_specs_from_title(title_div, review_data)
            
            # –ü–ª—é—Å—ã
            positive_elem = block.find("div", {"data-ftid": "short-review-content__positive"})
            if positive_elem:
                review_data["positive_text"] = positive_elem.get_text(strip=True)
            
            # –ú–∏–Ω—É—Å—ã
            negative_elem = block.find("div", {"data-ftid": "short-review-content__negative"})
            if negative_elem:
                review_data["negative_text"] = negative_elem.get_text(strip=True)
                
            # –ü–æ–ª–æ–º–∫–∏
            breakages_elem = block.find("div", {"data-ftid": "short-review-content__breakages"})
            if breakages_elem:
                review_data["breakages_text"] = breakages_elem.get_text(strip=True)
            
            # –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            photo_divs = block.find_all('div', class_='_1gzw4372')
            photo_urls = []
            for photo_div in photo_divs:
                img = photo_div.find('img')
                if img and img.get('src'):
                    photo_urls.append(img['src'])
            review_data["photos"] = photo_urls
            
            # –°–æ–∑–¥–∞–µ–º ReviewData
            return ReviewData(**review_data)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞: {e}")
            return None

    def _extract_car_specs_from_title(self, title_div, review_data: dict):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞"""
        try:
            text = title_div.get_text(strip=True)
            
            # –ì–æ–¥
            year_span = title_div.find('span', {'data-ftid': 'short-review-item__year'})
            if year_span:
                year_text = year_span.get_text(strip=True)
                try:
                    review_data['year'] = int(year_text)
                except ValueError:
                    pass
            
            # –û–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è
            volume_span = title_div.find('span', {'data-ftid': 'short-review-item__volume'})
            if volume_span:
                volume_text = volume_span.get_text(strip=True)
                try:
                    review_data['engine_volume'] = float(volume_text)
                except ValueError:
                    pass
            
            # –¢–∏–ø —Ç–æ–ø–ª–∏–≤–∞
            if '–±–µ–Ω–∑–∏–Ω' in text:
                review_data['fuel_type'] = '–±–µ–Ω–∑–∏–Ω'
            elif '–¥–∏–∑–µ–ª—å' in text:
                review_data['fuel_type'] = '–¥–∏–∑–µ–ª—å'
            elif '–≥–∏–±—Ä–∏–¥' in text:
                review_data['fuel_type'] = '–≥–∏–±—Ä–∏–¥'
            elif '—ç–ª–µ–∫—Ç—Ä–æ' in text:
                review_data['fuel_type'] = '—ç–ª–µ–∫—Ç—Ä–æ'
            
            # –ö–æ—Ä–æ–±–∫–∞ –ø–µ—Ä–µ–¥–∞—á
            if '–∞–≤—Ç–æ–º–∞—Ç' in text:
                review_data['transmission'] = '–∞–≤—Ç–æ–º–∞—Ç'
            elif '–º–µ—Ö–∞–Ω–∏–∫–∞' in text:
                review_data['transmission'] = '–º–µ—Ö–∞–Ω–∏–∫–∞'
                
            # –ü—Ä–∏–≤–æ–¥
            if '–ø–µ—Ä–µ–¥–Ω–∏–π' in text:
                review_data['drive_type'] = '–ø–µ—Ä–µ–¥–Ω–∏–π'
            elif '–∑–∞–¥–Ω–∏–π' in text:
                review_data['drive_type'] = '–∑–∞–¥–Ω–∏–π'
            elif '–ø–æ–ª–Ω—ã–π' in text:
                review_data['drive_type'] = '–ø–æ–ª–Ω—ã–π'
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {e}")

    def parse_model_reviews(self, 
                          model: ModelInfo, 
                          max_long_reviews: Optional[int] = None,
                          max_short_reviews: Optional[int] = None) -> List[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ (–¥–ª–∏–Ω–Ω—ã—Ö –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö)"""
        logger.info(f"üéØ –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
        
        all_reviews = []
        
        # –ü–∞—Ä—Å–∏–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
        if model.long_reviews_count > 0:
            long_reviews = self.parse_long_reviews(model, limit=max_long_reviews)
            all_reviews.extend(long_reviews)
            
        # –ü–∞—Ä—Å–∏–º –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–∑—ã–≤—ã
        if model.short_reviews_count > 0:
            short_reviews = self.parse_short_reviews(model, limit=max_short_reviews)
            all_reviews.extend(short_reviews)
            
        logger.info(f"‚úÖ –í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(all_reviews)} –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
        return all_reviews

    def save_to_json(self, reviews: List[ReviewData], filename: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –≤ JSON —Ñ–∞–π–ª"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä–∏
            reviews_data = [asdict(review) for review in reviews]
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º datetime –¥–ª—è JSON
            for review_data in reviews_data:
                if 'parsed_at' in review_data and review_data['parsed_at']:
                    review_data['parsed_at'] = review_data['parsed_at'].isoformat()
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(reviews_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤ –≤ {filename}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ JSON: {e}")

    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞"""
        return {
            'base_url': self.base_url,
            'cache_enabled': self.enable_cache,
            'delay': self.delay,
            **self.stats
        }

    def parse_limited_demo(self, 
                          max_brands: int = 3,
                          max_long_reviews: int = 3,
                          max_short_reviews: int = 10) -> Dict[str, Any]:
        """
        üéØ –î–ï–ú–û-–ü–ê–†–°–ò–ù–ì —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –¥–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥–∞: {max_brands} –±—Ä–µ–Ω–¥–æ–≤, {max_long_reviews} –¥–ª–∏–Ω–Ω—ã—Ö, {max_short_reviews} –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤")
        
        start_time = datetime.now()
        results = {
            'start_time': start_time.isoformat(),
            'brands_processed': [],
            'total_reviews': 0,
            'total_long_reviews': 0,
            'total_short_reviews': 0,
            'errors': []
        }
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –±—Ä–µ–Ω–¥–æ–≤
            brands = self.get_brands_catalog()
            if not brands:
                results['errors'].append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –±—Ä–µ–Ω–¥–æ–≤")
                return results
            
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –±—Ä–µ–Ω–¥—ã
            test_brands = brands[:max_brands]
            
            all_reviews = []
            
            for brand in test_brands:
                try:
                    logger.info(f"üè≠ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±—Ä–µ–Ω–¥: {brand.name}")
                    
                    # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
                    models = self.get_models_for_brand(brand)
                    if not models:
                        logger.warning(f"‚ö†Ô∏è  –ù–µ—Ç –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞ {brand.name}")
                        continue
                    
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å —Å –æ—Ç–∑—ã–≤–∞–º–∏ - –ù–ï –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –ø–æ–¥—Ä—è–¥ –¥–ª—è –¥–µ–º–æ
                    target_model = None
                    for model in models[:5]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –º–æ–¥–µ–ª–µ–π
                        if model.long_reviews_count > 0 or model.short_reviews_count > 0:
                            target_model = model
                            break
                    
                    if not target_model:
                        logger.warning(f"‚ö†Ô∏è  –ù–µ—Ç –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∑—ã–≤–∞–º–∏ –¥–ª—è –±—Ä–µ–Ω–¥–∞ {brand.name} (–ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –ø–µ—Ä–≤—ã–µ 5)")
                        continue
                    
                    # –ü–∞—Ä—Å–∏–º –æ—Ç–∑—ã–≤—ã
                    model_reviews = self.parse_model_reviews(
                        target_model,
                        max_long_reviews=max_long_reviews,
                        max_short_reviews=max_short_reviews
                    )
                    
                    all_reviews.extend(model_reviews)
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±—Ä–µ–Ω–¥—É
                    brand_stats = {
                        'brand': brand.name,
                        'model': target_model.name,
                        'long_reviews_available': target_model.long_reviews_count,
                        'short_reviews_available': target_model.short_reviews_count,
                        'long_reviews_parsed': len([r for r in model_reviews if r.review_type == 'long']),
                        'short_reviews_parsed': len([r for r in model_reviews if r.review_type == 'short']),
                        'total_parsed': len(model_reviews)
                    }
                    
                    results['brands_processed'].append(brand_stats)
                    
                except Exception as e:
                    error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±—Ä–µ–Ω–¥–∞ {brand.name}: {e}"
                    logger.error(f"‚ùå {error_msg}")
                    results['errors'].append(error_msg)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            results['total_reviews'] = len(all_reviews)
            results['total_long_reviews'] = len([r for r in all_reviews if r.review_type == 'long'])
            results['total_short_reviews'] = len([r for r in all_reviews if r.review_type == 'short'])
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = (datetime.now() - start_time).total_seconds()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            json_filename = f"data/master_parser_demo_{timestamp}.json"
            self.save_to_json(all_reviews, json_filename)
            results['saved_to'] = json_filename
            
            logger.info(f"üéâ –î–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {results['total_reviews']} –æ—Ç–∑—ã–≤–æ–≤ –∑–∞ {results['duration_seconds']:.1f} —Å–µ–∫")
            
        except Exception as e:
            error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –¥–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥–µ: {e}"
            logger.error(f"‚ùå {error_msg}")
            results['errors'].append(error_msg)
        
        return results


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Ç–µ—Ä-–ø–∞—Ä—Å–µ—Ä–∞
    parser = MasterDromParser(
        delay=1.0,
        cache_dir="data/cache",
        enable_cache=True
    )
    
    # –ó–∞–ø—É—Å–∫ –¥–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥–∞
    results = parser.parse_limited_demo(
        max_brands=3,
        max_long_reviews=3, 
        max_short_reviews=10
    )
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("üöó –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–ê–°–¢–ï–†-–ü–ê–†–°–ï–†–ê:")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±—Ä–µ–Ω–¥–æ–≤: {len(results['brands_processed'])}")
    print(f"–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {results['total_reviews']}")
    print(f"–î–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤: {results['total_long_reviews']}")
    print(f"–ö–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤: {results['total_short_reviews']}")
    print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {results.get('duration_seconds', 0):.1f} —Å–µ–∫")
    
    if results['errors']:
        print(f"–û—à–∏–±–æ–∫: {len(results['errors'])}")
        for error in results['errors']:
            print(f"  - {error}")
