#!/usr/bin/env python3
"""
üöó –ú–ê–°–¢–ï–†-–ü–ê–†–°–ï–† DROM.RU - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
=============================================

–ï–î–ò–ù–´–ô –ò –ì–õ–ê–í–ù–´–ô –ü–ê–†–°–ï–† –¥–ª—è drom.ru
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å—é –ª—É—á—à—É—é –ª–æ–≥–∏–∫—É –∏–∑ –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞:

‚úÖ –î–ª–∏–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ–±–∑–æ—Ä—ã) 
‚úÖ –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–∑—ã–≤—ã (–∫—Ä–∞—Ç–∫–∏–µ –º–Ω–µ–Ω–∏—è)
‚úÖ –ö–∞—Ç–∞–ª–æ–≥ –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π
‚úÖ –ù–∞–¥–µ–∂–Ω—É—é —Å–µ—Ç–µ–≤—É—é –ª–æ–≥–∏–∫—É
‚úÖ –û–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –∏ —Ä–µ—Ç—Ä–∞–∏
‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–µ—Ç—Ä–∏–∫–∏

–ì–õ–ê–í–ù–´–ô –ü–ê–†–°–ï–† –ü–†–û–ï–ö–¢–ê - –ù–ï –°–û–ó–î–ê–í–ê–ô–¢–ï –ù–û–í–´–•!

–ê–≤—Ç–æ—Ä: AI Assistant
–î–∞—Ç–∞: 26.08.2025
"""

import json
import logging
import os
import re
import random
import time
import hashlib
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class ReviewData:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–∞"""
    review_id: str
    brand: str
    model: str
    review_type: str  # 'long' –∏–ª–∏ 'short'
    
    # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è  
    year: Optional[int] = None
    engine_volume: Optional[float] = None
    fuel_type: Optional[str] = None
    transmission: Optional[str] = None
    drive_type: Optional[str] = None
    body_type: Optional[str] = None
    
    # –î–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞
    author: Optional[str] = None
    city: Optional[str] = None
    date: Optional[str] = None
    rating: Optional[float] = None
    title: Optional[str] = None
    
    # –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–∞
    positive_text: Optional[str] = None
    negative_text: Optional[str] = None
    general_text: Optional[str] = None
    breakages_text: Optional[str] = None
    content: Optional[str] = None
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    photos: List[str] = field(default_factory=list)
    photos_count: int = 0
    url: Optional[str] = None
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    views_count: Optional[int] = None
    likes_count: Optional[int] = None
    useful_count: Optional[int] = None
    not_useful_count: Optional[int] = None
    comments_count: Optional[int] = None
    
    # –°–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è
    parsed_at: Optional[datetime] = None
    content_hash: Optional[str] = None
    
    def __post_init__(self):
        if self.parsed_at is None:
            self.parsed_at = datetime.now()
        self.content_hash = self.generate_hash()
        self.photos_count = len(self.photos) if self.photos else 0
    
    def generate_hash(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ —Ö–µ—à–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        content_for_hash = f"{self.url or ''}_{self.title or ''}_{self.content or ''}_{self.positive_text or ''}_{self.negative_text or ''}"
        return hashlib.md5(content_for_hash.encode()).hexdigest()


@dataclass
class BrandInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±—Ä–µ–Ω–¥–µ"""
    name: str
    url: str
    reviews_count: int
    url_name: str  # –∏–º—è –≤ URL, –Ω–∞–ø—Ä–∏–º–µ—Ä 'alfa_romeo' –¥–ª—è 'Alfa Romeo'


@dataclass
class ModelInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    name: str
    brand: str
    url: str
    long_reviews_count: int = 0
    short_reviews_count: int = 0
    url_name: str = ""


class SimpleCache:
    """–ü—Ä–æ—Å—Ç–æ–π —Ñ–∞–π–ª–æ–≤—ã–π –∫—ç—à"""
    def __init__(self, cache_dir: str):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def _get_cache_path(self, url: str) -> Path:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫—ç—à–∞"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return self.cache_dir / f"{url_hash}.html"
    
    def get(self, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞"""
        cache_path = self._get_cache_path(url)
        if cache_path.exists():
            try:
                return cache_path.read_text(encoding='utf-8')
            except Exception:
                return None
        return None
    
    def set(self, url: str, content: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à"""
        cache_path = self._get_cache_path(url)
        try:
            cache_path.write_text(content, encoding='utf-8')
        except Exception:
            pass


class MasterDromParser:
    """
    üöó –ú–ê–°–¢–ï–†-–ü–ê–†–°–ï–† DROM.RU - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –í–°–Æ –ª—É—á—à—É—é –ª–æ–≥–∏–∫—É –∏–∑ –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞:
    - –ü–∞—Ä—Å–∏–Ω–≥ –¥–ª–∏–Ω–Ω—ã—Ö –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤
    - –ö–∞—Ç–∞–ª–æ–≥ –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π  
    - –ù–∞–¥–µ–∂–Ω–∞—è —Å–µ—Ç–µ–≤–∞—è –ª–æ–≥–∏–∫–∞ —Å —Ä–µ—Ç—Ä–∞—è–º–∏
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
    """

    def __init__(self, 
                 delay: float = 1.0, 
                 cache_dir: str = "data/cache",
                 enable_cache: bool = True):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Ç–µ—Ä-–ø–∞—Ä—Å–µ—Ä–∞"""
        
        self.base_url = "https://www.drom.ru"
        self.delay = delay
        self.enable_cache = enable_cache
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        Path(cache_dir).mkdir(parents=True, exist_ok=True)
        Path("logs").mkdir(exist_ok=True)
        Path("data").mkdir(exist_ok=True)
        
        # –ö—ç—à
        if self.enable_cache:
            self.cache = SimpleCache(cache_dir)
        else:
            self.cache = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'requests_made': 0,
            'requests_cached': 0,
            'requests_failed': 0,
            'reviews_parsed': 0
        }
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏
        self.session = requests.Session()
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                         "AppleWebKit/537.36 (KHTML, like Gecko) "
                         "Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        logger.info("üöó –ú–∞—Å—Ç–µ—Ä-–ø–∞—Ä—Å–µ—Ä Drom.ru –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _make_request(self, url: str, use_cache: bool = True) -> Optional[BeautifulSoup]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º, —Ä–µ—Ç—Ä–∞—è–º–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        """
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if use_cache and self.cache:
            cached_content = self.cache.get(url)
            if cached_content:
                logger.debug(f"üì¶ –ö—ç—à: {url}")
                self.stats['requests_cached'] += 1
                return BeautifulSoup(cached_content, "html.parser")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —Å —Ä–µ—Ç—Ä–∞—è–º–∏
        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.info(f"üåê –ó–∞–ø—Ä–æ—Å: {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                
                response = self.session.get(
                    url, 
                    headers=self.headers, 
                    timeout=30,
                    allow_redirects=True
                )
                response.raise_for_status()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                if use_cache and self.cache:
                    self.cache.set(url, response.text)
                
                self.stats['requests_made'] += 1
                soup = BeautifulSoup(response.content, "html.parser")
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
                time.sleep(random.uniform(self.delay, self.delay * 1.5))
                
                return soup
                
            except requests.RequestException as e:
                logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                
                if attempt < max_retries - 1:
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ –æ—à–∏–±–∫–µ
                    error_delay = (attempt + 1) * 3
                    logger.info(f"‚è≥ –ñ–¥–µ–º {error_delay} —Å–µ–∫ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π")
                    time.sleep(error_delay)
                else:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å {url} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                    self.stats['requests_failed'] += 1
                    
        return None

    def get_brands_catalog(self) -> List[BrandInfo]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∫–∞—Ç–∞–ª–æ–≥ –≤—Å–µ—Ö –±—Ä–µ–Ω–¥–æ–≤ –∏–∑ –±–ª–æ–∫–∞ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –æ—Ç–∑—ã–≤–æ–≤
        (–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑ production_drom_parser.py)
        """
        try:
            logger.info("üìã –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ –±—Ä–µ–Ω–¥–æ–≤")
            
            # –ü–∞—Ä—Å–∏–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç–∑—ã–≤–æ–≤
            url = f"{self.base_url}/reviews/"
            response = self.session.get(url, headers=self.headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º –±–ª–æ–∫ —Å –±—Ä–µ–Ω–¥–∞–º–∏
            brands_block = soup.find('div', {'data-ftid': 'component_cars-list'})
            if not brands_block or not hasattr(brands_block, 'find_all'):
                raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å –±—Ä–µ–Ω–¥–∞–º–∏")
            
            brands = []
            
            # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –±—Ä–µ–Ω–¥
            brand_items = brands_block.find_all('div', class_='frg44i0')
            
            for item in brand_items:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É
                    link = item.find('a', {'data-ftid': 'component_cars-list-item_hidden-link'})
                    if not link:
                        continue
                    
                    brand_url = link.get('href')
                    if not brand_url:
                        continue
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –±—Ä–µ–Ω–¥–∞ –∏–∑ URL
                    url_name = brand_url.rstrip('/').split('/')[-1]
                    
                    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è
                    name_span = item.find('span', {'data-ftid': 'component_cars-list-item_name'})
                    if not name_span:
                        continue
                    
                    brand_name = name_span.get_text(strip=True)
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤
                    counter_span = item.find('span', {'data-ftid': 'component_cars-list-item_counter'})
                    reviews_count = 0
                    if counter_span:
                        counter_text = counter_span.get_text(strip=True)
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                        count_match = re.search(r'\d+', counter_text.replace(' ', ''))
                        if count_match:
                            reviews_count = int(count_match.group())
                    
                    # –ü–æ–ª–Ω—ã–π URL –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if not brand_url.startswith('http'):
                        brand_url = f"{self.base_url}{brand_url}"
                    
                    brand_info = BrandInfo(
                        name=brand_name,
                        url=brand_url,
                        reviews_count=reviews_count,
                        url_name=url_name
                    )
                    brands.append(brand_info)
                    
                    logger.debug(f"–ù–∞–π–¥–µ–Ω –±—Ä–µ–Ω–¥: {brand_name} ({reviews_count} –æ—Ç–∑—ã–≤–æ–≤)")
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –±—Ä–µ–Ω–¥–∞: {e}")
                    continue
            
            time.sleep(self.delay)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–æ—Å–ª–µ –∑–∞–ø—Ä–æ—Å–∞
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(brands)} –±—Ä–µ–Ω–¥–æ–≤")
            return brands
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞ –±—Ä–µ–Ω–¥–æ–≤: {e}")
            return []

    def get_models_for_brand(self, brand: BrandInfo) -> List[ModelInfo]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞
        (–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑ production_drom_parser.py)
        """
        try:
            logger.info(f"üè≠ –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞ {brand.name}")
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –±—Ä–µ–Ω–¥–∞
            time.sleep(self.delay)
            response = self.session.get(brand.url, headers=self.headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            models = []
            
            # –ò—â–µ–º –±–ª–æ–∫ —Å –º–æ–¥–µ–ª—è–º–∏ - —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
            model_items = soup.find_all('a', href=re.compile(rf'/reviews/{brand.url_name}/[^/]+/?$'))
            
            for item in model_items:
                try:
                    model_url = item.get('href')
                    if not model_url:
                        continue
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑ URL
                    url_parts = model_url.rstrip('/').split('/')
                    if len(url_parts) < 4:
                        continue
                    
                    model_url_name = url_parts[-1]
                    
                    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏
                    model_name = item.get_text(strip=True)
                    if not model_name:
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π URL
                    if not model_url.startswith('http'):
                        full_model_url = f"{self.base_url}{model_url}"
                    else:
                        full_model_url = model_url
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–∞ –º–æ–¥–µ–ª—å –µ—â–µ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω–∞
                    if any(m.url_name == model_url_name for m in models):
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
                    long_count, short_count = self.get_review_counts_for_model_url(full_model_url)
                    
                    model_info = ModelInfo(
                        name=model_name,
                        brand=brand.name,
                        url=full_model_url,
                        long_reviews_count=long_count,
                        short_reviews_count=short_count,
                        url_name=model_url_name
                    )
                    models.append(model_info)
                    
                    logger.debug(f"üöó –ú–æ–¥–µ–ª—å: {model_name} (–¥–ª–∏–Ω–Ω—ã—Ö: {long_count}, –∫–æ—Ä–æ—Ç–∫–∏—Ö: {short_count})")
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –º–æ–¥–µ–ª–∏: {e}")
                    continue
            
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π –¥–ª—è {brand.name}")
            return models
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è {brand.name}: {e}")
            return []

    def get_review_counts_for_model_url(self, model_url: str) -> Tuple[int, int]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –ø–æ URL"""
        if not model_url.startswith('http'):
            full_url = urljoin(self.base_url, model_url)
        else:
            full_url = model_url
            
        soup = self._make_request(full_url)
        if not soup:
            return 0, 0
        
        long_reviews_count = 0
        short_reviews_count = 0
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É –∏–∑ production_drom_parser.py
            # –ò—â–µ–º –∫–Ω–æ–ø–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–µ–∂–¥—É –¥–ª–∏–Ω–Ω—ã–º–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏
            tabs_block = soup.find('div', class_='_65ykvx0')
            if tabs_block and hasattr(tabs_block, 'find'):
                # –ö–Ω–æ–ø–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
                long_reviews_tab = tabs_block.find('a', {'data-ftid': 'reviews_tab_button_long_reviews'})
                if long_reviews_tab and hasattr(long_reviews_tab, 'get_text'):
                    text = long_reviews_tab.get_text(strip=True)
                    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏–∑ —á–∏—Å–ª–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                    match = re.search(r'(\d+)', text.replace(' ', ''))
                    if match:
                        long_reviews_count = int(match.group(1))
                
                # –ö–Ω–æ–ø–∫–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤
                short_reviews_tab = tabs_block.find('a', {'data-ftid': 'reviews_tab_button_short_reviews'})
                if short_reviews_tab and hasattr(short_reviews_tab, 'get_text'):
                    text = short_reviews_tab.get_text(strip=True)
                    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏–∑ —á–∏—Å–ª–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                    match = re.search(r'(\d+)', text.replace(' ', ''))
                    if match:
                        short_reviews_count = int(match.group(1))
            
            logger.debug(f"üìä {full_url}: {long_reviews_count} –¥–ª–∏–Ω–Ω—ã—Ö, {short_reviews_count} –∫–æ—Ä–æ—Ç–∫–∏—Ö")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥—Å—á–µ—Ç–µ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {full_url}: {e}")
        
        return long_reviews_count, short_reviews_count

    def parse_long_reviews(self, model: ModelInfo, limit: Optional[int] = None) -> List[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"""
        logger.info(f"üìù –ü–∞—Ä—Å–∏–Ω–≥ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
        
        reviews = []
        page = 1
        max_pages = 10  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        
        while page <= max_pages:
            if limit and len(reviews) >= limit:
                break
                
            # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏
            if not model.url.startswith('http'):
                url = urljoin(self.base_url, model.url)
            else:
                url = model.url
                
            if page > 1:
                url += f"?page={page}"
                
            soup = self._make_request(url)
            if not soup:
                break
                
            # –ü–æ–∏—Å–∫ –±–ª–æ–∫–æ–≤ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
            review_blocks = soup.find_all("div", {"data-ftid": "review-item"})
            
            if not review_blocks:
                logger.info(f"üìÑ –ù–µ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
                break
                
            for block in review_blocks:
                if limit and len(reviews) >= limit:
                    break
                    
                review = self._parse_long_review_block(block, model)
                if review:
                    reviews.append(review)
                    self.stats['reviews_parsed'] += 1
                    
            page += 1
            
        logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(reviews)} –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤")
        return reviews

    def parse_short_reviews(self, model: ModelInfo, limit: Optional[int] = None) -> List[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤"""
        logger.info(f"üí≠ –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
        
        reviews = []
        page = 1
        max_pages = 10
        
        while page <= max_pages:
            if limit and len(reviews) >= limit:
                break
                
            # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏
            if not model.url.startswith('http'):
                base_url = urljoin(self.base_url, model.url)
            else:
                base_url = model.url
                
            url = f"{base_url}5kopeek/"
            if page > 1:
                url += f"?page={page}"
                
            soup = self._make_request(url)
            if not soup:
                break
                
            # –ü–æ–∏—Å–∫ –±–ª–æ–∫–æ–≤ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤
            review_blocks = soup.find_all("div", {"data-ftid": "short-review-item"})
            
            if not review_blocks:
                logger.info(f"üìÑ –ù–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
                break
                
            for block in review_blocks:
                if limit and len(reviews) >= limit:
                    break
                    
                review = self._parse_short_review_block(block, model)
                if review:
                    reviews.append(review)
                    self.stats['reviews_parsed'] += 1
                    
            page += 1
            
        logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(reviews)} –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤")
        return reviews

    def _parse_long_review_block(self, block, model: ModelInfo) -> Optional[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º ID –æ—Ç–∑—ã–≤–∞
            review_id = block.get('id', '')
            
            # URL –æ—Ç–∑—ã–≤–∞
            review_url = f"{model.url}{review_id}/"
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞
            review_data = {
                'review_id': review_id,
                'brand': model.brand.lower(),
                'model': model.url_name,
                'review_type': 'long',
                'url': review_url
            }
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–≤—Ç–æ—Ä–µ
            author_elem = block.find("a", class_="css-1u4ddp")
            if author_elem:
                review_data["author"] = author_elem.get_text(strip=True)
            
            # –î–∞—Ç–∞
            date_elem = block.find("span", class_="css-1tc5ro3") or block.find("time")
            if date_elem:
                review_data["date"] = date_elem.get_text(strip=True)
            
            # –ì–æ—Ä–æ–¥
            city_elem = block.find("span", {"data-ftid": "review-location"})
            if city_elem:
                review_data["city"] = city_elem.get_text(strip=True)
            
            # –†–µ–π—Ç–∏–Ω–≥
            rating_elem = block.find("div", class_="css-1vkpuwn") or block.find("span", {"data-ftid": "review-rating"})
            if rating_elem:
                rating_text = rating_elem.get_text(strip=True)
                rating_match = re.search(r'(\\d+(?:\\.\\d+)?)', rating_text)
                if rating_match:
                    try:
                        review_data["rating"] = float(rating_match.group(1))
                    except ValueError:
                        pass
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = block.find("h3") or block.find("div", {"data-ftid": "review-title"})
            if title_elem:
                review_data["title"] = title_elem.get_text(strip=True)
            
            # –ü–ª—é—Å—ã
            positive_elem = block.find("div", {"data-ftid": "review-content__positive"})
            if positive_elem:
                review_data["positive_text"] = positive_elem.get_text(strip=True)
            
            # –ú–∏–Ω—É—Å—ã
            negative_elem = block.find("div", {"data-ftid": "review-content__negative"})
            if negative_elem:
                review_data["negative_text"] = negative_elem.get_text(strip=True)
                
            # –ü–æ–ª–æ–º–∫–∏
            breakages_elem = block.find("div", {"data-ftid": "review-content__breakages"})
            if breakages_elem:
                review_data["breakages_text"] = breakages_elem.get_text(strip=True)
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content_parts = []
            content_sections = block.find_all("div", class_="css-6hj46s")
            for section in content_sections:
                text = section.get_text(strip=True)
                if text:
                    content_parts.append(text)
            
            if content_parts:
                review_data["content"] = "\\n".join(content_parts)
            
            # –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            photos = block.find_all("img")
            photo_urls = []
            for img in photos:
                src = img.get('src') or img.get('data-src')
                if src and 'photo' in src:
                    photo_urls.append(src)
            review_data["photos"] = photo_urls
            
            # –°–æ–∑–¥–∞–µ–º ReviewData
            return ReviewData(**review_data)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞: {e}")
            return None

    def _parse_short_review_block(self, block, model: ModelInfo) -> Optional[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–∫–∞ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º ID –æ—Ç–∑—ã–≤–∞
            review_id = block.get('id', '')
            
            # URL –æ—Ç–∑—ã–≤–∞
            review_url = f"{model.url}5kopeek/{review_id}/"
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞
            review_data = {
                'review_id': review_id,
                'brand': model.brand.lower(),
                'model': model.url_name,
                'review_type': 'short',
                'url': review_url
            }
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–≤—Ç–æ—Ä–µ
            author_elem = block.find("span", class_="css-1u4ddp")
            if author_elem:
                review_data["author"] = author_elem.get_text(strip=True)
            
            # –ì–æ—Ä–æ–¥
            city_elem = block.find("span", {"data-ftid": "short-review-city"})
            if city_elem:
                review_data["city"] = city_elem.get_text(strip=True)
            
            # –ü–∞—Ä—Å–∏–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_div = block.find('div', {'data-ftid': 'short-review-item__title'})
            if title_div:
                self._extract_car_specs_from_title(title_div, review_data)
            
            # –ü–ª—é—Å—ã
            positive_elem = block.find("div", {"data-ftid": "short-review-content__positive"})
            if positive_elem:
                review_data["positive_text"] = positive_elem.get_text(strip=True)
            
            # –ú–∏–Ω—É—Å—ã
            negative_elem = block.find("div", {"data-ftid": "short-review-content__negative"})
            if negative_elem:
                review_data["negative_text"] = negative_elem.get_text(strip=True)
                
            # –ü–æ–ª–æ–º–∫–∏
            breakages_elem = block.find("div", {"data-ftid": "short-review-content__breakages"})
            if breakages_elem:
                review_data["breakages_text"] = breakages_elem.get_text(strip=True)
            
            # –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            photo_divs = block.find_all('div', class_='_1gzw4372')
            photo_urls = []
            for photo_div in photo_divs:
                img = photo_div.find('img')
                if img and img.get('src'):
                    photo_urls.append(img['src'])
            review_data["photos"] = photo_urls
            
            # –°–æ–∑–¥–∞–µ–º ReviewData
            return ReviewData(**review_data)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞: {e}")
            return None

    def _extract_car_specs_from_title(self, title_div, review_data: dict):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞"""
        try:
            text = title_div.get_text(strip=True)
            
            # –ì–æ–¥
            year_span = title_div.find('span', {'data-ftid': 'short-review-item__year'})
            if year_span:
                year_text = year_span.get_text(strip=True)
                try:
                    review_data['year'] = int(year_text)
                except ValueError:
                    pass
            
            # –û–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è
            volume_span = title_div.find('span', {'data-ftid': 'short-review-item__volume'})
            if volume_span:
                volume_text = volume_span.get_text(strip=True)
                try:
                    review_data['engine_volume'] = float(volume_text)
                except ValueError:
                    pass
            
            # –¢–∏–ø —Ç–æ–ø–ª–∏–≤–∞
            if '–±–µ–Ω–∑–∏–Ω' in text:
                review_data['fuel_type'] = '–±–µ–Ω–∑–∏–Ω'
            elif '–¥–∏–∑–µ–ª—å' in text:
                review_data['fuel_type'] = '–¥–∏–∑–µ–ª—å'
            elif '–≥–∏–±—Ä–∏–¥' in text:
                review_data['fuel_type'] = '–≥–∏–±—Ä–∏–¥'
            elif '—ç–ª–µ–∫—Ç—Ä–æ' in text:
                review_data['fuel_type'] = '—ç–ª–µ–∫—Ç—Ä–æ'
            
            # –ö–æ—Ä–æ–±–∫–∞ –ø–µ—Ä–µ–¥–∞—á
            if '–∞–≤—Ç–æ–º–∞—Ç' in text:
                review_data['transmission'] = '–∞–≤—Ç–æ–º–∞—Ç'
            elif '–º–µ—Ö–∞–Ω–∏–∫–∞' in text:
                review_data['transmission'] = '–º–µ—Ö–∞–Ω–∏–∫–∞'
                
            # –ü—Ä–∏–≤–æ–¥
            if '–ø–µ—Ä–µ–¥–Ω–∏–π' in text:
                review_data['drive_type'] = '–ø–µ—Ä–µ–¥–Ω–∏–π'
            elif '–∑–∞–¥–Ω–∏–π' in text:
                review_data['drive_type'] = '–∑–∞–¥–Ω–∏–π'
            elif '–ø–æ–ª–Ω—ã–π' in text:
                review_data['drive_type'] = '–ø–æ–ª–Ω—ã–π'
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {e}")

    def parse_model_reviews(self, 
                          model: ModelInfo, 
                          max_long_reviews: Optional[int] = None,
                          max_short_reviews: Optional[int] = None) -> List[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ (–¥–ª–∏–Ω–Ω—ã—Ö –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö)"""
        logger.info(f"üéØ –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
        
        all_reviews = []
        
        # –ü–∞—Ä—Å–∏–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
        if model.long_reviews_count > 0:
            long_reviews = self.parse_long_reviews(model, limit=max_long_reviews)
            all_reviews.extend(long_reviews)
            
        # –ü–∞—Ä—Å–∏–º –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–∑—ã–≤—ã
        if model.short_reviews_count > 0:
            short_reviews = self.parse_short_reviews(model, limit=max_short_reviews)
            all_reviews.extend(short_reviews)
            
        logger.info(f"‚úÖ –í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(all_reviews)} –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
        return all_reviews

    def save_to_json(self, reviews: List[ReviewData], filename: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –≤ JSON —Ñ–∞–π–ª"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä–∏
            reviews_data = [asdict(review) for review in reviews]
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º datetime –¥–ª—è JSON
            for review_data in reviews_data:
                if 'parsed_at' in review_data and review_data['parsed_at']:
                    review_data['parsed_at'] = review_data['parsed_at'].isoformat()
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(reviews_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤ –≤ {filename}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ JSON: {e}")

    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞"""
        return {
            'base_url': self.base_url,
            'cache_enabled': self.enable_cache,
            'delay': self.delay,
            **self.stats
        }

    def parse_limited_demo(self, 
                          max_brands: int = 3,
                          max_long_reviews: int = 3,
                          max_short_reviews: int = 10) -> Dict[str, Any]:
        """
        üéØ –î–ï–ú–û-–ü–ê–†–°–ò–ù–ì —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –¥–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥–∞: {max_brands} –±—Ä–µ–Ω–¥–æ–≤, {max_long_reviews} –¥–ª–∏–Ω–Ω—ã—Ö, {max_short_reviews} –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤")
        
        start_time = datetime.now()
        results = {
            'start_time': start_time.isoformat(),
            'brands_processed': [],
            'total_reviews': 0,
            'total_long_reviews': 0,
            'total_short_reviews': 0,
            'errors': []
        }
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –±—Ä–µ–Ω–¥–æ–≤
            brands = self.get_brands_catalog()
            if not brands:
                results['errors'].append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –±—Ä–µ–Ω–¥–æ–≤")
                return results
            
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –±—Ä–µ–Ω–¥—ã
            test_brands = brands[:max_brands]
            
            all_reviews = []
            
            for brand in test_brands:
                try:
                    logger.info(f"üè≠ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±—Ä–µ–Ω–¥: {brand.name}")
                    
                    # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
                    models = self.get_models_for_brand(brand)
                    if not models:
                        logger.warning(f"‚ö†Ô∏è  –ù–µ—Ç –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞ {brand.name}")
                        continue
                    
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å —Å –æ—Ç–∑—ã–≤–∞–º–∏ - –ù–ï –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –ø–æ–¥—Ä—è–¥ –¥–ª—è –¥–µ–º–æ
                    target_model = None
                    for model in models[:5]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –º–æ–¥–µ–ª–µ–π
                        if model.long_reviews_count > 0 or model.short_reviews_count > 0:
                            target_model = model
                            break
                    
                    if not target_model:
                        logger.warning(f"‚ö†Ô∏è  –ù–µ—Ç –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∑—ã–≤–∞–º–∏ –¥–ª—è –±—Ä–µ–Ω–¥–∞ {brand.name} (–ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –ø–µ—Ä–≤—ã–µ 5)")
                        continue
                    
                    # –ü–∞—Ä—Å–∏–º –æ—Ç–∑—ã–≤—ã
                    model_reviews = self.parse_model_reviews(
                        target_model,
                        max_long_reviews=max_long_reviews,
                        max_short_reviews=max_short_reviews
                    )
                    
                    all_reviews.extend(model_reviews)
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±—Ä–µ–Ω–¥—É
                    brand_stats = {
                        'brand': brand.name,
                        'model': target_model.name,
                        'long_reviews_available': target_model.long_reviews_count,
                        'short_reviews_available': target_model.short_reviews_count,
                        'long_reviews_parsed': len([r for r in model_reviews if r.review_type == 'long']),
                        'short_reviews_parsed': len([r for r in model_reviews if r.review_type == 'short']),
                        'total_parsed': len(model_reviews)
                    }
                    
                    results['brands_processed'].append(brand_stats)
                    
                except Exception as e:
                    error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±—Ä–µ–Ω–¥–∞ {brand.name}: {e}"
                    logger.error(f"‚ùå {error_msg}")
                    results['errors'].append(error_msg)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            results['total_reviews'] = len(all_reviews)
            results['total_long_reviews'] = len([r for r in all_reviews if r.review_type == 'long'])
            results['total_short_reviews'] = len([r for r in all_reviews if r.review_type == 'short'])
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = (datetime.now() - start_time).total_seconds()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            json_filename = f"data/master_parser_demo_{timestamp}.json"
            self.save_to_json(all_reviews, json_filename)
            results['saved_to'] = json_filename
            
            logger.info(f"üéâ –î–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {results['total_reviews']} –æ—Ç–∑—ã–≤–æ–≤ –∑–∞ {results['duration_seconds']:.1f} —Å–µ–∫")
            
        except Exception as e:
            error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –¥–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥–µ: {e}"
            logger.error(f"‚ùå {error_msg}")
            results['errors'].append(error_msg)
        
        return results


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Ç–µ—Ä-–ø–∞—Ä—Å–µ—Ä–∞
    parser = MasterDromParser(
        delay=1.0,
        cache_dir="data/cache",
        enable_cache=True
    )
    
    # –ó–∞–ø—É—Å–∫ –¥–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥–∞
    results = parser.parse_limited_demo(
        max_brands=3,
        max_long_reviews=3, 
        max_short_reviews=10
    )
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("üöó –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–ê–°–¢–ï–†-–ü–ê–†–°–ï–†–ê:")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±—Ä–µ–Ω–¥–æ–≤: {len(results['brands_processed'])}")
    print(f"–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {results['total_reviews']}")
    print(f"–î–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤: {results['total_long_reviews']}")
    print(f"–ö–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤: {results['total_short_reviews']}")
    print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {results.get('duration_seconds', 0):.1f} —Å–µ–∫")
    
    if results['errors']:
        print(f"–û—à–∏–±–æ–∫: {len(results['errors'])}")
        for error in results['errors']:
            print(f"  - {error}")
