#!/usr/bin/env python3
"""
üöó –ú–ê–°–¢–ï–†-–ü–ê–†–°–ï–† DROM.RU - –û–ë–™–ï–î–ò–ù–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
==============================================

–ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ô –ò –ì–õ–ê–í–ù–´–ô –ü–ê–†–°–ï–† –¥–ª—è drom.ru
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –í–°–ï–ì–û –ª—É—á—à–µ–≥–æ –∏–∑ –≤—Å–µ—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤:

‚úÖ –î–ª–∏–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ–±–∑–æ—Ä—ã) - –∏–∑ drom_reviews.py
‚úÖ –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–∑—ã–≤—ã (–∫—Ä–∞—Ç–∫–∏–µ –º–Ω–µ–Ω–∏—è) - –∏–∑ production_drom_parser.py  
‚úÖ –ö–∞—Ç–∞–ª–æ–≥ –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π - –∏–∑ unified_master_parser.py
‚úÖ –ù–∞–¥–µ–∂–Ω—É—é —Å–µ—Ç–µ–≤—É—é –ª–æ–≥–∏–∫—É - –∏–∑ base.py –∏ sync_base.py
‚úÖ –†–µ—Ç—Ä–∞–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ - –∏–∑ retry_decorator.py
‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –∏ –∑–∞–¥–µ—Ä–∂–∫–∏ - –∏–∑ delay_manager.py –∏ metrics.py
‚úÖ –ü–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö - –∏–∑ review.py
‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ - –∏–∑ cache.py
‚úÖ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ - –∏–∑ logger.py

–ë–û–õ–¨–®–ï –ù–ï –°–û–ó–î–ê–ï–ú –ù–û–í–´–• –ü–ê–†–°–ï–†–û–í!
–†–ê–ó–í–ò–í–ê–ï–ú –¢–û–õ–¨–ö–û –≠–¢–û–¢!

–ê–≤—Ç–æ—Ä: AI Assistant  
–î–∞—Ç–∞: 26.08.2025
"""

import asyncio
import hashlib
import json
import logging
import os
import re
import random
import time
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã
try:
    from ..models.review import Review
except ImportError:
    # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è Review –µ—Å–ª–∏ –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    from dataclasses import dataclass
    from datetime import datetime
    @dataclass
    class Review:
        source: str = ""
        type: str = ""
        brand: str = ""
        model: str = ""
        year: int = None
        url: str = ""
        title: str = ""
        content: str = ""
        author: str = ""
        rating: float = None
        pros: str = ""
        cons: str = ""
        engine_volume: float = None
        fuel_type: str = ""
        transmission: str = ""
        drive_type: str = ""
        body_type: str = ""
        city: str = None
        date: str = None
        useful_count: int = None
        not_useful_count: int = None
        views_count: int = None
        likes_count: int = None
        comments_count: int = None
        parsed_at: datetime = None
        content_hash: str = ""

try:
    from ..utils.delay_manager import DelayManager
except ImportError:
    # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è DelayManager
    class DelayManager:
        def __init__(self, min_delay=1.0, max_delay=2.0):
            self.min_delay = min_delay
            self.max_delay = max_delay
        def apply_delay(self):
            time.sleep(random.uniform(self.min_delay, self.max_delay))

try:
    from ..utils.logger import get_logger
except ImportError:
    # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ª–æ–≥–≥–µ—Ä–∞
    def get_logger(name):
        import logging
        logging.basicConfig(level=logging.INFO)
        return logging.getLogger(name)

try:
    from ..utils.cache import Cache
except ImportError:
    # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –∫—ç—à–∞
    class Cache:
        def __init__(self, cache_dir):
            self.cache_dir = cache_dir
        def get(self, key): return None
        def set(self, key, value): pass

try:
    from ..utils.metrics import ParsingMetrics
except ImportError:
    # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –º–µ—Ç—Ä–∏–∫
    class ParsingMetrics:
        def __init__(self): pass
        def record_request(self, url, success): pass
        def get_stats(self): return {}

try:
    from ..database.schema import DatabaseManager
except ImportError:
    # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è DatabaseManager
    class DatabaseManager:
        def __init__(self): pass
        def save_reviews(self, reviews): pass

logger = get_logger(__name__)


@dataclass
class ReviewData:
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–∞"""
    review_id: str
    brand: str
    model: str
    review_type: str  # 'long' –∏–ª–∏ 'short'
    
    # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è  
    year: Optional[int] = None
    engine_volume: Optional[float] = None
    fuel_type: Optional[str] = None
    transmission: Optional[str] = None
    drive_type: Optional[str] = None
    body_type: Optional[str] = None
    
    # –î–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞
    author: Optional[str] = None
    city: Optional[str] = None
    date: Optional[str] = None
    rating: Optional[float] = None
    title: Optional[str] = None
    
    # –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–∞
    positive_text: Optional[str] = None
    negative_text: Optional[str] = None
    general_text: Optional[str] = None
    breakages_text: Optional[str] = None
    content: Optional[str] = None
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    photos: List[str] = field(default_factory=list)
    photos_count: int = 0
    url: Optional[str] = None
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    views_count: Optional[int] = None
    likes_count: Optional[int] = None
    useful_count: Optional[int] = None
    not_useful_count: Optional[int] = None
    comments_count: Optional[int] = None
    
    # –°–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è
    parsed_at: Optional[datetime] = None
    content_hash: Optional[str] = None
    
    def __post_init__(self):
        if self.parsed_at is None:
            self.parsed_at = datetime.now()
        self.content_hash = self.generate_hash()
        self.photos_count = len(self.photos) if self.photos else 0
    
    def generate_hash(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ —Ö–µ—à–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        content_for_hash = f"{self.url}_{self.title or ''}_{self.content or ''}_{self.positive_text or ''}_{self.negative_text or ''}"
        return hashlib.md5(content_for_hash.encode()).hexdigest()
    
    def to_review_model(self) -> Review:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –º–æ–¥–µ–ª—å Review –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        return Review(
            source="drom.ru",
            type=self.review_type,
            brand=self.brand,
            model=self.model,
            year=self.year,
            url=self.url or "",
            title=self.title or "",
            content=self.content or "",
            author=self.author or "",
            rating=self.rating,
            pros=self.positive_text or "",
            cons=self.negative_text or "",
            engine_volume=self.engine_volume,
            fuel_type=self.fuel_type or "",
            transmission=self.transmission or "",
            drive_type=self.drive_type or "",
            body_type=self.body_type or "",
            city=self.city,
            date=self.date,
            useful_count=self.useful_count,
            not_useful_count=self.not_useful_count,
            views_count=self.views_count,
            likes_count=self.likes_count,
            comments_count=self.comments_count,
            parsed_at=self.parsed_at,
            content_hash=self.content_hash or ""
        )


@dataclass
class BrandInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±—Ä–µ–Ω–¥–µ"""
    name: str
    url: str
    reviews_count: int
    url_name: str  # –∏–º—è –≤ URL, –Ω–∞–ø—Ä–∏–º–µ—Ä 'alfa_romeo' –¥–ª—è 'Alfa Romeo'


@dataclass
class ModelInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    name: str
    brand: str
    url: str
    long_reviews_count: int = 0
    short_reviews_count: int = 0
    url_name: str = ""


class NetworkError(Exception):
    """–û—à–∏–±–∫–∞ —Å–µ—Ç–µ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è"""
    pass


class ParseError(Exception):
    """–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö"""
    pass


class MasterDromParser:
    """
    üöó –ú–ê–°–¢–ï–†-–ü–ê–†–°–ï–† DROM.RU - –û–ë–™–ï–î–ò–ù–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –í–°–Æ –ª—É—á—à—É—é –ª–æ–≥–∏–∫—É –∏–∑ –≤—Å–µ—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤:
    - –î–ª–∏–Ω–Ω—ã–µ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–∑—ã–≤—ã
    - –ö–∞—Ç–∞–ª–æ–≥ –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π  
    - –ù–∞–¥–µ–∂–Ω—É—é —Å–µ—Ç–µ–≤—É—é –ª–æ–≥–∏–∫—É
    - –†–µ—Ç—Ä–∞–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫
    - –ú–µ—Ç—Ä–∏–∫–∏ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    - –ü–æ–ª–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    """

    def __init__(self, 
                 delay: float = 1.0, 
                 cache_dir: str = "data/cache",
                 enable_database: bool = True,
                 enable_cache: bool = True):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Ç–µ—Ä-–ø–∞—Ä—Å–µ—Ä–∞"""
        
        self.base_url = "https://www.drom.ru"
        self.cache_dir = cache_dir
        self.enable_database = enable_database
        self.enable_cache = enable_cache
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        Path(cache_dir).mkdir(parents=True, exist_ok=True)
        Path("logs").mkdir(exist_ok=True)
        Path("data").mkdir(exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.delay_manager = DelayManager(min_delay=delay, max_delay=delay*2)
        self.metrics = ParsingMetrics()
        
        # –ö—ç—à
        if self.enable_cache:
            self.cache = Cache(cache_dir)
        else:
            self.cache = None
            
        # –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö
        if self.enable_database:
            try:
                self.db_manager = DatabaseManager()
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å DatabaseManager: {e}")
                self.db_manager = None
        else:
            self.db_manager = None
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏
        self.session = requests.Session()
        self.session.proxies = {}
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                         "AppleWebKit/537.36 (KHTML, like Gecko) "
                         "Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        logger.info("–ú–∞—Å—Ç–µ—Ä-–ø–∞—Ä—Å–µ—Ä Drom.ru –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _make_request(self, url: str, use_cache: bool = True) -> Optional[BeautifulSoup]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —Ä–µ—Ç—Ä–∞—è–º–∏
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ª–æ–≥–∏–∫—É –∏–∑ –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤
        """
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if use_cache and self.cache:
            cached_content = self.cache.get(url)
            if cached_content:
                logger.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –¥–ª—è {url}")
                return BeautifulSoup(cached_content, "html.parser")
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.info(f"–ó–∞–ø—Ä–æ—Å –∫ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                
                response = self.session.get(
                    url, 
                    headers=self.headers, 
                    timeout=30,
                    allow_redirects=True
                )
                response.raise_for_status()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                if use_cache and self.cache:
                    self.cache.set(url, response.text)
                
                soup = BeautifulSoup(response.content, "html.parser")
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
                self.delay_manager.apply_delay()
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                if hasattr(self.metrics, 'record_request'):
                    self.metrics.record_request(url, True)
                
                return soup
                
            except requests.RequestException as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                if hasattr(self.metrics, 'record_request'):
                    self.metrics.record_request(url, False)
                
                if attempt < max_retries - 1:
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ –æ—à–∏–±–∫–µ
                    error_delay = (attempt + 1) * 5
                    logger.info(f"–ñ–¥–µ–º {error_delay} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π")
                    time.sleep(error_delay)
                else:
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å {url} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                    raise NetworkError(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è {url}: {e}")
                    
        return None

    def get_brands_catalog(self) -> List[BrandInfo]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ –≤—Å–µ—Ö –±—Ä–µ–Ω–¥–æ–≤
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ª–æ–≥–∏–∫—É –∏–∑ production_drom_parser.py –∏ unified_master_parser.py
        """
        logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ –±—Ä–µ–Ω–¥–æ–≤")
        
        url = f"{self.base_url}/reviews/"
        soup = self._make_request(url)
        
        if not soup:
            raise NetworkError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –±—Ä–µ–Ω–¥–æ–≤")
            
        brands = []
        
        # –ò—â–µ–º –±–ª–æ–∫ —Å –±—Ä–µ–Ω–¥–∞–º–∏
        cars_list = soup.find("div", {"data-ftid": "component_cars-list"})
        if not cars_list:
            logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å –±—Ä–µ–Ω–¥–∞–º–∏")
            return brands
            
        # –ü–∞—Ä—Å–∏–º –±—Ä–µ–Ω–¥—ã
        brand_items = cars_list.find_all("div", class_="frg44i0")
        
        for item in brand_items:
            try:
                # –°—Å—ã–ª–∫–∞ –Ω–∞ –±—Ä–µ–Ω–¥
                link = item.find("a", {"data-ftid": "component_cars-list-item_hidden-link"})
                if not link:
                    continue
                    
                brand_url = link.get("href")
                if not brand_url:
                    continue
                    
                # –ò–º—è –±—Ä–µ–Ω–¥–∞
                name_span = item.find("span", {"data-ftid": "component_cars-list-item_name"})
                if not name_span:
                    continue
                    
                brand_name = name_span.get_text(strip=True)
                
                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤
                counter_span = item.find("span", {"data-ftid": "component_cars-list-item_counter"})
                reviews_count = 0
                if counter_span:
                    counter_text = counter_span.get_text(strip=True)
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    numbers = re.findall(r'\\d+', counter_text.replace(' ', ''))
                    if numbers:
                        reviews_count = int(numbers[0])
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º url_name –∏–∑ —Å—Å—ã–ª–∫–∏
                url_name = brand_url.strip('/').split('/')[-1]
                
                brand = BrandInfo(
                    name=brand_name,
                    url=brand_url,
                    reviews_count=reviews_count,
                    url_name=url_name
                )
                
                brands.append(brand)
                logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω –±—Ä–µ–Ω–¥: {brand_name} ({reviews_count} –æ—Ç–∑—ã–≤–æ–≤)")
                
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –±—Ä–µ–Ω–¥–∞: {e}")
                continue
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(brands)} –±—Ä–µ–Ω–¥–æ–≤")
        return brands

    def get_models_for_brand(self, brand: BrandInfo) -> List[ModelInfo]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É –∏–∑ production_drom_parser.py
        """
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞ {brand.name}")
        
        if not brand.url.startswith('http'):
            url = urljoin(self.base_url, brand.url)
        else:
            url = brand.url
            
        soup = self._make_request(url)
        if not soup:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –±—Ä–µ–Ω–¥–∞ {brand.name}")
            return []
        
        models = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª–∏
        model_links = soup.find_all("a", href=re.compile(rf"/reviews/{brand.url_name}/\\w+/$"))
        
        for link in model_links:
            try:
                model_url = link.get("href")
                if not model_url:
                    continue
                    
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ URL
                url_parts = model_url.strip('/').split('/')
                if len(url_parts) < 3:
                    continue
                    
                model_url_name = url_parts[-1]
                
                # –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏
                model_name = link.get_text(strip=True)
                if not model_name:
                    model_name = model_url_name.replace('_', ' ').title()
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
                long_count, short_count = self.get_review_counts_for_model_url(model_url)
                
                model = ModelInfo(
                    name=model_name,
                    brand=brand.name,
                    url=model_url,
                    long_reviews_count=long_count,
                    short_reviews_count=short_count,
                    url_name=model_url_name
                )
                
                models.append(model)
                logger.debug(f"–ú–æ–¥–µ–ª—å: {model_name} (–¥–ª–∏–Ω–Ω—ã—Ö: {long_count}, –∫–æ—Ä–æ—Ç–∫–∏—Ö: {short_count})")
                
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –º–æ–¥–µ–ª–∏: {e}")
                continue
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π –¥–ª—è {brand.name}")
        return models

    def get_review_counts_for_model_url(self, model_url: str) -> Tuple[int, int]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –ø–æ URL
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É –∏–∑ production_drom_parser.py
        """
        if not model_url.startswith('http'):
            full_url = urljoin(self.base_url, model_url)
        else:
            full_url = model_url
            
        soup = self._make_request(full_url)
        if not soup:
            return 0, 0
        
        long_reviews_count = 0
        short_reviews_count = 0
        
        # –ò—â–µ–º —Ç–∞–±—ã —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—Ç–∑—ã–≤–æ–≤
        tabs = soup.find_all("a", {"data-ftid": re.compile(r"reviews_tab_button")})
        
        for tab in tabs:
            tab_text = tab.get_text(strip=True)
            
            # –î–ª–∏–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
            if "data-ftid" in tab.attrs and "long_reviews" in tab["data-ftid"]:
                numbers = re.findall(r'\\d+', tab_text.replace(' ', ''))
                if numbers:
                    long_reviews_count = int(numbers[0])
                    
            # –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–∑—ã–≤—ã
            elif "data-ftid" in tab.attrs and "short_reviews" in tab["data-ftid"]:
                numbers = re.findall(r'\\d+', tab_text.replace(' ', ''))
                if numbers:
                    short_reviews_count = int(numbers[0])
        
        return long_reviews_count, short_reviews_count

    def parse_long_reviews(self, model: ModelInfo, limit: Optional[int] = None) -> List[ReviewData]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ª—É—á—à—É—é –ª–æ–≥–∏–∫—É –∏–∑ drom_reviews.py –∏ production_drom_parser.py
        """
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
        
        reviews = []
        page = 1
        max_pages = 50  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        
        while page <= max_pages:
            if limit and len(reviews) >= limit:
                break
                
            # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏
            if not model.url.startswith('http'):
                url = urljoin(self.base_url, model.url)
            else:
                url = model.url
                
            if page > 1:
                url += f"?page={page}"
                
            soup = self._make_request(url)
            if not soup:
                break
                
            # –ü–æ–∏—Å–∫ –±–ª–æ–∫–æ–≤ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
            review_blocks = soup.find_all("div", {"data-ftid": "review-item"})
            
            if not review_blocks:
                logger.info(f"–ù–µ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
                break
                
            for block in review_blocks:
                if limit and len(reviews) >= limit:
                    break
                    
                review = self._parse_long_review_block(block, model)
                if review:
                    reviews.append(review)
                    
            page += 1
            
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(reviews)} –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤")
        return reviews

    def parse_short_reviews(self, model: ModelInfo, limit: Optional[int] = None) -> List[ReviewData]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤  
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É –∏–∑ production_drom_parser.py
        """
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
        
        reviews = []
        page = 1
        max_pages = 50
        
        while page <= max_pages:
            if limit and len(reviews) >= limit:
                break
                
            # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏
            if not model.url.startswith('http'):
                base_url = urljoin(self.base_url, model.url)
            else:
                base_url = model.url
                
            url = f"{base_url}5kopeek/"
            if page > 1:
                url += f"?page={page}"
                
            soup = self._make_request(url)
            if not soup:
                break
                
            # –ü–æ–∏—Å–∫ –±–ª–æ–∫–æ–≤ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤
            review_blocks = soup.find_all("div", {"data-ftid": "short-review-item"})
            
            if not review_blocks:
                logger.info(f"–ù–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
                break
                
            for block in review_blocks:
                if limit and len(reviews) >= limit:
                    break
                    
                review = self._parse_short_review_block(block, model)
                if review:
                    reviews.append(review)
                    
            page += 1
            
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(reviews)} –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤")
        return reviews

    def _parse_long_review_block(self, block, model: ModelInfo) -> Optional[ReviewData]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ª–æ–≥–∏–∫—É –∏–∑ drom_reviews.py –∏ production_drom_parser.py
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º ID –æ—Ç–∑—ã–≤–∞
            review_id = block.get('id', '')
            
            # URL –æ—Ç–∑—ã–≤–∞
            review_url = f"{model.url}{review_id}/"
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞
            review_data = {
                'review_id': review_id,
                'brand': model.brand.lower(),
                'model': model.url_name,
                'review_type': 'long',
                'url': review_url
            }
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–≤—Ç–æ—Ä–µ –∏ –¥–∞—Ç–µ
            author_elem = block.find("a", class_="css-1u4ddp")
            if author_elem:
                review_data["author"] = author_elem.get_text(strip=True)
            
            # –î–∞—Ç–∞ (–∏—â–µ–º –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö)
            date_elem = block.find("span", class_="css-1tc5ro3") or block.find("time")
            if date_elem:
                review_data["date"] = date_elem.get_text(strip=True)
            
            # –ì–æ—Ä–æ–¥
            city_elem = block.find("span", {"data-ftid": "review-location"})
            if city_elem:
                review_data["city"] = city_elem.get_text(strip=True)
            
            # –†–µ–π—Ç–∏–Ω–≥
            rating_elem = block.find("div", class_="css-1vkpuwn") or block.find("span", {"data-ftid": "review-rating"})
            if rating_elem:
                rating_text = rating_elem.get_text(strip=True)
                rating_match = re.search(r'(\\d+(?:\\.\\d+)?)', rating_text)
                if rating_match:
                    try:
                        review_data["rating"] = float(rating_match.group(1))
                    except ValueError:
                        pass
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = block.find("h3") or block.find("div", {"data-ftid": "review-title"})
            if title_elem:
                review_data["title"] = title_elem.get_text(strip=True)
            
            # –ü–ª—é—Å—ã
            positive_elem = block.find("div", {"data-ftid": "review-content__positive"})
            if positive_elem:
                review_data["positive_text"] = positive_elem.get_text(strip=True)
            
            # –ú–∏–Ω—É—Å—ã
            negative_elem = block.find("div", {"data-ftid": "review-content__negative"})
            if negative_elem:
                review_data["negative_text"] = negative_elem.get_text(strip=True)
                
            # –ü–æ–ª–æ–º–∫–∏
            breakages_elem = block.find("div", {"data-ftid": "review-content__breakages"})
            if breakages_elem:
                review_data["breakages_text"] = breakages_elem.get_text(strip=True)
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content_parts = []
            content_sections = block.find_all("div", class_="css-6hj46s")
            for section in content_sections:
                text = section.get_text(strip=True)
                if text:
                    content_parts.append(text)
            
            if content_parts:
                review_data["content"] = "\\n".join(content_parts)
            
            # –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            photos = block.find_all("img")
            photo_urls = []
            for img in photos:
                src = img.get('src') or img.get('data-src')
                if src and 'photo' in src:
                    photo_urls.append(src)
            review_data["photos"] = photo_urls
            
            # –°–æ–∑–¥–∞–µ–º ReviewData
            review = ReviewData(**review_data)
            return review
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞: {e}")
            return None

    def _parse_short_review_block(self, block, model: ModelInfo) -> Optional[ReviewData]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–∫–∞ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É –∏–∑ production_drom_parser.py
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º ID –æ—Ç–∑—ã–≤–∞
            review_id = block.get('id', '')
            
            # URL –æ—Ç–∑—ã–≤–∞
            review_url = f"{model.url}5kopeek/{review_id}/"
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞
            review_data = {
                'review_id': review_id,
                'brand': model.brand.lower(),
                'model': model.url_name,
                'review_type': 'short',
                'url': review_url
            }
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–≤—Ç–æ—Ä–µ
            author_elem = block.find("span", class_="css-1u4ddp")
            if author_elem:
                review_data["author"] = author_elem.get_text(strip=True)
            
            # –ì–æ—Ä–æ–¥
            city_elem = block.find("span", {"data-ftid": "short-review-city"})
            if city_elem:
                review_data["city"] = city_elem.get_text(strip=True)
            
            # –ü–∞—Ä—Å–∏–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_div = block.find('div', {'data-ftid': 'short-review-item__title'})
            if title_div:
                self._extract_car_specs_from_title(title_div, review_data)
            
            # –ü–ª—é—Å—ã
            positive_elem = block.find("div", {"data-ftid": "short-review-content__positive"})
            if positive_elem:
                review_data["positive_text"] = positive_elem.get_text(strip=True)
            
            # –ú–∏–Ω—É—Å—ã
            negative_elem = block.find("div", {"data-ftid": "short-review-content__negative"})
            if negative_elem:
                review_data["negative_text"] = negative_elem.get_text(strip=True)
                
            # –ü–æ–ª–æ–º–∫–∏
            breakages_elem = block.find("div", {"data-ftid": "short-review-content__breakages"})
            if breakages_elem:
                review_data["breakages_text"] = breakages_elem.get_text(strip=True)
            
            # –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            photo_divs = block.find_all('div', class_='_1gzw4372')
            photo_urls = []
            for photo_div in photo_divs:
                img = photo_div.find('img')
                if img and img.get('src'):
                    photo_urls.append(img['src'])
            review_data["photos"] = photo_urls
            
            # –°–æ–∑–¥–∞–µ–º ReviewData
            review = ReviewData(**review_data)
            return review
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞: {e}")
            return None

    def _extract_car_specs_from_title(self, title_div, review_data: dict):
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞
        –õ–æ–≥–∏–∫–∞ –∏–∑ production_drom_parser.py
        """
        try:
            text = title_div.get_text(strip=True)
            
            # –ì–æ–¥
            year_span = title_div.find('span', {'data-ftid': 'short-review-item__year'})
            if year_span:
                year_text = year_span.get_text(strip=True)
                try:
                    review_data['year'] = int(year_text)
                except ValueError:
                    pass
            
            # –û–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è
            volume_span = title_div.find('span', {'data-ftid': 'short-review-item__volume'})
            if volume_span:
                volume_text = volume_span.get_text(strip=True)
                try:
                    review_data['engine_volume'] = float(volume_text)
                except ValueError:
                    pass
            
            # –¢–∏–ø —Ç–æ–ø–ª–∏–≤–∞
            if '–±–µ–Ω–∑–∏–Ω' in text:
                review_data['fuel_type'] = '–±–µ–Ω–∑–∏–Ω'
            elif '–¥–∏–∑–µ–ª—å' in text:
                review_data['fuel_type'] = '–¥–∏–∑–µ–ª—å'
            elif '–≥–∏–±—Ä–∏–¥' in text:
                review_data['fuel_type'] = '–≥–∏–±—Ä–∏–¥'
            elif '—ç–ª–µ–∫—Ç—Ä–æ' in text:
                review_data['fuel_type'] = '—ç–ª–µ–∫—Ç—Ä–æ'
            
            # –ö–æ—Ä–æ–±–∫–∞ –ø–µ—Ä–µ–¥–∞—á
            if '–∞–≤—Ç–æ–º–∞—Ç' in text:
                review_data['transmission'] = '–∞–≤—Ç–æ–º–∞—Ç'
            elif '–º–µ—Ö–∞–Ω–∏–∫–∞' in text:
                review_data['transmission'] = '–º–µ—Ö–∞–Ω–∏–∫–∞'
                
            # –ü—Ä–∏–≤–æ–¥
            if '–ø–µ—Ä–µ–¥–Ω–∏–π' in text:
                review_data['drive_type'] = '–ø–µ—Ä–µ–¥–Ω–∏–π'
            elif '–∑–∞–¥–Ω–∏–π' in text:
                review_data['drive_type'] = '–∑–∞–¥–Ω–∏–π'
            elif '–ø–æ–ª–Ω—ã–π' in text:
                review_data['drive_type'] = '–ø–æ–ª–Ω—ã–π'
                
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {e}")

    def parse_model_reviews(self, 
                          model: ModelInfo, 
                          max_long_reviews: Optional[int] = None,
                          max_short_reviews: Optional[int] = None) -> List[ReviewData]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ (–¥–ª–∏–Ω–Ω—ã—Ö –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö)
        """
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
        
        all_reviews = []
        
        # –ü–∞—Ä—Å–∏–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
        if model.long_reviews_count > 0:
            long_reviews = self.parse_long_reviews(model, limit=max_long_reviews)
            all_reviews.extend(long_reviews)
            
        # –ü–∞—Ä—Å–∏–º –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–∑—ã–≤—ã
        if model.short_reviews_count > 0:
            short_reviews = self.parse_short_reviews(model, limit=max_short_reviews)
            all_reviews.extend(short_reviews)
            
        logger.info(f"–í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(all_reviews)} –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {model.brand} {model.name}")
        return all_reviews

    def save_to_database(self, reviews: List[ReviewData]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        if not self.db_manager:
            logger.warning("DatabaseManager –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return
            
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–æ–¥–µ–ª–∏ Review –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            review_models = [review.to_review_model() for review in reviews]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ—Ä–µ–∑ DatabaseManager
            self.db_manager.save_reviews(review_models)
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö: {e}")

    def save_to_json(self, reviews: List[ReviewData], filename: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –≤ JSON —Ñ–∞–π–ª"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä–∏
            reviews_data = [asdict(review) for review in reviews]
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º datetime –¥–ª—è JSON
            for review_data in reviews_data:
                if 'parsed_at' in review_data and review_data['parsed_at']:
                    review_data['parsed_at'] = review_data['parsed_at'].isoformat()
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(reviews_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤ –≤ {filename}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ JSON: {e}")

    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞"""
        stats = {
            'cache_enabled': self.enable_cache,
            'database_enabled': self.enable_database,
            'base_url': self.base_url,
        }
        
        if hasattr(self.metrics, 'get_stats'):
            stats.update(self.metrics.get_stats())
            
        return stats

    def parse_limited_demo(self, 
                          max_brands: int = 3,
                          max_long_reviews: int = 3,
                          max_short_reviews: int = 10) -> Dict[str, Any]:
        """
        –î–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        logger.info(f"–ó–∞–ø—É—Å–∫ –¥–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥–∞: {max_brands} –±—Ä–µ–Ω–¥–æ–≤, {max_long_reviews} –¥–ª–∏–Ω–Ω—ã—Ö, {max_short_reviews} –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤")
        
        start_time = datetime.now()
        results = {
            'start_time': start_time.isoformat(),
            'brands_processed': [],
            'total_reviews': 0,
            'total_long_reviews': 0,
            'total_short_reviews': 0,
            'errors': []
        }
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –±—Ä–µ–Ω–¥–æ–≤
            brands = self.get_brands_catalog()
            if not brands:
                results['errors'].append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –±—Ä–µ–Ω–¥–æ–≤")
                return results
            
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –±—Ä–µ–Ω–¥—ã
            test_brands = brands[:max_brands]
            
            all_reviews = []
            
            for brand in test_brands:
                try:
                    logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±—Ä–µ–Ω–¥: {brand.name}")
                    
                    # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
                    models = self.get_models_for_brand(brand)
                    if not models:
                        logger.warning(f"–ù–µ—Ç –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞ {brand.name}")
                        continue
                    
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å —Å –æ—Ç–∑—ã–≤–∞–º–∏
                    target_model = None
                    for model in models:
                        if model.long_reviews_count > 0 or model.short_reviews_count > 0:
                            target_model = model
                            break
                    
                    if not target_model:
                        logger.warning(f"–ù–µ—Ç –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∑—ã–≤–∞–º–∏ –¥–ª—è –±—Ä–µ–Ω–¥–∞ {brand.name}")
                        continue
                    
                    # –ü–∞—Ä—Å–∏–º –æ—Ç–∑—ã–≤—ã
                    model_reviews = self.parse_model_reviews(
                        target_model,
                        max_long_reviews=max_long_reviews,
                        max_short_reviews=max_short_reviews
                    )
                    
                    all_reviews.extend(model_reviews)
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±—Ä–µ–Ω–¥—É
                    brand_stats = {
                        'brand': brand.name,
                        'model': target_model.name,
                        'long_reviews_available': target_model.long_reviews_count,
                        'short_reviews_available': target_model.short_reviews_count,
                        'long_reviews_parsed': len([r for r in model_reviews if r.review_type == 'long']),
                        'short_reviews_parsed': len([r for r in model_reviews if r.review_type == 'short']),
                        'total_parsed': len(model_reviews)
                    }
                    
                    results['brands_processed'].append(brand_stats)
                    
                except Exception as e:
                    error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±—Ä–µ–Ω–¥–∞ {brand.name}: {e}"
                    logger.error(error_msg)
                    results['errors'].append(error_msg)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            results['total_reviews'] = len(all_reviews)
            results['total_long_reviews'] = len([r for r in all_reviews if r.review_type == 'long'])
            results['total_short_reviews'] = len([r for r in all_reviews if r.review_type == 'short'])
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = (datetime.now() - start_time).total_seconds()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            json_filename = f"data/master_parser_demo_{timestamp}.json"
            self.save_to_json(all_reviews, json_filename)
            results['saved_to'] = json_filename
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
            if self.enable_database and all_reviews:
                self.save_to_database(all_reviews)
            
            logger.info(f"–î–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {results['total_reviews']} –æ—Ç–∑—ã–≤–æ–≤ –∑–∞ {results['duration_seconds']:.1f} —Å–µ–∫")
            
        except Exception as e:
            error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –¥–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥–µ: {e}"
            logger.error(error_msg)
            results['errors'].append(error_msg)
        
        return results


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Ç–µ—Ä-–ø–∞—Ä—Å–µ—Ä–∞
    parser = MasterDromParser(
        delay=1.0,
        cache_dir="data/cache",
        enable_database=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –ë–î –¥–ª—è –¥–µ–º–æ
        enable_cache=True
    )
    
    # –ó–∞–ø—É—Å–∫ –¥–µ–º–æ-–ø–∞—Ä—Å–∏–Ω–≥–∞
    results = parser.parse_limited_demo(
        max_brands=3,
        max_long_reviews=3, 
        max_short_reviews=10
    )
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("üöó –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–ê–°–¢–ï–†-–ü–ê–†–°–ï–†–ê:")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±—Ä–µ–Ω–¥–æ–≤: {len(results['brands_processed'])}")
    print(f"–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {results['total_reviews']}")
    print(f"–î–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤: {results['total_long_reviews']}")
    print(f"–ö–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤: {results['total_short_reviews']}")
    print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {results.get('duration_seconds', 0):.1f} —Å–µ–∫")
    
    if results['errors']:
        print(f"–û—à–∏–±–æ–∫: {len(results['errors'])}")
        for error in results['errors']:
            print(f"  - {error}")
