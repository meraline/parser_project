#!/usr/bin/env python3
"""
üöó –ï–î–ò–ù–´–ô –ü–ê–†–°–ï–† –û–¢–ó–´–í–û–í DROM.RU - –ú–ê–°–¢–ï–†-–í–ï–†–°–ò–Ø
===============================================

–ì–õ–ê–í–ù–´–ô –ü–ê–†–°–ï–†, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –í–°–ï —Ñ—É–Ω–∫—Ü–∏–∏:
‚úÖ –î–ª–∏–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ–±–∑–æ—Ä—ã)
‚úÖ –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–∑—ã–≤—ã (–∫—Ä–∞—Ç–∫–∏–µ –º–Ω–µ–Ω–∏—è) 
‚úÖ –ö–∞—Ç–∞–ª–æ–≥ –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π
‚úÖ –ë–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏

–ë–û–õ–¨–®–ï –ù–ï –°–û–ó–î–ê–ï–ú –ù–û–í–´–• –ü–ê–†–°–ï–†–û–í!
–†–ê–ó–í–ò–í–ê–ï–ú –¢–û–õ–¨–ö–û –≠–¢–û–¢!

–ê–≤—Ç–æ—Ä: AI Assistant
–î–∞—Ç–∞: 26.08.2025
"""

import requests
import json
import time
import logging
import re
import os
from typing import Dict, List, Optional, Tuple, Union
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from dataclasses import dataclass, asdict
from datetime import datetime
import hashlib


@dataclass 
class ReviewData:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–∞"""
    review_id: str
    brand: str
    model: str
    review_type: str  # 'long' –∏–ª–∏ 'short'
    year: Optional[int] = None
    engine_volume: Optional[float] = None
    fuel_type: Optional[str] = None
    transmission: Optional[str] = None
    drive_type: Optional[str] = None
    author: Optional[str] = None
    city: Optional[str] = None
    date: Optional[str] = None
    rating: Optional[int] = None
    title: Optional[str] = None
    positive: Optional[str] = None
    negative: Optional[str] = None
    general: Optional[str] = None
    breakages: Optional[str] = None
    photos: List[str] = None
    url: Optional[str] = None
    
    def __post_init__(self):
        if self.photos is None:
            self.photos = []


@dataclass
class ModelInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    brand: str
    model: str
    model_url: str
    long_reviews_count: int = 0
    short_reviews_count: int = 0


class UnifiedDromParser:
    """–ï–î–ò–ù–´–ô –ú–ê–°–¢–ï–†-–ü–ê–†–°–ï–† –¥–ª—è drom.ru - –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –í–°–ï —Ñ—É–Ω–∫—Ü–∏–∏"""

    def __init__(self, delay: float = 1.0, cache_dir: str = "data/cache"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
        self.base_url = "https://www.drom.ru"
        self.delay = delay
        self.cache_dir = cache_dir
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs(cache_dir, exist_ok=True)
        os.makedirs("logs", exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.setup_logging()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏
        self.session = requests.Session()
        self.session.proxies = {}
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'long_reviews_found': 0,
            'short_reviews_found': 0,
            'models_processed': 0,
            'brands_processed': 0
        }
        
        self.logger.info("üöÄ –ï–¥–∏–Ω—ã–π –ø–∞—Ä—Å–µ—Ä DROM.RU –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        
        if not self.logger.handlers:
            handler = logging.FileHandler('logs/unified_parser.log', encoding='utf-8')
            formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            
            # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π handler
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)

    def make_request(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        self.stats['total_requests'] += 1
        
        for attempt in range(retries):
            try:
                self.logger.info(f"üåê –ó–∞–ø—Ä–æ—Å: {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                
                response = self.session.get(url, headers=self.headers, timeout=30)
                
                if response.status_code == 200:
                    self.stats['successful_requests'] += 1
                    soup = BeautifulSoup(response.content, 'html.parser')
                    time.sleep(self.delay)
                    return soup
                elif response.status_code == 404:
                    self.logger.warning(f"‚ùå 404 Not Found: {url}")
                    self.stats['failed_requests'] += 1
                    return None
                else:
                    self.logger.warning(f"‚ö†Ô∏è –°—Ç–∞—Ç—É—Å {response.status_code}: {url}")
                    
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                
            if attempt < retries - 1:
                wait_time = (attempt + 1) * 2
                self.logger.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {wait_time}—Å –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º...")
                time.sleep(wait_time)
        
        self.stats['failed_requests'] += 1
        return None

    def get_brand_catalog(self) -> List[Dict[str, str]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ –±—Ä–µ–Ω–¥–æ–≤"""
        cache_file = os.path.join(self.cache_dir, "brand_catalog.json")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    brands = json.load(f)
                self.logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(brands)} –±—Ä–µ–Ω–¥–æ–≤ –∏–∑ –∫—ç—à–∞")
                return brands
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞: {e}")
        
        # –ü–∞—Ä—Å–∏–º –∫–∞—Ç–∞–ª–æ–≥
        self.logger.info("üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–∞ –±—Ä–µ–Ω–¥–æ–≤...")
        soup = self.make_request(f"{self.base_url}/reviews/")
        
        if not soup:
            self.logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –±—Ä–µ–Ω–¥–æ–≤")
            return []
        
        brands = []
        brand_links = soup.find_all('a', {'data-ftid': 'component_cars-list-item_hidden-link'})
        
        for link in brand_links:
            try:
                brand_name = link.get_text(strip=True)
                brand_url = link.get('href', '')
                
                if brand_url and brand_name:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º slug –∏–∑ URL
                    brand_slug = brand_url.rstrip('/').split('/')[-1]
                    
                    brands.append({
                        'name': brand_name,
                        'slug': brand_slug,
                        'url': brand_url
                    })
                    
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±—Ä–µ–Ω–¥–∞: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(brands, f, ensure_ascii=False, indent=2)
            self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(brands)} –±—Ä–µ–Ω–¥–æ–≤ –≤ –∫—ç—à")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")
        
        self.stats['brands_processed'] = len(brands)
        return brands

    def get_brand_models(self, brand_slug: str) -> List[ModelInfo]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±—Ä–µ–Ω–¥–∞"""
        cache_file = os.path.join(self.cache_dir, f"models_{brand_slug}.json")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    models_data = json.load(f)
                models = [ModelInfo(**model) for model in models_data]
                self.logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π {brand_slug} –∏–∑ –∫—ç—à–∞")
                return models
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π: {e}")
        
        # –ü–∞—Ä—Å–∏–º –º–æ–¥–µ–ª–∏
        brand_url = f"{self.base_url}/reviews/{brand_slug}/"
        self.logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π –±—Ä–µ–Ω–¥–∞: {brand_slug}")
        
        soup = self.make_request(brand_url)
        if not soup:
            return []
        
        models = []
        model_links = soup.find_all('a', href=True)
        
        for link in model_links:
            href = link.get('href', '')
            if f'/reviews/{brand_slug}/' in href and href.count('/') >= 4:
                try:
                    model_name = link.get_text(strip=True)
                    if model_name and len(model_name) > 1:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º slug –º–æ–¥–µ–ª–∏ –∏–∑ URL
                        parts = href.rstrip('/').split('/')
                        if len(parts) >= 4:
                            model_slug = parts[-1]
                            
                            # –ò—â–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –æ—Ç–∑—ã–≤–æ–≤
                            parent = link.find_parent()
                            long_count = 0
                            short_count = 0
                            
                            # –ü–æ–∏—Å–∫ —Å—á–µ—Ç—á–∏–∫–æ–≤ –≤ —Å–æ—Å–µ–¥–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
                            if parent:
                                text = parent.get_text()
                                numbers = re.findall(r'\d+', text)
                                if numbers:
                                    long_count = int(numbers[0])
                                    if len(numbers) > 1:
                                        short_count = int(numbers[1])
                            
                            models.append(ModelInfo(
                                brand=brand_slug,
                                model=model_slug,
                                model_url=href,
                                long_reviews_count=long_count,
                                short_reviews_count=short_count
                            ))
                            
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        seen = set()
        unique_models = []
        for model in models:
            key = (model.brand, model.model)
            if key not in seen:
                seen.add(key)
                unique_models.append(model)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        try:
            models_data = [asdict(model) for model in unique_models]
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(models_data, f, ensure_ascii=False, indent=2)
            self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(unique_models)} –º–æ–¥–µ–ª–µ–π {brand_slug} –≤ –∫—ç—à")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π: {e}")
        
        return unique_models

    def parse_long_reviews(self, brand: str, model: str, limit: int = 10) -> List[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"""
        url = f"{self.base_url}/reviews/{brand}/{model}/"
        self.logger.info(f"üìù –ü–∞—Ä—Å–∏–Ω–≥ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤: {brand}/{model}")
        
        soup = self.make_request(url)
        if not soup:
            return []
        
        reviews = []
        review_items = soup.find_all('div', {'data-ftid': 'review-item'})
        
        self.logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(review_items)} –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤")
        
        for i, item in enumerate(review_items[:limit]):
            try:
                review = self.extract_long_review_data(item, brand, model, url)
                if review:
                    reviews.append(review)
                    self.stats['long_reviews_found'] += 1
                    
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞ {i+1}: {e}")
        
        return reviews

    def parse_short_reviews(self, brand: str, model: str, limit: int = 20) -> List[ReviewData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤"""
        url = f"{self.base_url}/reviews/{brand}/{model}/5kopeek/"
        self.logger.info(f"üí¨ –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤: {brand}/{model}")
        
        soup = self.make_request(url)
        if not soup:
            return []
        
        reviews = []
        review_items = soup.find_all('div', {'data-ftid': 'short-review-item'})
        
        self.logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(review_items)} –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤")
        
        for i, item in enumerate(review_items[:limit]):
            try:
                review = self.extract_short_review_data(item, brand, model, url)
                if review:
                    reviews.append(review)
                    self.stats['short_reviews_found'] += 1
                    
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞ {i+1}: {e}")
        
        return reviews

    def extract_long_review_data(self, item, brand: str, model: str, base_url: str) -> Optional[ReviewData]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞"""
        try:
            # ID –æ—Ç–∑—ã–≤–∞
            review_id = item.get('id', f"long_{hash(str(item)[:100])}")
            
            # –ê–≤—Ç–æ—Ä –∏ –¥–∞—Ç–∞
            author_elem = item.find('span', {'data-ftid': 'review-item-author-name'})
            author = author_elem.get_text(strip=True) if author_elem else None
            
            date_elem = item.find('span', {'data-ftid': 'review-item-date'})
            date = date_elem.get_text(strip=True) if date_elem else None
            
            # –ì–æ—Ä–æ–¥
            city_elem = item.find('span', {'data-ftid': 'review-item-author-city'})
            city = city_elem.get_text(strip=True) if city_elem else None
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = item.find('h3', {'data-ftid': 'review-item-title'})
            title = title_elem.get_text(strip=True) if title_elem else None
            
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∞–≤—Ç–æ
            car_info = item.find('div', {'data-ftid': 'review-item-car-info'})
            year, engine_volume, fuel_type, transmission, drive_type = None, None, None, None, None
            
            if car_info:
                info_text = car_info.get_text()
                year = self.extract_year(info_text)
                engine_volume = self.extract_engine_volume(info_text)
                fuel_type = self.extract_fuel_type(info_text)
                transmission = self.extract_transmission(info_text)
                drive_type = self.extract_drive_type(info_text)
            
            # –ö–æ–Ω—Ç–µ–Ω—Ç –æ—Ç–∑—ã–≤–∞
            positive_elem = item.find('div', {'data-ftid': 'review-item-positive'})
            positive = positive_elem.get_text(strip=True) if positive_elem else None
            
            negative_elem = item.find('div', {'data-ftid': 'review-item-negative'})
            negative = negative_elem.get_text(strip=True) if negative_elem else None
            
            general_elem = item.find('div', {'data-ftid': 'review-item-general'})
            general = general_elem.get_text(strip=True) if general_elem else None
            
            breakages_elem = item.find('div', {'data-ftid': 'review-item-breakages'})
            breakages = breakages_elem.get_text(strip=True) if breakages_elem else None
            
            # –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            photos = []
            img_elements = item.find_all('img', src=True)
            for img in img_elements:
                src = img.get('src')
                if src and 'photo' in src:
                    photos.append(src)
            
            # –†–µ–π—Ç–∏–Ω–≥
            rating_elem = item.find('div', class_=re.compile(r'rating|stars'))
            rating = self.extract_rating(rating_elem) if rating_elem else None
            
            return ReviewData(
                review_id=review_id,
                brand=brand,
                model=model,
                review_type='long',
                year=year,
                engine_volume=engine_volume,
                fuel_type=fuel_type,
                transmission=transmission,
                drive_type=drive_type,
                author=author,
                city=city,
                date=date,
                rating=rating,
                title=title,
                positive=positive,
                negative=negative,
                general=general,
                breakages=breakages,
                photos=photos,
                url=base_url
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞: {e}")
            return None

    def extract_short_review_data(self, item, brand: str, model: str, base_url: str) -> Optional[ReviewData]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞"""
        try:
            # ID –æ—Ç–∑—ã–≤–∞
            review_id = item.get('id', f"short_{hash(str(item)[:100])}")
            
            # –ê–≤—Ç–æ—Ä –∏ –¥–∞—Ç–∞ (–≤ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–∞—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –¥—Ä—É–≥–æ–º —Ñ–æ—Ä–º–∞—Ç–µ)
            author_elem = item.find('span', class_=re.compile(r'author|user'))
            author = author_elem.get_text(strip=True) if author_elem else None
            
            # –î–∞—Ç–∞
            date_elems = item.find_all('span')
            date = None
            for elem in date_elems:
                text = elem.get_text(strip=True)
                if '–Ω–∞–∑–∞–¥' in text or '–≥–æ–¥' in text or re.search(r'\d+\.\d+\.\d+', text):
                    date = text
                    break
            
            # –ì–æ—Ä–æ–¥
            city_elem = item.find('span', {'data-ftid': 'short-review-city'})
            city = city_elem.get_text(strip=True) if city_elem else None
            
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∞–≤—Ç–æ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_elem = item.find('div', {'data-ftid': 'short-review-item__title'})
            year, engine_volume, fuel_type, transmission, drive_type = None, None, None, None, None
            
            if title_elem:
                title_text = title_elem.get_text()
                year = self.extract_year(title_text)
                engine_volume = self.extract_engine_volume(title_text)
                fuel_type = self.extract_fuel_type(title_text)
                transmission = self.extract_transmission(title_text)
                drive_type = self.extract_drive_type(title_text)
            
            # –ö–æ–Ω—Ç–µ–Ω—Ç –æ—Ç–∑—ã–≤–∞
            positive_elem = item.find('div', {'data-ftid': 'short-review-content__positive'})
            positive = positive_elem.get_text(strip=True) if positive_elem else None
            
            negative_elem = item.find('div', {'data-ftid': 'short-review-content__negative'})
            negative = negative_elem.get_text(strip=True) if negative_elem else None
            
            breakages_elem = item.find('div', {'data-ftid': 'short-review-content__breakages'})
            breakages = breakages_elem.get_text(strip=True) if breakages_elem else None
            
            # –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            photos = []
            img_elements = item.find_all('img', src=True)
            for img in img_elements:
                src = img.get('src')
                if src and ('photo' in src or 'auto.drom.ru' in src):
                    photos.append(src)
            
            return ReviewData(
                review_id=review_id,
                brand=brand,
                model=model,
                review_type='short',
                year=year,
                engine_volume=engine_volume,
                fuel_type=fuel_type,
                transmission=transmission,
                drive_type=drive_type,
                author=author,
                city=city,
                date=date,
                rating=None,  # –í –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–∞—Ö –æ–±—ã—á–Ω–æ –Ω–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–∞
                title=None,
                positive=positive,
                negative=negative,
                general=None,
                breakages=breakages,
                photos=photos,
                url=base_url
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–∑—ã–≤–∞: {e}")
            return None

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    def extract_year(self, text: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞"""
        match = re.search(r'\b(19|20)\d{2}\b', text)
        if match:
            year = int(match.group())
            if 1980 <= year <= datetime.now().year:
                return year
        return None

    def extract_engine_volume(self, text: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –¥–≤–∏–≥–∞—Ç–µ–ª—è"""
        patterns = [
            r'(\d+(?:\.\d+)?)\s*–ª(?:\.|[^–∞-—è—ë])',
            r'(\d{4})\s*—Å–º¬≥',
        ]
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    volume = float(match.group(1))
                    if volume > 100:  # —Å–º¬≥
                        volume = volume / 1000
                    if 0.5 <= volume <= 10.0:
                        return volume
                except ValueError:
                    continue
        return None

    def extract_fuel_type(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ç–æ–ø–ª–∏–≤–∞"""
        fuel_types = {
            '–±–µ–Ω–∑–∏–Ω': ['–±–µ–Ω–∑–∏–Ω', 'gasoline', 'petrol'],
            '–¥–∏–∑–µ–ª—å': ['–¥–∏–∑–µ–ª—å', '–¥–∏–∑–µ–ª', 'diesel'],
            '–≥–∏–±—Ä–∏–¥': ['–≥–∏–±—Ä–∏–¥', 'hybrid'],
            '—ç–ª–µ–∫—Ç—Ä–æ': ['—ç–ª–µ–∫—Ç—Ä–æ', 'electric', '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π'],
            '–≥–∞–∑': ['–≥–∞–∑', 'lpg', 'cng']
        }
        
        text_lower = text.lower()
        for fuel_type, keywords in fuel_types.items():
            for keyword in keywords:
                if keyword in text_lower:
                    return fuel_type
        return None

    def extract_transmission(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏–∏"""
        transmissions = {
            '–∞–≤—Ç–æ–º–∞—Ç': ['–∞–≤—Ç–æ–º–∞—Ç', '–∞–∫–ø–ø', 'automatic', 'at'],
            '–º–µ—Ö–∞–Ω–∏–∫–∞': ['–º–µ—Ö–∞–Ω–∏–∫–∞', '–º–∫–ø–ø', 'manual', 'mt'],
            '–≤–∞—Ä–∏–∞—Ç–æ—Ä': ['–≤–∞—Ä–∏–∞—Ç–æ—Ä', 'cvt'],
            '—Ä–æ–±–æ—Ç': ['—Ä–æ–±–æ—Ç', 'amt', 'robot']
        }
        
        text_lower = text.lower()
        for trans_type, keywords in transmissions.items():
            for keyword in keywords:
                if keyword in text_lower:
                    return trans_type
        return None

    def extract_drive_type(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø—Ä–∏–≤–æ–¥–∞"""
        drive_types = {
            '–ø–µ—Ä–µ–¥–Ω–∏–π': ['–ø–µ—Ä–µ–¥–Ω–∏–π', 'fwd', 'front'],
            '–∑–∞–¥–Ω–∏–π': ['–∑–∞–¥–Ω–∏–π', 'rwd', 'rear'],
            '–ø–æ–ª–Ω—ã–π': ['–ø–æ–ª–Ω—ã–π', 'awd', '4wd', 'all']
        }
        
        text_lower = text.lower()
        for drive_type, keywords in drive_types.items():
            for keyword in keywords:
                if keyword in text_lower:
                    return drive_type
        return None

    def extract_rating(self, element) -> Optional[int]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–∞"""
        if not element:
            return None
            
        # –ü–æ–∏—Å–∫ —á–∏—Å–ª–æ–≤–æ–≥–æ —Ä–µ–π—Ç–∏–Ω–≥–∞
        text = element.get_text()
        match = re.search(r'(\d+(?:\.\d+)?)', text)
        if match:
            try:
                rating = float(match.group(1))
                if 1 <= rating <= 5:
                    return int(rating)
            except ValueError:
                pass
        
        # –ü–æ–∏—Å–∫ –∑–≤–µ–∑–¥ –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        stars = element.find_all(class_=re.compile(r'star|rate'))
        if stars:
            return len(stars)
        
        return None

    def parse_model_reviews(self, brand: str, model: str, long_limit: int = 3, short_limit: int = 10) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤ –º–æ–¥–µ–ª–∏"""
        self.logger.info(f"üöó –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –º–æ–¥–µ–ª–∏: {brand}/{model}")
        
        # –ü–∞—Ä—Å–∏–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
        long_reviews = self.parse_long_reviews(brand, model, long_limit)
        
        # –ü–∞—Ä—Å–∏–º –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–∑—ã–≤—ã
        short_reviews = self.parse_short_reviews(brand, model, short_limit)
        
        self.stats['models_processed'] += 1
        
        result = {
            'brand': brand,
            'model': model,
            'long_reviews': [asdict(review) for review in long_reviews],
            'short_reviews': [asdict(review) for review in short_reviews],
            'stats': {
                'long_reviews_count': len(long_reviews),
                'short_reviews_count': len(short_reviews),
                'total_reviews': len(long_reviews) + len(short_reviews)
            }
        }
        
        self.logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω –ø–∞—Ä—Å–∏–Ω–≥ {brand}/{model}: "
                        f"{len(long_reviews)} –¥–ª–∏–Ω–Ω—ã—Ö + {len(short_reviews)} –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤")
        
        return result

    def parse_multiple_models(self, models_limit: int = 3, long_limit: int = 3, short_limit: int = 10) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        self.logger.info(f"üéØ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {models_limit} –º–æ–¥–µ–ª–µ–π")
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–∞—Ç–∞–ª–æ–≥ –±—Ä–µ–Ω–¥–æ–≤
        brands = self.get_brand_catalog()
        if not brands:
            self.logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –±—Ä–µ–Ω–¥–æ–≤")
            return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –±—Ä–µ–Ω–¥–æ–≤'}
        
        results = []
        models_processed = 0
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –ø–µ—Ä–≤—ã–º –±—Ä–µ–Ω–¥–∞–º
        for brand in brands[:5]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 5 –±—Ä–µ–Ω–¥–æ–≤
            if models_processed >= models_limit:
                break
                
            brand_slug = brand['slug']
            self.logger.info(f"üè≠ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±—Ä–µ–Ω–¥: {brand['name']} ({brand_slug})")
            
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ –±—Ä–µ–Ω–¥–∞
            models = self.get_brand_models(brand_slug)
            
            # –ü–∞—Ä—Å–∏–º –ø–µ—Ä–≤—ã–µ –º–æ–¥–µ–ª–∏ —ç—Ç–æ–≥–æ –±—Ä–µ–Ω–¥–∞
            for model in models[:2]:  # –ú–∞–∫—Å–∏–º—É–º 2 –º–æ–¥–µ–ª–∏ —Å –±—Ä–µ–Ω–¥–∞
                if models_processed >= models_limit:
                    break
                
                try:
                    result = self.parse_model_reviews(
                        model.brand, 
                        model.model, 
                        long_limit, 
                        short_limit
                    )
                    results.append(result)
                    models_processed += 1
                    
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏
                    time.sleep(self.delay * 2)
                    
                except Exception as e:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –º–æ–¥–µ–ª–∏ {model.brand}/{model.model}: {e}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        total_long_reviews = sum(len(r['long_reviews']) for r in results)
        total_short_reviews = sum(len(r['short_reviews']) for r in results)
        
        final_result = {
            'summary': {
                'models_processed': models_processed,
                'total_long_reviews': total_long_reviews,
                'total_short_reviews': total_short_reviews,
                'total_reviews': total_long_reviews + total_short_reviews
            },
            'models': results,
            'stats': self.stats
        }
        
        self.logger.info(f"üéâ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
        self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        self.logger.info(f"   - –ú–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {models_processed}")
        self.logger.info(f"   - –î–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤: {total_long_reviews}")
        self.logger.info(f"   - –ö–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤: {total_short_reviews}")
        self.logger.info(f"   - –í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {total_long_reviews + total_short_reviews}")
        
        return final_result

    def save_results(self, results: Dict, filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"parsing_results_{timestamp}.json"
        
        filepath = os.path.join("data", filename)
        os.makedirs("data", exist_ok=True)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")
            return filepath
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            return None


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üöó –ï–î–ò–ù–´–ô –ü–ê–†–°–ï–† –û–¢–ó–´–í–û–í DROM.RU")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä
    parser = UnifiedDromParser(delay=1.0)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ 3 –º–æ–¥–µ–ª–µ–π
    print("üéØ –ü–∞—Ä—Å–∏–Ω–≥ –ø–µ—Ä–≤—ã—Ö 3 –º–æ–¥–µ–ª–µ–π...")
    results = parser.parse_multiple_models(
        models_limit=3,
        long_limit=3,
        short_limit=10
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    filepath = parser.save_results(results)
    
    print("\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   - –ú–æ–¥–µ–ª–µ–π: {results['summary']['models_processed']}")
    print(f"   - –î–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤: {results['summary']['total_long_reviews']}")
    print(f"   - –ö–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤: {results['summary']['total_short_reviews']}")
    print(f"   - –í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {results['summary']['total_reviews']}")
    print(f"   - –§–∞–π–ª: {filepath}")
    
    print("\nüéâ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")


if __name__ == "__main__":
    main()
