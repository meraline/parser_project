#!/usr/bin/env python3
"""
–û—Ç–ª–∞–¥–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è
"""
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –∫–æ–¥—É
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))

from auto_reviews_parser.parsers.drom import DromParser
from playwright.sync_api import sync_playwright


def debug_specific_addition():
    print("üîß –û—Ç–ª–∞–¥–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è...")

    url = "https://www.drom.ru/reviews/toyota/camry/1428758/187642/"
    parser = DromParser()

    with sync_playwright() as p:
        browser = p.chromium.launch(executable_path=parser.chrome_path, headless=True)

        try:
            page = browser.new_page()
            page.goto(url, wait_until="networkidle")

            # –ü–æ–ª—É—á–∞–µ–º HTML
            content = page.content()

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
            from bs4 import BeautifulSoup

            soup = BeautifulSoup(content, "html.parser")

            # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—ã —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            print("üîç –ü–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫...")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            tables = soup.find_all("table", class_="drom-table")
            print(f"–ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü drom-table: {len(tables)}")

            for i, table in enumerate(tables):
                print(f"\nüìã –¢–∞–±–ª–∏—Ü–∞ {i+1}:")
                rows = table.find_all("tr")
                for row in rows:
                    cells = row.find_all(["td", "th"])
                    if len(cells) >= 2:
                        key = cells[0].get_text(strip=True)
                        value = cells[1].get_text(strip=True)
                        print(f"  {key}: {value}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            print(f"\nüìÑ –î–ª–∏–Ω–∞ HTML: {len(content)}")

            # –ò—â–µ–º –ª—é–±—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            if "–ì–æ–¥ –≤—ã–ø—É—Å–∫–∞" in content:
                print("‚úÖ –ù–∞–π–¥–µ–Ω–æ '–ì–æ–¥ –≤—ã–ø—É—Å–∫–∞' –≤ HTML")
            else:
                print("‚ùå '–ì–æ–¥ –≤—ã–ø—É—Å–∫–∞' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ HTML")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–µ—Ä–∞
            structured_data = parser._extract_review_data(soup, url)
            print(f"\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç _extract_review_data:")
            print(f"  car_specs: {structured_data.get('car_specs', {})}")

        finally:
            browser.close()


if __name__ == "__main__":
    debug_specific_addition()
