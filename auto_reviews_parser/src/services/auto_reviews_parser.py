"""–°—Ç–∞–±–∏–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –æ—Ç–∑—ã–≤–æ–≤ –∏ –±–æ—Ä—Ç–∂—É—Ä–Ω–∞–ª–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å Drom.ru –∏ Drive2.ru –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
–†–∞–±–æ—Ç–∞–µ—Ç –≤ —â–∞–¥—è—â–µ–º —Ä–µ–∂–∏–º–µ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
"""

import sqlite3
import time
import random
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import re
import json
import logging
from urllib.parse import urljoin, urlparse
import hashlib
from pathlib import Path

from botasaurus.browser import browser, Driver
from botasaurus.request import request, Request
from botasaurus.soupify import soupify
from botasaurus import bt

from src.config.settings import Config
from src.database.reviews_database import ReviewsDatabase
from src.parsers import DromParser, Drive2Parser
from src.models import ReviewData

# ==================== –ì–õ–ê–í–ù–´–ô –ü–ê–†–°–ï–† ====================


class AutoReviewsParser:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –ø–∞—Ä—Å–µ—Ä–∞ –æ—Ç–∑—ã–≤–æ–≤ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π"""

    def __init__(self, db_path: str = Config.DB_PATH):
        self.db = ReviewsDatabase(db_path)
        self.setup_logging()
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–æ–≤
        self.drom_parser = DromParser(self.db)
        self.drive2_parser = Drive2Parser(self.db)

    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)

        log_file = log_dir / f"parser_{datetime.now().strftime('%Y%m%d')}.log"

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler(log_file, encoding="utf-8"),
                logging.StreamHandler(),
            ],
        )

    def initialize_sources_queue(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—á–µ—Ä–µ–¥–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        conn = sqlite3.connect(self.db.db_path)
        cursor = conn.cursor()

        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—É—é –æ—á–µ—Ä–µ–¥—å
        cursor.execute("DELETE FROM sources_queue")

        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π
        for brand, models in Config.TARGET_BRANDS.items():
            for model in models:
                for source in ["drom.ru", "drive2.ru"]:
                    cursor.execute(
                        """
                        INSERT INTO sources_queue (brand, model, source, priority)
                        VALUES (?, ?, ?, ?)
                    """,
                        (brand, model, source, 1),
                    )

        conn.commit()
        conn.close()

        total_sources = (
            len(Config.TARGET_BRANDS)
            * sum(len(models) for models in Config.TARGET_BRANDS.values())
            * 2
        )
        print(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –æ—á–µ—Ä–µ–¥—å –∏–∑ {total_sources} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

    def get_next_source(self) -> Optional[Tuple[str, str, str]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        conn = sqlite3.connect(self.db.db_path)
        cursor = conn.cursor()

        # –ò—â–µ–º –Ω–µ—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        cursor.execute(
            """
            SELECT id, brand, model, source FROM sources_queue 
            WHERE status = 'pending' 
            ORDER BY priority DESC, RANDOM()
            LIMIT 1
        """
        )

        result = cursor.fetchone()

        if result:
            source_id, brand, model, source = result

            # –û—Ç–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã–π
            cursor.execute(
                """
                UPDATE sources_queue 
                SET status = 'processing', last_parsed = CURRENT_TIMESTAMP 
                WHERE id = ?
            """,
                (source_id,),
            )

            conn.commit()
            conn.close()

            return brand, model, source

        conn.close()
        return None

    def mark_source_completed(
        self, brand: str, model: str, source: str, pages_parsed: int, reviews_found: int
    ):
        """–û—Ç–º–µ—Ç–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–≥–æ"""
        conn = sqlite3.connect(self.db.db_path)
        cursor = conn.cursor()

        cursor.execute(
            """
            UPDATE sources_queue 
            SET status = 'completed', parsed_pages = ?, total_pages = ?
            WHERE brand = ? AND model = ? AND source = ?
        """,
            (pages_parsed, pages_parsed, brand, model, source),
        )

        conn.commit()
        conn.close()

    def parse_single_source(self, brand: str, model: str, source: str) -> int:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        print(f"\nüéØ –ü–∞—Ä—Å–∏–Ω–≥: {brand} {model} –Ω–∞ {source}")

        reviews = []
        data = {"brand": brand, "model": model, "max_pages": Config.PAGES_PER_SESSION}

        try:
            if source == "drom.ru":
                # –í—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ —Å –ø–µ—Ä–µ–¥–∞—á–µ–π —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –ø–∞—Ä—Å–µ—Ä–∞ —á–µ—Ä–µ–∑ metadata
                reviews = self.drom_parser.parse_brand_model_reviews(
                    data, metadata=self.drom_parser
                )
            elif source == "drive2.ru":
                # –í—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å–∏–≥–Ω–∞—Ç—É—Ä–æ–π
                reviews = self.drive2_parser.parse_brand_model_reviews(data)
            if reviews is None:
                logging.warning(
                    f"Parser returned no reviews for {brand} {model} on {source}"
                )
                reviews = []

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–∑—ã–≤—ã –≤ –±–∞–∑—É
            saved_count = 0
            for review in reviews:
                if self.db.save_review(review):
                    saved_count += 1

            print(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –∏–∑ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤")

            # –û—Ç–º–µ—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π
            self.mark_source_completed(
                brand, model, source, Config.PAGES_PER_SESSION, saved_count
            )

            return saved_count

        except Exception as e:
            logging.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {brand} {model} {source}: {e}")
            return 0

    def run_parsing_session(
        self, max_sources: int = 10, session_duration_hours: int = 2
    ):
        """–ó–∞–ø—É—Å–∫ —Å–µ—Å—Å–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        print(f"\nüöÄ –ó–ê–ü–£–°–ö –°–ï–°–°–ò–ò –ü–ê–†–°–ò–ù–ì–ê")
        print(f"{'='*60}")
        print(f"–ú–∞–∫—Å–∏–º—É–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–∞ —Å–µ—Å—Å–∏—é: {max_sources}")
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {session_duration_hours} —á–∞—Å–æ–≤")
        print(f"{'='*60}")

        session_start = datetime.now()
        session_end = session_start + timedelta(hours=session_duration_hours)

        sources_processed = 0
        total_reviews_saved = 0

        while sources_processed < max_sources and datetime.now() < session_end:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫
            source_info = self.get_next_source()

            if not source_info:
                print("\n‚úÖ –í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
                break

            brand, model, source = source_info

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
            current_count = self.db.get_reviews_count(brand, model)
            if current_count >= Config.MAX_REVIEWS_PER_MODEL:
                print(
                    f"  ‚ö†Ô∏è –õ–∏–º–∏—Ç –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {brand} {model} –¥–æ—Å—Ç–∏–≥–Ω—É—Ç ({current_count})"
                )
                self.mark_source_completed(brand, model, source, 0, 0)
                continue

            # –ü–∞—Ä—Å–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫
            try:
                reviews_saved = self.parse_single_source(brand, model, source)
                total_reviews_saved += reviews_saved
                sources_processed += 1

                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
                if sources_processed < max_sources:
                    delay = random.uniform(30, 60)  # 30-60 —Å–µ–∫—É–Ω–¥ –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
                    print(f"  ‚è≥ –ü–∞—É–∑–∞ {delay:.1f} —Å–µ–∫...")
                    time.sleep(delay)

            except Exception as e:
                logging.error(
                    f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {brand} {model} {source}: {e}"
                )
                sources_processed += 1

                # –£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –ø–∞—É–∑–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                time.sleep(Config.ERROR_DELAY)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏
        session_duration = datetime.now() - session_start

        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ï–°–°–ò–ò")
        print(f"{'='*60}")
        print(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {session_duration}")
        print(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {sources_processed}")
        print(f"–û—Ç–∑—ã–≤–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {total_reviews_saved}")
        print(f"{'='*60}")

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã
        stats = self.db.get_parsing_stats()
        print(f"\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–•")
        print(f"{'='*60}")
        print(f"–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {stats['total_reviews']}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤: {stats['unique_brands']}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {stats['unique_models']}")
        print(f"–ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º: {stats['by_source']}")
        print(f"–ü–æ —Ç–∏–ø–∞–º: {stats['by_type']}")
        print(f"{'='*60}")

    def run_continuous_parsing(
        self, daily_sessions: int = 4, session_sources: int = 10
    ):
        """–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏"""
        print(f"\nüîÑ –†–ï–ñ–ò–ú –ù–ï–ü–†–ï–†–´–í–ù–û–ì–û –ü–ê–†–°–ò–ù–ì–ê")
        print(f"–°–µ—Å—Å–∏–π –≤ –¥–µ–Ω—å: {daily_sessions}")
        print(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–∞ —Å–µ—Å—Å–∏—é: {session_sources}")
        print(f"–ò–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏: {24 // daily_sessions} —á–∞—Å–æ–≤")

        session_interval = timedelta(hours=24 // daily_sessions)

        while True:
            try:
                # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Å—Å–∏—é –ø–∞—Ä—Å–∏–Ω–≥–∞
                self.run_parsing_session(
                    max_sources=session_sources, session_duration_hours=2
                )

                # –ñ–¥–µ–º –¥–æ —Å–ª–µ–¥—É—é—â–µ–π —Å–µ—Å—Å–∏–∏
                next_session = datetime.now() + session_interval
                print(
                    f"\n‚è∞ –°–ª–µ–¥—É—é—â–∞—è —Å–µ—Å—Å–∏—è: {next_session.strftime('%Y-%m-%d %H:%M:%S')}"
                )

                while datetime.now() < next_session:
                    remaining = next_session - datetime.now()
                    print(f"‚è≥ –î–æ —Å–ª–µ–¥—É—é—â–µ–π —Å–µ—Å—Å–∏–∏: {remaining}", end="\r")
                    time.sleep(60)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É

            except KeyboardInterrupt:
                print("\nüëã –ü–∞—Ä—Å–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                break
            except Exception as e:
                logging.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
                print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
                print("‚è≥ –ü–∞—É–∑–∞ 30 –º–∏–Ω—É—Ç –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º...")
                time.sleep(1800)  # 30 –º–∏–Ω—É—Ç –ø–∞—É–∑–∞ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–µ


# ==================== –£–¢–ò–õ–ò–¢–´ –£–ü–†–ê–í–õ–ï–ù–ò–Ø ====================


class ParserManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–æ–º"""

    def __init__(self, db_path: str = Config.DB_PATH):
        self.parser = AutoReviewsParser(db_path)

    def show_status(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—É—Å –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ –æ—á–µ—Ä–µ–¥–∏"""
        stats = self.parser.db.get_parsing_stats()

        print(f"\nüìä –°–¢–ê–¢–£–° –ë–ê–ó–´ –î–ê–ù–ù–´–•")
        print(f"{'='*50}")
        print(f"–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {stats['total_reviews']:,}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤: {stats['unique_brands']}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {stats['unique_models']}")

        if stats["by_source"]:
            print(f"\n–ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
            for source, count in stats["by_source"].items():
                print(f"  {source}: {count:,}")

        if stats["by_type"]:
            print(f"\n–ü–æ —Ç–∏–ø–∞–º:")
            for type_name, count in stats["by_type"].items():
                print(f"  {type_name}: {count:,}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–µ—Ä–µ–¥–∏
        conn = sqlite3.connect(self.parser.db.db_path)
        cursor = conn.cursor()

        cursor.execute("SELECT status, COUNT(*) FROM sources_queue GROUP BY status")
        queue_stats = dict(cursor.fetchall())

        conn.close()

        print(f"\nüìã –°–¢–ê–¢–£–° –û–ß–ï–†–ï–î–ò")
        print(f"{'='*50}")
        total_sources = sum(queue_stats.values())

        for status, count in queue_stats.items():
            percentage = (count / total_sources * 100) if total_sources > 0 else 0
            print(f"{status}: {count} ({percentage:.1f}%)")

        print(f"–í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {total_sources}")

    def reset_queue(self):
        """–°–±—Ä–æ—Å –æ—á–µ—Ä–µ–¥–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        print("üîÑ –°–±—Ä–æ—Å –æ—á–µ—Ä–µ–¥–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
        self.parser.initialize_sources_queue()

    def export_data(self, output_format: str = "xlsx"):
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã"""
        print(f"üì§ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ {output_format}...")

        conn = sqlite3.connect(self.parser.db.db_path)

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ—Ç–∑—ã–≤—ã
        query = """
            SELECT 
                source, type, brand, model, year, title, author, rating,
                content, pros, cons, mileage, engine_volume, fuel_type,
                transmission, body_type, drive_type, publish_date, 
                views_count, likes_count, comments_count, url, parsed_at
            FROM reviews
            ORDER BY brand, model, parsed_at DESC
        """

        df_data = []
        cursor = conn.cursor()
        cursor.execute(query)

        columns = [description[0] for description in cursor.description]

        for row in cursor.fetchall():
            df_data.append(dict(zip(columns, row)))

        conn.close()

        if not df_data:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        if output_format.lower() == "xlsx":
            filename = f"auto_reviews_export_{timestamp}.xlsx"
            bt.write_excel(df_data, filename.replace(".xlsx", ""))
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filename}")

        elif output_format.lower() == "json":
            filename = f"auto_reviews_export_{timestamp}.json"
            bt.write_json(df_data, filename.replace(".json", ""))
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filename}")

        else:
            print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {output_format}")


# ==================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ====================


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
    import argparse

    parser = argparse.ArgumentParser(description="–ü–∞—Ä—Å–µ—Ä –æ—Ç–∑—ã–≤–æ–≤ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π")
    parser.add_argument(
        "command",
        choices=["init", "parse", "continuous", "status", "export"],
        help="–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è",
    )
    parser.add_argument(
        "--sources",
        type=int,
        default=10,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–∞ —Å–µ—Å—Å–∏—é (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)",
    )
    parser.add_argument(
        "--sessions",
        type=int,
        default=4,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ—Å—Å–∏–π –≤ –¥–µ–Ω—å –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 4)",
    )
    parser.add_argument(
        "--format",
        default="xlsx",
        choices=["xlsx", "json"],
        help="–§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: xlsx)",
    )

    args = parser.parse_args()

    manager = ParserManager()

    if args.command == "init":
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞...")
        manager.reset_queue()
        print("‚úÖ –ü–∞—Ä—Å–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")

    elif args.command == "parse":
        print("üéØ –ó–∞–ø—É—Å–∫ —Ä–∞–∑–æ–≤–æ–π —Å–µ—Å—Å–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
        manager.parser.run_parsing_session(max_sources=args.sources)

    elif args.command == "continuous":
        print("üîÑ –ó–∞–ø—É—Å–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
        manager.parser.run_continuous_parsing(
            daily_sessions=args.sessions, session_sources=args.sources
        )

    elif args.command == "status":
        manager.show_status()

    elif args.command == "export":
        manager.export_data(output_format=args.format)


if __name__ == "__main__":
    main()

    # parser = AutoReviewsParser()
    # parser.run_parsing_session(max_sources=10)
    # parser.run_continuous_parsing(daily_sessions=4, session_sources=10)
    # parser.run_parsing_session(max_sources=10)
