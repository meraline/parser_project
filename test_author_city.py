#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞ –∏ –≥–æ—Ä–æ–¥–∞
"""
import sys
import os
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –∫–æ–¥—É
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))

from auto_reviews_parser.parsers.drom import DromParser


def test_author_and_city():
    print("üîß –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞ –∏ –≥–æ—Ä–æ–¥–∞...")

    parser = DromParser()

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º URL
    test_url = "https://www.drom.ru/reviews/toyota/camry/1428758/"

    with sync_playwright() as p:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω
        if os.path.exists(parser.chrome_path):
            browser = p.chromium.launch(
                executable_path=parser.chrome_path, headless=True
            )
        else:
            browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        try:
            parser._go_to_page(page, test_url)
            html_content = page.content()
            browser.close()

            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(html_content, "html.parser")

            print(f"üîç –ò—â–µ–º –∞–≤—Ç–æ—Ä–∞ –∏ –≥–æ—Ä–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {test_url}")

            # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º title
            title = soup.find("title")
            if title:
                print(f"üìÑ Title: {title.text.strip()}")

            # 2. –ò—â–µ–º –∞–≤—Ç–æ—Ä–∞
            print("\nüë§ –ü–æ–∏—Å–∫ –∞–≤—Ç–æ—Ä–∞:")

            # –í–∞—Ä–∏–∞–Ω—Ç 1: itemprop="name"
            author_elem = soup.find("span", {"itemprop": "name"})
            if author_elem:
                print(f"  üìç [itemprop=name]: {author_elem.text.strip()}")

            # –í–∞—Ä–∏–∞–Ω—Ç 2: –≤ –º–µ—Ç–∞-–¥–∞–Ω–Ω—ã—Ö
            author_meta = soup.find("meta", {"name": "author"})
            if author_meta:
                print(f"  üìç [meta author]: {author_meta.get('content', '')}")

            # –í–∞—Ä–∏–∞–Ω—Ç 3: –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö h1, h2
            for tag in ["h1", "h2", "h3"]:
                headers = soup.find_all(tag)
                for header in headers:
                    text = header.text.strip()
                    if "–∞–≤—Ç–æ—Ä" in text.lower() or len(text.split()) <= 3:
                        print(f"  üìç [{tag}]: {text}")

            # –í–∞—Ä–∏–∞–Ω—Ç 4: –≤ –±–ª–æ–∫–∞—Ö —Å –∫–ª–∞—Å—Å–∞–º–∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ user, author, name
            for class_pattern in ["user", "author", "name", "reviewer"]:
                elements = soup.find_all(
                    class_=lambda x: x and class_pattern in x.lower()
                )
                for elem in elements[:3]:  # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                    text = elem.text.strip()
                    if text and len(text) < 100:
                        print(f"  üìç [.{class_pattern}*]: {text[:50]}")

            # 3. –ò—â–µ–º –≥–æ—Ä–æ–¥
            print("\nüèôÔ∏è –ü–æ–∏—Å–∫ –≥–æ—Ä–æ–¥–∞:")

            # –í–∞—Ä–∏–∞–Ω—Ç 1: –≤ –º–µ—Ç–∞-–¥–∞–Ω–Ω—ã—Ö
            location_meta = soup.find("meta", {"name": "geo.placename"})
            if location_meta:
                print(f"  üìç [meta geo]: {location_meta.get('content', '')}")

            # –í–∞—Ä–∏–∞–Ω—Ç 2: –≤ –∫–ª–∞—Å—Å–∞—Ö —Å city, location, geo
            for class_pattern in ["city", "location", "geo", "place"]:
                elements = soup.find_all(
                    class_=lambda x: x and class_pattern in x.lower()
                )
                for elem in elements[:3]:  # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                    text = elem.text.strip()
                    if text and len(text) < 100:
                        print(f"  üìç [.{class_pattern}*]: {text[:50]}")

            # –í–∞—Ä–∏–∞–Ω—Ç 3: –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É "–≥–æ—Ä–æ–¥", "–ö–µ–º–µ—Ä–æ–≤–æ"
            text_elements = soup.find_all(
                text=lambda text: text
                and (
                    "–≥–æ—Ä–æ–¥" in text.lower()
                    or "–∫–µ–º–µ—Ä–æ–≤–æ" in text.lower()
                    or "–º–æ—Å–∫–≤–∞" in text.lower()
                )
            )
            for text_elem in text_elements[:3]:
                parent = text_elem.parent
                if parent:
                    print(f"  üìç [text search]: {parent.text.strip()[:100]}")

            # 4. –ü—Ä–æ–≤–µ—Ä–∏–º –±–ª–æ–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ—Ç–∑—ã–≤–µ
            print("\nüìä –ë–ª–æ–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:")
            info_blocks = soup.find_all(
                class_=lambda x: x
                and (
                    "info" in x.lower() or "details" in x.lower() or "meta" in x.lower()
                )
            )
            for block in info_blocks[:5]:
                text = block.text.strip()
                if text and len(text) < 200:
                    print(f"  üì¶ {text[:100]}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            browser.close()


if __name__ == "__main__":
    test_author_and_city()
