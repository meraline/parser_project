#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π —Å drom.ru.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –æ—Ç–∑—ã–≤–æ–≤.
"""

import requests
import time
import re
from typing import List, Dict, Set
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import logging


class DromCatalogExtractor:
    """–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –∫–∞—Ç–∞–ª–æ–≥–∞ –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π —Å Drom.ru."""

    def __init__(self, delay: float = 2.0):
        self.base_url = "https://www.drom.ru"
        self.reviews_url = "https://www.drom.ru/reviews/"
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
        )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def get_all_brands(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –±—Ä–µ–Ω–¥–æ–≤ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç–∑—ã–≤–æ–≤."""
        try:
            response = self.session.get(self.reviews_url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")
            brands = []

            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –±—Ä–µ–Ω–¥—ã –≤ —Ä–∞–∑–¥–µ–ª–µ –æ—Ç–∑—ã–≤–æ–≤
            # –û–±—ã—á–Ω–æ –æ–Ω–∏ –∏–º–µ—é—Ç –≤–∏–¥ /reviews/{brand}/
            brand_links = soup.find_all("a", href=re.compile(r"/reviews/[a-z0-9\-]+/$"))

            for link in brand_links:
                href = link.get("href")
                if href:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞ –∏–∑ URL
                    match = re.search(r"/reviews/([a-z0-9\-]+)/", href)
                    if match:
                        brand = match.group(1)
                        if (
                            brand not in brands and len(brand) > 1
                        ):  # –ò—Å–∫–ª—é—á–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª—É—á–∞–π–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                            brands.append(brand)

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –±—Ä–µ–Ω–¥—ã —Ç–∞–∫–∏–º —Å–ø–æ—Å–æ–±–æ–º, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥
            if not brands:
                brands = self._get_brands_alternative_method(soup)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
            brands.sort()

            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(brands)} –±—Ä–µ–Ω–¥–æ–≤")
            return brands

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –±—Ä–µ–Ω–¥–æ–≤: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤
            return self._get_fallback_brands()

    def _get_brands_alternative_method(self, soup: BeautifulSoup) -> List[str]:
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –±—Ä–µ–Ω–¥–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ."""
        brands = []

        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –±—Ä–µ–Ω–¥–æ–≤
        all_links = soup.find_all("a", href=True)

        for link in all_links:
            href = link.get("href", "")
            text = link.get_text(strip=True).lower()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Å—Å—ã–ª–∫–æ–π –Ω–∞ –±—Ä–µ–Ω–¥
            if "/reviews/" in href and text:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –±—Ä–µ–Ω–¥ –∏–∑ URL –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞
                brand_match = re.search(r"/reviews/([a-z0-9\-]+)", href)
                if brand_match:
                    brand = brand_match.group(1)
                    if brand not in brands and self._is_valid_brand_name(brand):
                        brands.append(brand)

        return brands

    def _is_valid_brand_name(self, brand: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –≤–∞–ª–∏–¥–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º –±—Ä–µ–Ω–¥–∞."""
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        invalid_names = {
            "new",
            "used",
            "catalog",
            "search",
            "reviews",
            "news",
            "auto",
            "cars",
            "sale",
            "buy",
            "sell",
            "price",
            "photo",
        }

        return (
            len(brand) >= 3
            and brand not in invalid_names
            and not brand.isdigit()
            and re.match(r"^[a-z0-9\-]+$", brand)
        )

    def _get_fallback_brands(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤."""
        return [
            "acura",
            "alfa-romeo",
            "aston-martin",
            "audi",
            "bentley",
            "bmw",
            "bugatti",
            "buick",
            "cadillac",
            "changan",
            "chery",
            "chevrolet",
            "chrysler",
            "citroen",
            "daewoo",
            "datsun",
            "dodge",
            "faw",
            "ferrari",
            "fiat",
            "ford",
            "geely",
            "genesis",
            "great-wall",
            "honda",
            "hyundai",
            "infiniti",
            "isuzu",
            "jaguar",
            "jeep",
            "kia",
            "lada",
            "lamborghini",
            "land-rover",
            "lexus",
            "lifan",
            "maserati",
            "mazda",
            "mercedes",
            "mini",
            "mitsubishi",
            "nissan",
            "opel",
            "peugeot",
            "porsche",
            "renault",
            "rolls-royce",
            "seat",
            "skoda",
            "smart",
            "ssangyong",
            "subaru",
            "suzuki",
            "toyota",
            "volkswagen",
            "volvo",
            "zaz",
        ]

    def get_brand_models(self, brand: str) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –±—Ä–µ–Ω–¥–∞."""
        try:
            brand_url = f"{self.reviews_url}{brand}/"

            self.logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞: {brand}")
            response = self.session.get(brand_url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")
            models = []

            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª–∏
            # –û–±—ã—á–Ω–æ –æ–Ω–∏ –∏–º–µ—é—Ç –≤–∏–¥ /reviews/{brand}/{model}/
            model_links = soup.find_all(
                "a", href=re.compile(rf"/reviews/{re.escape(brand)}/[a-z0-9\-_]+/$")
            )

            for link in model_links:
                href = link.get("href")
                if href:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ URL
                    match = re.search(
                        rf"/reviews/{re.escape(brand)}/([a-z0-9\-_]+)/", href
                    )
                    if match:
                        model = match.group(1)
                        if model not in models and self._is_valid_model_name(model):
                            models.append(model)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
            models.sort()

            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞ {brand}")
            time.sleep(self.delay)

            return models

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞ {brand}: {e}")
            return []

    def _is_valid_model_name(self, model: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –≤–∞–ª–∏–¥–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏."""
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        invalid_names = {"reviews", "catalog", "all", "new", "used", "search"}

        return (
            len(model) >= 1
            and model not in invalid_names
            and re.match(r"^[a-z0-9\-_]+$", model)
        )

    def get_model_review_urls(
        self, brand: str, model: str, max_pages: int = 100
    ) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ URL –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        try:
            model_url = f"{self.reviews_url}{brand}/{model}/"

            self.logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {brand} {model}")

            all_review_urls = []
            page = 1

            while page <= max_pages:
                if page == 1:
                    page_url = model_url
                else:
                    page_url = f"{model_url}?page={page}"

                response = self.session.get(page_url, timeout=10)
                response.raise_for_status()

                soup = BeautifulSoup(response.content, "html.parser")

                # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
                # –û–±—ã—á–Ω–æ –æ–Ω–∏ –∏–º–µ—é—Ç –≤–∏–¥ /reviews/{brand}/{model}/{review_id}/
                review_links = soup.find_all(
                    "a",
                    href=re.compile(
                        rf"/reviews/{re.escape(brand)}/{re.escape(model)}/\d+/"
                    ),
                )

                page_review_urls = []
                for link in review_links:
                    href = link.get("href")
                    if href:
                        full_url = urljoin(self.base_url, href)
                        if full_url not in all_review_urls:
                            page_review_urls.append(full_url)
                            all_review_urls.append(full_url)

                self.logger.info(
                    f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –Ω–∞–π–¥–µ–Ω–æ {len(page_review_urls)} –æ—Ç–∑—ã–≤–æ–≤"
                )

                # –ï—Å–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –æ—Ç–∑—ã–≤–æ–≤, –∑–Ω–∞—á–∏—Ç –º—ã –¥–æ—à–ª–∏ –¥–æ –∫–æ–Ω—Ü–∞
                if not page_review_urls:
                    break

                page += 1
                time.sleep(self.delay)

            self.logger.info(
                f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_review_urls)} –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {brand} {model}"
            )
            return all_review_urls

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è {brand} {model}: {e}")
            return []

    def get_full_catalog(self) -> Dict[str, Dict[str, List[str]]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥: –±—Ä–µ–Ω–¥—ã -> –º–æ–¥–µ–ª–∏ -> –æ—Ç–∑—ã–≤—ã."""
        catalog = {}

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –±—Ä–µ–Ω–¥—ã
        brands = self.get_all_brands()

        for brand in brands:
            self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±—Ä–µ–Ω–¥–∞: {brand}")
            catalog[brand] = {}

            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ –±—Ä–µ–Ω–¥–∞
            models = self.get_brand_models(brand)

            for model in models:
                # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–∑—ã–≤—ã –º–æ–¥–µ–ª–∏
                review_urls = self.get_model_review_urls(brand, model)
                catalog[brand][model] = review_urls

                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏
                time.sleep(self.delay)

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±—Ä–µ–Ω–¥–∞–º–∏
            time.sleep(self.delay * 2)

        return catalog

    def save_catalog_to_file(self, catalog: Dict, filename: str = "drom_catalog.txt"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–∞—Ç–∞–ª–æ–≥ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª."""
        with open(filename, "w", encoding="utf-8") as f:
            for brand, models in catalog.items():
                f.write(f"BRAND: {brand}\n")
                for model, reviews in models.items():
                    f.write(f"  MODEL: {model} ({len(reviews)} –æ—Ç–∑—ã–≤–æ–≤)\n")
                    for review_url in reviews:
                        f.write(f"    {review_url}\n")
                f.write("\n")

        self.logger.info(f"–ö–∞—Ç–∞–ª–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {filename}")


def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –∫–∞—Ç–∞–ª–æ–≥–∞."""
    extractor = DromCatalogExtractor(delay=1.0)

    print("üîç –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –±—Ä–µ–Ω–¥–æ–≤...")
    brands = extractor.get_all_brands()
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(brands)} –±—Ä–µ–Ω–¥–æ–≤: {', '.join(brands[:10])}...")

    if brands:
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø–µ—Ä–≤–æ–º –±—Ä–µ–Ω–¥–µ
        test_brand = brands[0]
        print(f"\nüöó –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞: {test_brand}")
        models = extractor.get_brand_models(test_brand)
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π: {', '.join(models[:5])}...")

        if models:
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏
            test_model = models[0]
            print(f"\nüìù –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è: {test_brand} {test_model}")
            reviews = extractor.get_model_review_urls(
                test_brand, test_model, max_pages=2
            )
            print(f"–ù–∞–π–¥–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤")

            if reviews:
                print("–ü—Ä–∏–º–µ—Ä—ã URL:")
                for url in reviews[:3]:
                    print(f"  - {url}")


if __name__ == "__main__":
    main()
